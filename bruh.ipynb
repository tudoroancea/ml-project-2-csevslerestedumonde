{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.utils.data as tdata\n",
    "import torchvision\n",
    "from torchvision.transforms.functional import resize\n",
    "from PIL import Image\n",
    "import albumentations\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import os\n",
    "import numpy as np\n",
    "import _thread\n",
    "import tqdm\n",
    "import re\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "DEVICE = \"cuda\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Models"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class LinkNetDecoder(torch.nn.Module):\n",
    "    \"\"\"LinkNet decoder\"\"\"\n",
    "\n",
    "    def __init__(self, in_channel, out_channel):\n",
    "        super(LinkNetDecoder, self).__init__()\n",
    "        self.conv1 = torch.nn.Conv2d(in_channel, in_channel // 4, kernel_size=1)\n",
    "        self.bn1 = torch.nn.BatchNorm2d(in_channel // 4)\n",
    "        self.up = torch.nn.ConvTranspose2d(\n",
    "            in_channel // 4,\n",
    "            in_channel // 4,\n",
    "            kernel_size=3,\n",
    "            stride=2,\n",
    "            padding=1,\n",
    "            output_padding=1,\n",
    "        )\n",
    "        self.bn2 = torch.nn.BatchNorm2d(in_channel // 4)\n",
    "        self.conv2 = torch.nn.Conv2d(in_channel // 4, out_channel, kernel_size=1)\n",
    "        self.bn3 = torch.nn.BatchNorm2d(out_channel)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = torch.nn.functional.relu(x)\n",
    "        x = self.up(x)\n",
    "        x = self.bn2(x)\n",
    "        x = torch.nn.functional.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn3(x)\n",
    "        x = torch.nn.functional.relu(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class LinkNet(torch.nn.Module):\n",
    "    \"\"\"LinkNet\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, in_channel, out_channel, filters=None, resnet=None, pretrained=False\n",
    "    ):\n",
    "        super(LinkNet, self).__init__()\n",
    "        if filters is None:\n",
    "            filters = [64, 128, 256, 512]\n",
    "        if resnet is None:\n",
    "            resnet = torchvision.models.resnet34(pretrained=pretrained)\n",
    "        assert len(filters) == 4\n",
    "        self.conv1 = (\n",
    "            resnet.conv1\n",
    "            if in_channel == 3\n",
    "            else torch.nn.Conv2d(in_channel, 64, kernel_size=7, stride=2, padding=3)\n",
    "        )\n",
    "        self.bn1 = resnet.bn1\n",
    "        self.maxpool1 = resnet.maxpool\n",
    "        self.encoders = torch.nn.ModuleList()\n",
    "        self.encoders.append(resnet.layer1)\n",
    "        self.encoders.append(resnet.layer2)\n",
    "        self.encoders.append(resnet.layer3)\n",
    "        self.encoders.append(resnet.layer4)\n",
    "\n",
    "        self.decoders = torch.nn.ModuleList()\n",
    "        filters = filters[::-1]\n",
    "        for i in range(len(filters) - 1):\n",
    "            self.decoders.append(LinkNetDecoder(filters[i], filters[i + 1]))\n",
    "        self.decoders.append(LinkNetDecoder(filters[3], filters[3]))\n",
    "        self.up = torch.nn.ConvTranspose2d(filters[3], 32, kernel_size=3, stride=2)\n",
    "        self.conv2 = torch.nn.Conv2d(32, 32, kernel_size=2)\n",
    "        self.conv3 = torch.nn.Conv2d(32, out_channel, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        xs = []\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = torch.nn.functional.relu(x)\n",
    "        x = self.maxpool1(x)\n",
    "        for enc in self.encoders:\n",
    "            x = enc(x)\n",
    "            xs.append(x)\n",
    "        xs = xs[::-1]\n",
    "        for i in range(3):\n",
    "            x = self.decoders[i](x)\n",
    "            if x.shape[2:] != xs[i + 1].shape[2:]:\n",
    "                x = resize(x, xs[i + 1].shape[2:])\n",
    "            x = x + xs[i + 1]\n",
    "        x = self.decoders[3](x)\n",
    "\n",
    "        x = self.up(x)\n",
    "        x = torch.nn.functional.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = torch.nn.functional.relu(x)\n",
    "        x = self.conv3(x)\n",
    "        return x\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def img_crop(images, size, stride=16, padding=0):\n",
    "    \"\"\"Crop the image into patches of a given size\"\"\"\n",
    "    list_img_patches = []\n",
    "    for image in images:\n",
    "        ndim = image.ndim\n",
    "        imgwidth = image.shape[0] - size\n",
    "        imgheight = image.shape[1] - size\n",
    "        if ndim == 2:\n",
    "            image = image.reshape((image.shape[0], image.shape[1], 1))\n",
    "        new_image = np.zeros(\n",
    "            (image.shape[0] + 2 * padding, image.shape[1] + 2 * padding, image.shape[2])\n",
    "        )\n",
    "        new_image[\n",
    "            padding : padding + image.shape[0], padding : padding + image.shape[1], :\n",
    "        ] = image\n",
    "        image = new_image\n",
    "        for i in range(padding, imgheight + padding + 1, stride):\n",
    "            for j in range(padding, imgwidth + padding + 1, stride):\n",
    "                im_patch = image[\n",
    "                    j - padding : j + size + padding,\n",
    "                    i - padding : i + size + padding,\n",
    "                    :,\n",
    "                ]\n",
    "                if ndim == 2:\n",
    "                    im_patch = im_patch.reshape((im_patch.shape[0], im_patch.shape[1]))\n",
    "                list_img_patches.append(im_patch)\n",
    "    return np.array(list_img_patches, dtype=np.float32)\n",
    "\n",
    "\n",
    "class RoadDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"The class RoadDataset loads the data and executes the pre-processing operations on it\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        image_path,\n",
    "        mask_path,\n",
    "        transform=None,\n",
    "        one_hot=False,\n",
    "        rotations=None,\n",
    "        crop=False,\n",
    "        crop_size=224,\n",
    "        stride=16,\n",
    "        padding=0,\n",
    "    ):\n",
    "        self.transform = transform\n",
    "        self.one_hot = one_hot\n",
    "        self.images = self.load_images(image_path)\n",
    "        self.masks = self.load_images(mask_path)\n",
    "        self.images_augmented = []\n",
    "        self.masks_augmented = []\n",
    "\n",
    "        if rotations is not None:\n",
    "            self.images, self.masks = self.rotate(self.images, self.masks, rotations)\n",
    "\n",
    "        # Crop the images into patches with respect to the given size\n",
    "        if crop:\n",
    "            self.images = img_crop(self.images, crop_size, stride, padding)\n",
    "            self.masks = img_crop(self.masks, crop_size, stride, padding)\n",
    "\n",
    "        # Data augmentation\n",
    "        for i in range(len(self.images)):\n",
    "            output = self.transform(image=self.images[i], mask=self.masks[i])\n",
    "            self.images_augmented.append(output[\"image\"])\n",
    "            self.masks_augmented.append(output[\"mask\"])\n",
    "\n",
    "    @staticmethod\n",
    "    def rotate(images, masks, rotations):\n",
    "        \"\"\"This method applies rotations to the image according to the given angles\"\"\"\n",
    "        ims = []\n",
    "        msks = []\n",
    "        for im, msk in zip(images, masks):\n",
    "            for rotation in rotations:\n",
    "                ims.append(albumentations.rotate(im, rotation))\n",
    "                msks.append(albumentations.rotate(msk, rotation))\n",
    "        return np.asarray(ims), np.asarray(msks)\n",
    "\n",
    "    def get_images(self):\n",
    "        return self.images, self.masks\n",
    "\n",
    "    @staticmethod\n",
    "    def load_images(image_path):\n",
    "        \"\"\"This method loads the images from the given path\"\"\"\n",
    "        images = []\n",
    "        for img in os.listdir(image_path):\n",
    "            path = os.path.join(image_path, img)\n",
    "            image = Image.open(path)\n",
    "            images.append(np.asarray(image))\n",
    "        return np.asarray(images)\n",
    "\n",
    "    def augment(self, index):\n",
    "        \"\"\"This method applies data augmentation to the images\"\"\"\n",
    "        output = self.transform(image=self.images[index], mask=self.masks[index])\n",
    "        self.images_augmented[index] = output[\"image\"]\n",
    "        self.masks_augmented[index] = output[\"mask\"]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"This method returns the image at a certain position and its mask\"\"\"\n",
    "        image = self.images_augmented[index]\n",
    "        mask = self.masks_augmented[index]\n",
    "        #         if self.transform is not None:\n",
    "        #             output = self.transform(image=image, mask=mask)\n",
    "        #             image = output['image']\n",
    "        #             mask = output['mask']\n",
    "\n",
    "        _thread.start_new_thread(self.augment, (index,))\n",
    "        mask = mask.reshape((1,) + mask.shape)\n",
    "        if self.one_hot:\n",
    "            one_hot_mask = torch.zeros((2,) + mask.shape[1:])\n",
    "            one_hot_mask.scatter_(0, mask.long(), 1).float()\n",
    "            mask = one_hot_mask\n",
    "        return (image / 255), (mask > 100).float()\n",
    "\n",
    "\n",
    "def get_loader(\n",
    "    data_path,\n",
    "    mask_path,\n",
    "    transform,\n",
    "    batch_size,\n",
    "    num_worker,\n",
    "    shuffle,\n",
    "    pin_memory,\n",
    "    one_hot=False,\n",
    "    rotations=None,\n",
    "    crop=False,\n",
    "    crop_size=224,\n",
    "    stride=16,\n",
    "    padding=0,\n",
    "):\n",
    "    \"\"\"Create the DataLoader class\"\"\"\n",
    "    dataset = RoadDataset(\n",
    "        data_path,\n",
    "        mask_path,\n",
    "        transform,\n",
    "        one_hot=one_hot,\n",
    "        rotations=rotations,\n",
    "        crop=crop,\n",
    "        crop_size=crop_size,\n",
    "        stride=stride,\n",
    "        padding=padding,\n",
    "    )\n",
    "    return torch.utils.data.DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        pin_memory=pin_memory,\n",
    "        num_workers=num_worker,\n",
    "        generator=torch.Generator().manual_seed(127),\n",
    "    )\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Training"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def f1(pred, label):\n",
    "    \"\"\"Compute F1 score\"\"\"\n",
    "    pred = pred.view(-1)\n",
    "    label = label.view(-1)\n",
    "    tp = (label * pred).sum().to(torch.float32)\n",
    "    # tn = ((1 - label) * (1 - pred)).sum().to(torch.float32)\n",
    "    fp = ((1 - label) * pred).sum().to(torch.float32)\n",
    "    fn = (label * (1 - pred)).sum().to(torch.float32)\n",
    "    eps = 1e-7\n",
    "    precision = tp / (tp + fp + eps)\n",
    "    recall = tp / (tp + fn + eps)\n",
    "    return 2 * precision * recall / (precision + recall + eps), precision, recall\n",
    "\n",
    "\n",
    "def compute_metrics(loader, model, device, one_hot=False):\n",
    "    \"\"\"Compute the accuracy rate on the given dataset with the input model\"\"\"\n",
    "    model.eval()\n",
    "    log = dict()\n",
    "    num_correct = 0\n",
    "    num_pixels = 0\n",
    "    f1_score = 0\n",
    "    precision = 0\n",
    "    recall = 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            output = model(x)\n",
    "            output = output[:, -1, :, :].unsqueeze(1)\n",
    "            pred = torch.sigmoid(output)\n",
    "            if one_hot:\n",
    "                pred: torch.Tensor = pred.argmax(1)\n",
    "                y = y.argmax(1)\n",
    "            else:\n",
    "                pred: torch.Tensor = (pred >= 0.5).float()\n",
    "            num_correct += torch.sum(pred == y).item()\n",
    "            num_pixels += torch.numel(pred)\n",
    "            a, b, c = f1(pred, y)\n",
    "            f1_score += a.item()\n",
    "            precision += b.item()\n",
    "            recall += c.item()\n",
    "\n",
    "    log[\"acc\"] = num_correct / num_pixels * 100\n",
    "    log[\"f1 score\"] = f1_score / len(loader) * 100\n",
    "    log[\"precision\"] = precision / len(loader) * 100\n",
    "    log[\"recall\"] = recall / len(loader) * 100\n",
    "    model.train()\n",
    "    return log\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "prefix = \"/kaggle/input/d/tudoroancea/road-segmentation-dataset/\"\n",
    "Train_image_path = prefix + \"data/training/images\"\n",
    "Train_mask_path = prefix + \"data/training/groundtruth\"\n",
    "Test_image_path = prefix + \"data/testing/images\"\n",
    "Test_mask_path = prefix + \"data/testing/groundtruth\"\n",
    "\n",
    "LEARNING_RATE = 1e-4\n",
    "BATCH_SIZE = 4\n",
    "NUM_EPOCHS = 1000\n",
    "NUM_WORKERS = 0\n",
    "IMAGE_SIZE = 400\n",
    "PIN_MEMORY = True\n",
    "ONE_HOT = False\n",
    "CROP = False\n",
    "CROP_SIZE = 224\n",
    "STRIDE = 16\n",
    "PADDING = 0\n",
    "T = 50\n",
    "\n",
    "torch.manual_seed(0)\n",
    "torch.cuda.manual_seed(0)\n",
    "\n",
    "\n",
    "def train_model(model, loader, optimizer, criterion, scaler):\n",
    "    \"\"\"Train the model\"\"\"\n",
    "    acc_loss = 0\n",
    "    for data, target in loader:\n",
    "        data = data.to(DEVICE)\n",
    "        target = target.to(DEVICE)\n",
    "        with torch.cuda.amp.autocast():\n",
    "            output = model(data)\n",
    "            loss = 0\n",
    "            for i in range(output.shape[1]):\n",
    "                pred = output[:, i, :, :].unsqueeze(1)\n",
    "                loss += criterion(pred, target)\n",
    "\n",
    "            acc_loss += loss.item()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "    return acc_loss\n",
    "\n",
    "\n",
    "def main(name):\n",
    "    if ONE_HOT:\n",
    "        name += \"_with_one_hot\"\n",
    "\n",
    "    # Create the network\n",
    "    net = LinkNet(\n",
    "        in_channel=3,\n",
    "        out_channel=1,\n",
    "        resnet=torchvision.models.resnet152(pretrained=True),\n",
    "        filters=[256, 512, 1024, 2048],\n",
    "    ).to(DEVICE)\n",
    "    # Define the criterion and optimizer\n",
    "    criterion = torch.nn.BCEWithLogitsLoss()\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "    # Define the scheduler and scaler\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=T)\n",
    "    scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "    # Define the data augmentation used for the training set, and also create the data loader for it\n",
    "    train_transform = albumentations.Compose(\n",
    "        [\n",
    "            albumentations.Resize(height=IMAGE_SIZE, width=IMAGE_SIZE),\n",
    "            albumentations.Flip(),\n",
    "            albumentations.Transpose(),\n",
    "            albumentations.Rotate(),\n",
    "            albumentations.CoarseDropout(max_holes=8, max_height=8, max_width=8),\n",
    "            albumentations.OneOf(\n",
    "                [\n",
    "                    albumentations.OpticalDistortion(),\n",
    "                    albumentations.GridDistortion(),\n",
    "                    albumentations.ElasticTransform(),\n",
    "                ],\n",
    "                p=0.5,\n",
    "            ),\n",
    "            ToTensorV2(),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    train_loader = get_loader(\n",
    "        Train_image_path,\n",
    "        Train_mask_path,\n",
    "        train_transform,\n",
    "        BATCH_SIZE,\n",
    "        NUM_WORKERS,\n",
    "        shuffle=True,\n",
    "        pin_memory=PIN_MEMORY,\n",
    "        one_hot=ONE_HOT,\n",
    "        crop=CROP,\n",
    "        crop_size=CROP_SIZE,\n",
    "        stride=STRIDE,\n",
    "        padding=PADDING,\n",
    "    )\n",
    "\n",
    "    # Define the data augmentation used for the validation set, and also create the data loader for it\n",
    "    val_transform = albumentations.Compose(\n",
    "        [\n",
    "            albumentations.Resize(height=IMAGE_SIZE, width=IMAGE_SIZE),\n",
    "            albumentations.Flip(),\n",
    "            albumentations.Transpose(),\n",
    "            ToTensorV2(),\n",
    "        ]\n",
    "    )\n",
    "    val_loader = get_loader(\n",
    "        Test_image_path,\n",
    "        Test_mask_path,\n",
    "        val_transform,\n",
    "        BATCH_SIZE,\n",
    "        NUM_WORKERS,\n",
    "        shuffle=False,\n",
    "        pin_memory=PIN_MEMORY,\n",
    "        rotations=[0, 90, 180, 270],\n",
    "        one_hot=ONE_HOT,\n",
    "        crop=CROP,\n",
    "        crop_size=CROP_SIZE,\n",
    "        stride=STRIDE,\n",
    "        padding=PADDING,\n",
    "    )\n",
    "\n",
    "    # Train the model, then save the training logs and the best model\n",
    "    logs = []\n",
    "    loop = tqdm.tqdm(range(NUM_EPOCHS))\n",
    "    max_f1 = 0\n",
    "\n",
    "    for e in loop:\n",
    "        loss = train_model(net, train_loader, optimizer, criterion, scaler)\n",
    "        scheduler.step()\n",
    "        log = compute_metrics(val_loader, net, DEVICE, one_hot=ONE_HOT)\n",
    "        log[\"epochs\"] = e\n",
    "        log[\"loss\"] = loss\n",
    "        logs.append(log)\n",
    "        if log[\"f1 score\"] > max_f1:\n",
    "            max_f1 = log[\"f1 score\"]\n",
    "            if max_f1 > 80.0:\n",
    "                torch.save(net, \"checkpoints/\" + name + \"_max_f1.pth\")\n",
    "        loop.set_postfix(\n",
    "            loss=loss, acc=log[\"acc\"], f1_score=log[\"f1 score\"], max_f1=max_f1\n",
    "        )\n",
    "\n",
    "    # Save the logs into a file\n",
    "    f = open(\"logs/\" + name + \"_results\", mode=\"w\")\n",
    "    for log in logs:\n",
    "        f.write(str(log) + \"\\n\")\n",
    "    f.close()\n",
    "    #     torch.save(net, 'checkpoints/' + name + '.pth')\n",
    "    return net\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "main(\"bruh\")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Creating Submission\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def create_different_prospective(images, rotations, transposes):\n",
    "    \"\"\"Apply transformations to the image and return different prospectives\"\"\"\n",
    "    ims = []\n",
    "    for image in images:\n",
    "        for rotation in rotations:\n",
    "            ims.append(albumentations.rotate(image, rotation))\n",
    "            if transposes:\n",
    "                im = albumentations.hflip(image)\n",
    "                ims.append(albumentations.rotate(im, rotation))\n",
    "    ims = np.array(ims)\n",
    "    ims = torch.tensor(ims).transpose(1, -1).transpose(2, -1).float()\n",
    "    return ims\n",
    "\n",
    "\n",
    "def combine_different_prospective(images, rotations, transposes):\n",
    "    \"\"\"Combine predictions of different prospectives\"\"\"\n",
    "    outputs = []\n",
    "    index = 0\n",
    "    while index < len(images):\n",
    "        output = np.zeros(images[0].shape)\n",
    "        for rotation in rotations:\n",
    "            im = images[index, 0]\n",
    "            output += albumentations.rotate(im, -rotation)\n",
    "            index += 1\n",
    "            if transposes:\n",
    "                im = images[index, 0]\n",
    "                im = albumentations.rotate(im, -rotation)\n",
    "                output += albumentations.hflip(im)\n",
    "                index += 1\n",
    "        output = output / len(images)\n",
    "        outputs.append(output)\n",
    "    return np.array(outputs)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "foreground_threshold = 0.5\n",
    "\n",
    "\n",
    "def patch_to_label(patch):\n",
    "    df = np.mean(patch)\n",
    "    if df > foreground_threshold:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "\n",
    "def mask_to_submission_strings(image_filename):\n",
    "    \"\"\"Reads a single image and outputs the strings that should go into the submission file\"\"\"\n",
    "    img_number = int(re.search(r\"\\d+\", image_filename).group(0))\n",
    "    im = mpimg.imread(image_filename)\n",
    "    patch_size = 16\n",
    "    for j in range(0, im.shape[1], patch_size):\n",
    "        for i in range(0, im.shape[0], patch_size):\n",
    "            patch = im[i : i + patch_size, j : j + patch_size]\n",
    "            label = patch_to_label(patch)\n",
    "            yield (\"{:03d}_{}_{},{}\".format(img_number, j, i, label))\n",
    "\n",
    "\n",
    "def masks_to_submission(submission_filename, *image_filenames):\n",
    "    \"\"\"Converts images into a submission file\"\"\"\n",
    "    with open(submission_filename, \"w\") as f:\n",
    "        f.write(\"id,prediction\\n\")\n",
    "        for fn in image_filenames[0:]:\n",
    "            f.writelines(\"{}\\n\".format(s) for s in mask_to_submission_strings(fn))\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "IMAGE_SIZE = 608\n",
    "ROTATIONS = [0, 90, 180, 270]\n",
    "TRANSPOSE = True\n",
    "ONE_HOT = False\n",
    "model_name = \"bruh_max_f1.pth\"\n",
    "\n",
    "model = torch.load(\"/kaggle/working/checkpoints/\" + model_name).to(DEVICE)\n",
    "model.eval()\n",
    "\n",
    "# Create the directory to store the predictions\n",
    "path = prefix + \"data/test_set_images\"\n",
    "pred_path = \"/kaggle/working/prediction/\" + model_name\n",
    "if not os.path.exists(pred_path):\n",
    "    os.makedirs(pred_path)\n",
    "\n",
    "# For each image, apply Test Time Augmentation, make predictions and save predictions\n",
    "images = os.listdir(path)\n",
    "for image in tqdm.tqdm(images):\n",
    "    img_path = os.path.join(path, image, image + \".png\")\n",
    "    im = np.asarray(Image.open(img_path)) / 255\n",
    "    ims = create_different_prospective([im], ROTATIONS, TRANSPOSE)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(ims.to(DEVICE))\n",
    "        output = output[:, 0].unsqueeze(1)\n",
    "        predicts = torch.sigmoid(output).cpu().detach()\n",
    "        # predicts = torch.sigmoid(model(ims.to(device))).cpu().detach()\n",
    "\n",
    "    if ONE_HOT:\n",
    "        predicts = predicts.argmax(1).unsqueeze(1).float()\n",
    "    predict = combine_different_prospective(\n",
    "        predicts.numpy(), ROTATIONS, TRANSPOSE\n",
    "    ).reshape((608, 608))\n",
    "    # predict[predict < 0.5] = 0\n",
    "    # predict[predict >= 0.5] = 1\n",
    "    predict *= 255\n",
    "    Image.fromarray(predict).convert(\"L\").save(os.path.join(pred_path, image) + \".png\")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Generate the submission file\n",
    "submission_filename = \"dummy_submission.csv\"\n",
    "image_filenames = []\n",
    "for i in range(1, 51):\n",
    "    image_filename = pred_path + \"/test_\" + str(i) + \".png\"\n",
    "    #     print(image_filename)\n",
    "    image_filenames.append(image_filename)\n",
    "masks_to_submission(submission_filename, *image_filenames)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
