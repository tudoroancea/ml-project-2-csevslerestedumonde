{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEVICE= cuda\n"
     ]
    }
   ],
   "source": [
    "import _thread\n",
    "import os\n",
    "import re\n",
    "\n",
    "import albumentations\n",
    "import matplotlib.image as mpimg\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.utils.data\n",
    "import torchvision\n",
    "import tqdm\n",
    "from PIL import Image\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from torchvision.transforms.functional import resize\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"DEVICE=\", DEVICE)\n",
    "SUBMISSION_THRESHOLD = 0.25\n",
    "Train_image_path = \"data/training/images\"\n",
    "Train_mask_path = \"data/training/groundtruth\"\n",
    "Validation_image_path = \"data/validating/images\"\n",
    "Validation_mask_path = \"data/validating/groundtruth\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data processing"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "class RoadDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    The class RoadDataset loads the data and executes the pre-processing operations on it.\n",
    "    More specifically, it re-applies the specified transform every time data is fetched via a dataloader.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        image_path: str,\n",
    "        mask_path: str,\n",
    "        transform,\n",
    "    ):\n",
    "        self.transform = transform\n",
    "        self.images = self.load_images(image_path)\n",
    "        self.masks = self.load_images(mask_path)\n",
    "        self.images_augmented = []\n",
    "        self.masks_augmented = []\n",
    "\n",
    "        # Data augmentation\n",
    "        for i in range(len(self.images)):\n",
    "            output = self.transform(image=self.images[i], mask=self.masks[i])\n",
    "            self.images_augmented.append(output[\"image\"])\n",
    "            self.masks_augmented.append(output[\"mask\"])\n",
    "\n",
    "    def get_images(self):\n",
    "        return self.images, self.masks\n",
    "\n",
    "    @staticmethod\n",
    "    def load_images(image_path):\n",
    "        \"\"\"This method loads the images from the given path\"\"\"\n",
    "        images = []\n",
    "        for img in os.listdir(image_path):\n",
    "            path = os.path.join(image_path, img)\n",
    "            image = Image.open(path)\n",
    "            images.append(np.asarray(image))\n",
    "            # images.append(cv2.imread(path))\n",
    "        return np.asarray(images)\n",
    "\n",
    "    def augment(self, index):\n",
    "        \"\"\"This method applies data augmentation to the images\"\"\"\n",
    "        output = self.transform(image=self.images[index], mask=self.masks[index])\n",
    "        self.images_augmented[index] = output[\"image\"]\n",
    "        self.masks_augmented[index] = output[\"mask\"]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"This method returns the image at a certain position and its mask\"\"\"\n",
    "        image = self.images_augmented[index]\n",
    "        mask = self.masks_augmented[index]\n",
    "        _thread.start_new_thread(self.augment, (index,))\n",
    "        return (image / 255), (mask.unsqueeze(0) > 100).float()\n",
    "\n",
    "\n",
    "def get_loader(\n",
    "    data_path: str,\n",
    "    mask_path: str,\n",
    "    transform,\n",
    "    batch_size: int = 4,\n",
    ") -> torch.utils.data.DataLoader:\n",
    "    \"\"\"Create the DataLoader class\"\"\"\n",
    "    dataset = RoadDataset(\n",
    "        data_path,\n",
    "        mask_path,\n",
    "        transform,\n",
    "    )\n",
    "    return torch.utils.data.DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        pin_memory=True,\n",
    "        generator=torch.Generator().manual_seed(127),\n",
    "    )\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Models"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# ================================================================================================\n",
    "# UNET\n",
    "# ================================================================================================\n",
    "\n",
    "\n",
    "class DoubleConv(torch.nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.double_conv = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(\n",
    "                in_channels, out_channels, kernel_size=3, padding=1, bias=False\n",
    "            ),\n",
    "            torch.nn.BatchNorm2d(out_channels),\n",
    "            torch.nn.ReLU(inplace=True),\n",
    "            torch.nn.Conv2d(\n",
    "                out_channels, out_channels, kernel_size=3, padding=1, bias=False\n",
    "            ),\n",
    "            torch.nn.BatchNorm2d(out_channels),\n",
    "            torch.nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        return self.double_conv(x)\n",
    "\n",
    "\n",
    "class Down(torch.nn.Module):\n",
    "    \"\"\"Downscaling with maxpool then double conv\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.maxpool_conv = torch.nn.Sequential(\n",
    "            torch.nn.MaxPool2d(2), DoubleConv(in_channels, out_channels)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.maxpool_conv(x)\n",
    "\n",
    "\n",
    "class Up(torch.nn.Module):\n",
    "    \"\"\"Upscaling then double conv\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.up = torch.nn.ConvTranspose2d(\n",
    "            in_channels, in_channels // 2, kernel_size=2, stride=2\n",
    "        )\n",
    "        self.conv = DoubleConv(in_channels, out_channels)\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.up(x1)\n",
    "        x = torch.cat([x2, x1], dim=1)\n",
    "        del x1\n",
    "        del x2\n",
    "        torch.cuda.empty_cache()\n",
    "        return self.conv(x)\n",
    "\n",
    "\n",
    "class UNet(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.inconv = DoubleConv(3, 64)\n",
    "        self.down1 = Down(64, 128)\n",
    "        self.down2 = Down(128, 256)\n",
    "        self.down3 = Down(256, 512)\n",
    "        self.down4 = Down(512, 1024)\n",
    "        self.up1 = Up(1024, 512)\n",
    "        self.up2 = Up(512, 256)\n",
    "        self.up3 = Up(256, 128)\n",
    "        self.up4 = Up(128, 64)\n",
    "        self.outconv = torch.nn.Conv2d(64, 1, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.inconv(x)\n",
    "        x2 = self.down1(x1)\n",
    "        x3 = self.down2(x2)\n",
    "        x4 = self.down3(x3)\n",
    "        x5 = self.down4(x4)\n",
    "        x = self.up1(x5, x4)\n",
    "        x = self.up2(x, x3)\n",
    "        x = self.up3(x, x2)\n",
    "        x = self.up4(x, x1)\n",
    "        logits = self.outconv(x)\n",
    "        torch.cuda.empty_cache()\n",
    "        return logits\n",
    "\n",
    "\n",
    "# ================================================================================================\n",
    "# LINKNET\n",
    "# ================================================================================================\n",
    "\n",
    "\n",
    "class LinkNetDecoderBlock(torch.nn.Module):\n",
    "    def __init__(self, in_channel, out_channel):\n",
    "        super(LinkNetDecoderBlock, self).__init__()\n",
    "        self.conv1 = torch.nn.Conv2d(in_channel, in_channel // 4, kernel_size=1)\n",
    "        self.bn1 = torch.nn.BatchNorm2d(in_channel // 4)\n",
    "        self.up = torch.nn.ConvTranspose2d(\n",
    "            in_channel // 4,\n",
    "            in_channel // 4,\n",
    "            kernel_size=3,\n",
    "            stride=2,\n",
    "            padding=1,\n",
    "            output_padding=1,\n",
    "        )\n",
    "        self.bn2 = torch.nn.BatchNorm2d(in_channel // 4)\n",
    "        self.conv2 = torch.nn.Conv2d(in_channel // 4, out_channel, kernel_size=1)\n",
    "        self.bn3 = torch.nn.BatchNorm2d(out_channel)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = torch.nn.functional.relu(x)\n",
    "        x = self.up(x)\n",
    "        x = self.bn2(x)\n",
    "        x = torch.nn.functional.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn3(x)\n",
    "        x = torch.nn.functional.relu(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class LinkNet(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        encoder,\n",
    "        channels=(64, 128, 256, 512),\n",
    "    ):\n",
    "        super().__init__()\n",
    "        assert len(channels) == 4\n",
    "        self.conv1 = encoder.conv1\n",
    "        self.bn1 = encoder.bn1\n",
    "        self.maxpool1 = encoder.maxpool\n",
    "        self.encoders = torch.nn.ModuleList()\n",
    "        self.encoders.append(encoder.layer1)\n",
    "        self.encoders.append(encoder.layer2)\n",
    "        self.encoders.append(encoder.layer3)\n",
    "        self.encoders.append(encoder.layer4)\n",
    "\n",
    "        self.decoders = torch.nn.ModuleList()\n",
    "        channels = channels[::-1]\n",
    "        for i in range(len(channels) - 1):\n",
    "            self.decoders.append(LinkNetDecoderBlock(channels[i], channels[i + 1]))\n",
    "        self.decoders.append(LinkNetDecoderBlock(channels[-1], channels[-1]))\n",
    "        self.up = torch.nn.ConvTranspose2d(channels[-1], 32, kernel_size=3, stride=2)\n",
    "        self.conv2 = torch.nn.Conv2d(32, 32, kernel_size=2)\n",
    "        self.conv3 = torch.nn.Conv2d(32, 1, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        xs = []\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = torch.nn.functional.relu(x)\n",
    "        x = self.maxpool1(x)\n",
    "        for enc in self.encoders:\n",
    "            x = enc(x)\n",
    "            xs.append(x)\n",
    "        xs = xs[::-1]\n",
    "        for i in range(3):\n",
    "            x = self.decoders[i](x)\n",
    "            if x.shape[2:] != xs[i + 1].shape[2:]:\n",
    "                x = resize(x, xs[i + 1].shape[2:])\n",
    "            x = x + xs[i + 1]\n",
    "        x = self.decoders[3](x)\n",
    "\n",
    "        x = self.up(x)\n",
    "        x = torch.nn.functional.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = torch.nn.functional.relu(x)\n",
    "        x = self.conv3(x)\n",
    "        return x\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Training"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def f1(pred, label):\n",
    "    \"\"\"Compute F1 score\"\"\"\n",
    "    pred = pred.view(-1)\n",
    "    label = label.view(-1)\n",
    "    tp = (label * pred).sum().to(torch.float32)\n",
    "    # tn = ((1 - label) * (1 - pred)).sum().to(torch.float32)\n",
    "    fp = ((1 - label) * pred).sum().to(torch.float32)\n",
    "    fn = (label * (1 - pred)).sum().to(torch.float32)\n",
    "    eps = 1e-7\n",
    "    precision = tp / (tp + fp + eps)\n",
    "    recall = tp / (tp + fn + eps)\n",
    "    return 2 * precision * recall / (precision + recall + eps), precision, recall\n",
    "\n",
    "\n",
    "def compute_metrics(loader, model, device):\n",
    "    \"\"\"Compute the accuracy rate on the given dataset with the input model\"\"\"\n",
    "    model.eval()\n",
    "    log = dict()\n",
    "    num_correct = 0\n",
    "    num_pixels = 0\n",
    "    f1_score = 0\n",
    "    precision = 0\n",
    "    recall = 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            output = model(x)\n",
    "            output = output[:, -1, :, :].unsqueeze(1)\n",
    "            pred: torch.Tensor = (torch.sigmoid(output) >= 0.5).float()\n",
    "            num_correct += torch.sum(pred == y).item()\n",
    "            num_pixels += torch.numel(pred)\n",
    "            a, b, c = f1(pred, y)\n",
    "            f1_score += a.item()\n",
    "            precision += b.item()\n",
    "            recall += c.item()\n",
    "\n",
    "    log[\"acc\"] = num_correct / num_pixels * 100\n",
    "    log[\"f1 score\"] = f1_score / len(loader) * 100\n",
    "    log[\"precision\"] = precision / len(loader) * 100\n",
    "    log[\"recall\"] = recall / len(loader) * 100\n",
    "    model.train()\n",
    "    return log\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "def epoch(model, loader, optimizer, criterion, scaler):\n",
    "    \"\"\"Train the model\"\"\"\n",
    "    acc_loss = 0\n",
    "    for data, target in loader:\n",
    "        data = data.to(DEVICE)\n",
    "        target = target.to(DEVICE)\n",
    "        with torch.cuda.amp.autocast():\n",
    "            output = model(data)\n",
    "            loss = 0\n",
    "            for i in range(output.shape[1]):\n",
    "                pred = output[:, i, :, :].unsqueeze(1)\n",
    "                loss += criterion(pred, target)\n",
    "\n",
    "            acc_loss += loss.item()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "    return acc_loss\n",
    "\n",
    "\n",
    "def train(\n",
    "    model,\n",
    "    model_name,\n",
    "    train_loader,\n",
    "    validation_loader,\n",
    "    lr: float = 1.0e-4,\n",
    "    epochs: int = 10,\n",
    "    with_dice: bool = False,\n",
    "):\n",
    "    log_file_name = os.path.relpath(os.path.join(\"logs\", model_name + \".csv\"))\n",
    "    with open(log_file_name, \"w\") as f:\n",
    "        f.write(\"epoch,loss,f1,iou,accuracy,precision,recall\\n\")\n",
    "\n",
    "    model_file_name = os.path.join(\"checkpoints\", model_name + \".pth\")\n",
    "\n",
    "    # Define the criterion and optimizer\n",
    "    criterion = torch.nn.BCEWithLogitsLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    # Define the scheduler and scaler\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=50)\n",
    "    scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "    # Define the data augmentation used for the training set, and also create the data loader for it\n",
    "\n",
    "    # Train the model, then save the training logs and the best model\n",
    "    loop = tqdm.tqdm(range(epochs))\n",
    "    max_f1 = 0\n",
    "    for e in loop:\n",
    "        loss = epoch(model, train_loader, optimizer, criterion, scaler)\n",
    "        scheduler.step()\n",
    "        metrics = compute_metrics(validation_loader, model, DEVICE)\n",
    "        if metrics[\"f1 score\"] > max_f1:\n",
    "            max_f1 = metrics[\"f1 score\"]\n",
    "            if max_f1 > 80.0:\n",
    "                torch.save(model, model_file_name + \".maxf1\")\n",
    "\n",
    "        with open(log_file_name, \"a\") as f:\n",
    "            f.write(\n",
    "                \"{},{},{},{},{},{},{}\\n\".format(\n",
    "                    e,\n",
    "                    loss,\n",
    "                    metrics[\"f1 score\"],\n",
    "                    0,\n",
    "                    metrics[\"acc\"],\n",
    "                    metrics[\"precision\"],\n",
    "                    metrics[\"recall\"],\n",
    "                )\n",
    "            )\n",
    "\n",
    "        loop.set_postfix(loss=loss, f1_score=metrics[\"f1 score\"], max_f1=max_f1)\n",
    "\n",
    "    # Save the logs into a file\n",
    "    torch.save(model, model_file_name)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "# DO NOT RUN THIS CELL ==============================================================================\n",
    "train_transform = albumentations.Compose(\n",
    "    [\n",
    "        albumentations.Flip(),\n",
    "        albumentations.Transpose(),\n",
    "        albumentations.Rotate(),\n",
    "        albumentations.CoarseDropout(max_holes=8, max_height=8, max_width=8),\n",
    "        albumentations.OneOf(\n",
    "            [\n",
    "                albumentations.OpticalDistortion(),\n",
    "                albumentations.GridDistortion(),\n",
    "                albumentations.ElasticTransform(),\n",
    "            ],\n",
    "            p=0.5,\n",
    "        ),\n",
    "        ToTensorV2(),\n",
    "    ]\n",
    ")\n",
    "train_loader = get_loader(\n",
    "    data_path=Train_image_path,\n",
    "    mask_path=Train_mask_path,\n",
    "    transform=train_transform,\n",
    "    batch_size=4,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# RUN THIS CELL ==============================================================================\n",
    "train_transform_no_distortion = albumentations.Compose(\n",
    "    [\n",
    "        albumentations.Flip(),\n",
    "        albumentations.Transpose(),\n",
    "        albumentations.Rotate(),\n",
    "        albumentations.CoarseDropout(max_holes=8, max_height=8, max_width=8),\n",
    "        albumentations.OneOf(\n",
    "            [\n",
    "                albumentations.OpticalDistortion(),\n",
    "                albumentations.GridDistortion(),\n",
    "                albumentations.ElasticTransform(),\n",
    "            ],\n",
    "            p=0.5,\n",
    "        ),\n",
    "        ToTensorV2(),\n",
    "    ]\n",
    ")\n",
    "train_loader_no_distortion = get_loader(\n",
    "    data_path=Train_image_path,\n",
    "    mask_path=Train_mask_path,\n",
    "    transform=train_transform_no_distortion,\n",
    "    batch_size=4,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Define the data augmentation used for the validation set, and also create the data loader for it\n",
    "val_transform = albumentations.Compose(\n",
    "    [\n",
    "        albumentations.Flip(),\n",
    "        albumentations.Transpose(),\n",
    "        ToTensorV2(),\n",
    "    ]\n",
    ")\n",
    "val_loader = get_loader(\n",
    "    data_path=Validation_image_path,\n",
    "    mask_path=Validation_mask_path,\n",
    "    transform=val_transform,\n",
    "    batch_size=4,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Actual training"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## BCE loss with distortions"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [1:54:06<00:00,  6.85s/it, f1_score=87.2, loss=1.09, max_f1=89.5]\n"
     ]
    }
   ],
   "source": [
    "train(\n",
    "    model=UNet().to(DEVICE),\n",
    "    model_name=\"unet\",\n",
    "    epochs=1000,\n",
    "    train_loader=train_loader,\n",
    "    validation_loader=val_loader,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [1:47:39<00:00,  6.46s/it, f1_score=87.5, loss=0.962, max_f1=88.8]\n"
     ]
    }
   ],
   "source": [
    "train(\n",
    "    model=LinkNet(\n",
    "        encoder=torchvision.models.resnet18(\n",
    "            weights=torchvision.models.ResNet18_Weights.IMAGENET1K_V1\n",
    "        ),\n",
    "        channels=(64, 128, 256, 512),\n",
    "    ).to(DEVICE),\n",
    "    model_name=\"linknet18\",\n",
    "    epochs=1000,\n",
    "    train_loader=train_loader,\n",
    "    validation_loader=val_loader,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [1:54:06<00:00,  6.85s/it, f1_score=88.3, loss=0.921, max_f1=89.4]   \n"
     ]
    }
   ],
   "source": [
    "train(\n",
    "    model=LinkNet(\n",
    "        encoder=torchvision.models.resnet34(\n",
    "            weights=torchvision.models.ResNet34_Weights.IMAGENET1K_V1\n",
    "        ),\n",
    "        channels=(64, 128, 256, 512),\n",
    "    ).to(DEVICE),\n",
    "    model_name=\"linknet34\",\n",
    "    epochs=1000,\n",
    "    train_loader=train_loader,\n",
    "    validation_loader=val_loader,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet50-11ad3fa6.pth\" to /home/oancea/.cache/torch/hub/checkpoints/resnet50-11ad3fa6.pth\n",
      "100.0%\n",
      "100%|██████████| 1000/1000 [2:02:59<00:00,  7.38s/it, f1_score=86.2, loss=0.678, max_f1=89.2] \n"
     ]
    }
   ],
   "source": [
    "train(\n",
    "    model=LinkNet(\n",
    "        encoder=torchvision.models.resnet50(\n",
    "            weights=torchvision.models.ResNet50_Weights.IMAGENET1K_V2\n",
    "        ),\n",
    "        channels=(256, 512, 1024, 2048),\n",
    "    ).to(DEVICE),\n",
    "    model_name=\"linknet50\",\n",
    "    epochs=1000,\n",
    "    train_loader=train_loader,\n",
    "    validation_loader=val_loader,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train(\n",
    "    model=LinkNet(\n",
    "        encoder=torchvision.models.resnet101(\n",
    "            weights=torchvision.models.ResNet101_Weights.IMAGENET1K_V2\n",
    "        ),\n",
    "        channels=(256, 512, 1024, 2048),\n",
    "    ).to(DEVICE),\n",
    "    model_name=\"linknet101\",\n",
    "    epochs=1000,\n",
    "    train_loader=train_loader,\n",
    "    validation_loader=val_loader,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train(\n",
    "    model=LinkNet(\n",
    "        encoder=torchvision.models.resnet152(\n",
    "            weights=torchvision.models.ResNet152_Weights.IMAGENET1K_V2\n",
    "        ),\n",
    "        channels=(256, 512, 1024, 2048),\n",
    "    ).to(DEVICE),\n",
    "    model_name=\"linknet152\",\n",
    "    epochs=1000,\n",
    "    train_loader=train_loader,\n",
    "    validation_loader=val_loader,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## BCE loss without distortions"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train(\n",
    "    model=UNet().to(DEVICE),\n",
    "    model_name=\"unet_no_distortion\",\n",
    "    epochs=1000,\n",
    "    train_loader=train_loader_no_distortion,\n",
    "    validation_loader=val_loader,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Dice loss with distortions"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pass"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Create submissions"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "def patch_to_label(patch):\n",
    "    df = np.mean(patch)\n",
    "    if df > SUBMISSION_THRESHOLD:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "\n",
    "def mask_to_submission_strings(image_filename):\n",
    "    \"\"\"Reads a single image and outputs the strings that should go into the submission file\"\"\"\n",
    "    print(image_filename, os.path.basename(image_filename))\n",
    "    img_number = int(re.search(r\"\\d+\", os.path.basename(image_filename)).group(0))\n",
    "    print(img_number)\n",
    "    im = mpimg.imread(image_filename)\n",
    "    patch_size = 16\n",
    "    for j in range(0, im.shape[1], patch_size):\n",
    "        for i in range(0, im.shape[0], patch_size):\n",
    "            patch = im[i : i + patch_size, j : j + patch_size]\n",
    "            label = patch_to_label(patch)\n",
    "            yield (\"{:03d}_{}_{},{}\".format(img_number, j, i, label))\n",
    "\n",
    "\n",
    "def masks_to_submission(submission_filename, *image_filenames):\n",
    "    \"\"\"Converts images into a submission file\"\"\"\n",
    "    with open(submission_filename, \"w\") as f:\n",
    "        f.write(\"id,prediction\\n\")\n",
    "        for fn in image_filenames:\n",
    "            f.writelines(\"{}\\n\".format(s) for s in mask_to_submission_strings(fn))\n",
    "\n",
    "\n",
    "def create_postprocessing_images(images, rotations, transposes):\n",
    "    \"\"\"Apply transformations to the image and return different prospectives\"\"\"\n",
    "    ims = []\n",
    "    for image in images:\n",
    "        for rotation in rotations:\n",
    "            ims.append(albumentations.rotate(image, rotation))\n",
    "            if transposes:\n",
    "                im = albumentations.hflip(image)\n",
    "                ims.append(albumentations.rotate(im, rotation))\n",
    "    ims = np.array(ims)\n",
    "    ims = torch.tensor(ims).transpose(1, -1).transpose(2, -1).float()\n",
    "    return ims\n",
    "\n",
    "\n",
    "def combine_postprocessing_images(images, rotations, transposes):\n",
    "    \"\"\"Combine predictions of different prospectives\"\"\"\n",
    "    outputs = []\n",
    "    index = 0\n",
    "    while index < len(images):\n",
    "        output = np.zeros(images[0].shape)\n",
    "        for rotation in rotations:\n",
    "            im = images[index, 0]\n",
    "            output += albumentations.rotate(im, -rotation)\n",
    "            index += 1\n",
    "            if transposes:\n",
    "                im = images[index, 0]\n",
    "                im = albumentations.rotate(im, -rotation)\n",
    "                output += albumentations.hflip(im)\n",
    "                index += 1\n",
    "        output = output / len(images)\n",
    "        outputs.append(output)\n",
    "    return np.array(outputs)\n",
    "\n",
    "\n",
    "def create_submission(model_name: str):\n",
    "    model_file_name = os.path.join(\"checkpoints\", model_name + \".pth.maxf1\")\n",
    "    model = torch.load(model_file_name, map_location=torch.device(DEVICE)).to(DEVICE)\n",
    "    model.eval()\n",
    "\n",
    "    # Create the directory to store the predictions\n",
    "    path = \"data/test_set_images\"\n",
    "    pred_path = \"predictions/\" + model_name\n",
    "    if not os.path.exists(pred_path):\n",
    "        os.makedirs(pred_path)\n",
    "\n",
    "    # For each image, apply postprocessing augmentation, make predictions and save predictions\n",
    "    for image in tqdm.tqdm(os.listdir(path)):\n",
    "        img_path = os.path.join(path, image, image + \".png\")\n",
    "        im = np.asarray(Image.open(img_path)) / 255\n",
    "        ims = create_postprocessing_images(\n",
    "            [im], rotations=[0, 90, 180, 270], transposes=True\n",
    "        )\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output = model(ims.to(DEVICE))\n",
    "            predicts = torch.sigmoid(output).cpu().detach()\n",
    "\n",
    "        predict = combine_postprocessing_images(\n",
    "            predicts.numpy(), rotations=[0, 90, 180, 270], transposes=True\n",
    "        ).reshape((608, 608))\n",
    "        predict[predict < 0.5] = 0\n",
    "        predict[predict >= 0.5] = 1\n",
    "        predict *= 255\n",
    "        Image.fromarray(predict).convert(\"L\").save(\n",
    "            os.path.join(pred_path, image) + \".png\"\n",
    "        )\n",
    "\n",
    "    # Generate the submission file\n",
    "    submission_filename = os.path.join(\n",
    "        \"submissions\", \"submission_{}.csv\".format(model_name)\n",
    "    )\n",
    "    image_filenames = []\n",
    "    for i in range(1, 51):\n",
    "        image_filename = pred_path + \"/test_\" + str(i) + \".png\"\n",
    "        image_filenames.append(image_filename)\n",
    "    masks_to_submission(submission_filename, *image_filenames)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:21<00:00,  2.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictions/linknet101/test_1.png test_1.png\n",
      "1\n",
      "predictions/linknet101/test_2.png test_2.png\n",
      "2\n",
      "predictions/linknet101/test_3.png test_3.png\n",
      "3\n",
      "predictions/linknet101/test_4.png test_4.png\n",
      "4\n",
      "predictions/linknet101/test_5.png test_5.png\n",
      "5\n",
      "predictions/linknet101/test_6.png test_6.png\n",
      "6\n",
      "predictions/linknet101/test_7.png test_7.png\n",
      "7\n",
      "predictions/linknet101/test_8.png test_8.png\n",
      "8\n",
      "predictions/linknet101/test_9.png test_9.png\n",
      "9\n",
      "predictions/linknet101/test_10.png test_10.png\n",
      "10\n",
      "predictions/linknet101/test_11.png test_11.png\n",
      "11\n",
      "predictions/linknet101/test_12.png test_12.png\n",
      "12\n",
      "predictions/linknet101/test_13.png test_13.png\n",
      "13\n",
      "predictions/linknet101/test_14.png test_14.png\n",
      "14\n",
      "predictions/linknet101/test_15.png test_15.png\n",
      "15\n",
      "predictions/linknet101/test_16.png test_16.png\n",
      "16\n",
      "predictions/linknet101/test_17.png test_17.png\n",
      "17\n",
      "predictions/linknet101/test_18.png test_18.png\n",
      "18\n",
      "predictions/linknet101/test_19.png test_19.png\n",
      "19\n",
      "predictions/linknet101/test_20.png test_20.png\n",
      "20\n",
      "predictions/linknet101/test_21.png test_21.png\n",
      "21\n",
      "predictions/linknet101/test_22.png test_22.png\n",
      "22\n",
      "predictions/linknet101/test_23.png test_23.png\n",
      "23\n",
      "predictions/linknet101/test_24.png test_24.png\n",
      "24\n",
      "predictions/linknet101/test_25.png test_25.png\n",
      "25\n",
      "predictions/linknet101/test_26.png test_26.png\n",
      "26\n",
      "predictions/linknet101/test_27.png test_27.png\n",
      "27\n",
      "predictions/linknet101/test_28.png test_28.png\n",
      "28\n",
      "predictions/linknet101/test_29.png test_29.png\n",
      "29\n",
      "predictions/linknet101/test_30.png test_30.png\n",
      "30\n",
      "predictions/linknet101/test_31.png test_31.png\n",
      "31\n",
      "predictions/linknet101/test_32.png test_32.png\n",
      "32\n",
      "predictions/linknet101/test_33.png test_33.png\n",
      "33\n",
      "predictions/linknet101/test_34.png test_34.png\n",
      "34\n",
      "predictions/linknet101/test_35.png test_35.png\n",
      "35\n",
      "predictions/linknet101/test_36.png test_36.png\n",
      "36\n",
      "predictions/linknet101/test_37.png test_37.png\n",
      "37\n",
      "predictions/linknet101/test_38.png test_38.png\n",
      "38\n",
      "predictions/linknet101/test_39.png test_39.png\n",
      "39\n",
      "predictions/linknet101/test_40.png test_40.png\n",
      "40\n",
      "predictions/linknet101/test_41.png test_41.png\n",
      "41\n",
      "predictions/linknet101/test_42.png test_42.png\n",
      "42\n",
      "predictions/linknet101/test_43.png test_43.png\n",
      "43\n",
      "predictions/linknet101/test_44.png test_44.png\n",
      "44\n",
      "predictions/linknet101/test_45.png test_45.png\n",
      "45\n",
      "predictions/linknet101/test_46.png test_46.png\n",
      "46\n",
      "predictions/linknet101/test_47.png test_47.png\n",
      "47\n",
      "predictions/linknet101/test_48.png test_48.png\n",
      "48\n",
      "predictions/linknet101/test_49.png test_49.png\n",
      "49\n",
      "predictions/linknet101/test_50.png test_50.png\n",
      "50\n"
     ]
    }
   ],
   "source": [
    "# create_submission(\"unet\")\n",
    "# create_submission(\"linknet18\")\n",
    "# create_submission(\"linknet34\")\n",
    "# create_submission(\"linknet50\")\n",
    "create_submission(\"linknet101\")\n",
    "# create_submission(\"linknet152\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
