{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as tdata\n",
    "import torchvision\n",
    "from torch.nn import functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = \"mps\"\n",
    "torch.manual_seed(127)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data import, exploration and pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RoadsDataset(tdata.Dataset):\n",
    "    root: str\n",
    "    num_images: int\n",
    "    images: list[torch.Tensor]\n",
    "    gt_images: list[torch.Tensor]\n",
    "\n",
    "    def __init__(self, root: str, num_images=20, transform=None, target_transform=None):\n",
    "        self.root = root\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        self.num_images = num_images\n",
    "        assert 10 <= num_images <= 100\n",
    "        self.images = []\n",
    "        self.gt_images = []\n",
    "        for i in range(num_images):\n",
    "            image_path = os.path.join(self.root, \"images/satImage_%.3d.png\" % (i + 1))\n",
    "            self.images.append(\n",
    "                torchvision.io.read_image(image_path).type(torch.float32).to(device)\n",
    "            )\n",
    "            gt_image_path = os.path.join(\n",
    "                self.root, \"groundtruth/satImage_%.3d.png\" % (i + 1)\n",
    "            )\n",
    "            self.gt_images.append(\n",
    "                torch.movedim(\n",
    "                    F.one_hot(\n",
    "                        torch.div(\n",
    "                            torch.squeeze(torchvision.io.read_image(gt_image_path)),\n",
    "                            255,\n",
    "                        ).type(torch.int64),\n",
    "                        2,\n",
    "                    ),\n",
    "                    2,\n",
    "                    0,\n",
    "                ).to(dtype=torch.float32, device=device)\n",
    "            )\n",
    "\n",
    "        print(\"Loaded {} images from {}\".format(num_images, root))\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_images\n",
    "\n",
    "    def __getitem__(self, item: int) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        if self.transform:\n",
    "            image = self.transform(self.images[item])\n",
    "        else:\n",
    "            image = self.images[item]\n",
    "\n",
    "        if self.target_transform:\n",
    "            gt_image = self.target_transform(self.gt_images[item])\n",
    "        else:\n",
    "            gt_image = self.gt_images[item]\n",
    "\n",
    "        return image, gt_image\n",
    "\n",
    "    def convert_gt_to_one_hot(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = RoadsDataset(root=\"data/training\", num_images=30)\n",
    "training_dataloader = tdata.DataLoader(training_data, batch_size=5, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, _ = training_data[0]\n",
    "X = X.to(device=\"cpu\") / 255.0\n",
    "# plt.subplot(121)\n",
    "plt.imshow(torch.movedim(X, 0, 2))\n",
    "# plt.subplot(122)\n",
    "# plt.imshow(torch.movedim(Y,0,2), cmap=\"grayscale\")\n",
    "# plt.tight_layout()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UNet model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoubleConv(nn.Module):\n",
    "    \"\"\"(convolution => [BN] => ReLU) * 2\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.double_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.double_conv(x)\n",
    "\n",
    "\n",
    "class Down(nn.Module):\n",
    "    \"\"\"Downscaling with maxpool then double conv\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.maxpool_conv = nn.Sequential(\n",
    "            nn.MaxPool2d(2), DoubleConv(in_channels, out_channels)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.maxpool_conv(x)\n",
    "\n",
    "\n",
    "class Up(nn.Module):\n",
    "    \"\"\"Upscaling then double conv\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.up = nn.ConvTranspose2d(\n",
    "            in_channels, in_channels // 2, kernel_size=2, stride=2\n",
    "        )\n",
    "        self.conv = DoubleConv(in_channels, out_channels)\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.up(x1)\n",
    "        # input is CHW\n",
    "        # diffY = x2.size()[2] - x1.size()[2]\n",
    "        # diffX = x2.size()[3] - x1.size()[3]\n",
    "        # x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2, diffY // 2, diffY - diffY // 2])\n",
    "        # if you have padding issues, see\n",
    "        # https://github.com/HaiyongJiang/U-Net-Pytorch-Unstructured-Buggy/commit/0e854509c2cea854e247a9c615f175f76fbb2e3a\n",
    "        # https://github.com/xiaopeng-liao/Pytorch-UNet/commit/8ebac70e633bac59fc22bb5195e513d5832fb3bd\n",
    "        x = torch.cat([x2, x1], dim=1)\n",
    "        return self.conv(x)\n",
    "\n",
    "\n",
    "class OutConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(OutConv, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, n_channels, n_classes):\n",
    "        super().__init__()\n",
    "        self.n_channels = n_channels\n",
    "        self.n_classes = n_classes\n",
    "\n",
    "        self.inc = DoubleConv(n_channels, 64)\n",
    "        self.down1 = Down(64, 128)\n",
    "        self.down2 = Down(128, 256)\n",
    "        self.down3 = Down(256, 512)\n",
    "        self.down4 = Down(512, 1024)\n",
    "        self.up1 = Up(1024, 512)\n",
    "        self.up2 = Up(512, 256)\n",
    "        self.up3 = Up(256, 128)\n",
    "        self.up4 = Up(128, 64)\n",
    "        self.outc = OutConv(64, n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.inc(x)\n",
    "        x2 = self.down1(x1)\n",
    "        x3 = self.down2(x2)\n",
    "        x4 = self.down3(x3)\n",
    "        x5 = self.down4(x4)\n",
    "        x = self.up1(x5, x4)\n",
    "        x = self.up2(x, x3)\n",
    "        x = self.up3(x, x2)\n",
    "        x = self.up4(x, x1)\n",
    "        logits = self.outc(x)\n",
    "        return logits\n",
    "\n",
    "\n",
    "unet_model = UNet(n_channels=3, n_classes=2).to(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fun = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(unet_model.parameters(), lr=1e-3)\n",
    "\n",
    "\n",
    "def train(dataloader: tdata.DataLoader, model: nn.Module, loss_fun, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch_num, (X_batch, Y_batch) in enumerate(dataloader):\n",
    "        X_batch, Y_batch = X_batch.to(device), Y_batch.to(device)\n",
    "        # Compute prediction error\n",
    "        pred = model(X_batch)\n",
    "        loss = loss_fun(pred, Y_batch)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loss, current = loss.item(), batch_num * len(X_batch)\n",
    "        print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "\n",
    "epochs = 10\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(training_dataloader, unet_model, loss_fun, optimizer)\n",
    "\n",
    "print(\"Done training!\")\n",
    "torch.save(unet_model.state_dict(), \"unet_model.pth\")\n",
    "print(\"Saved PyTorch Model State to unet_model.pth\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.6 | packaged by conda-forge | (main, Aug 22 2022, 20:38:29) [Clang 13.0.1 ]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9fee673dc4fc01db422c313512934603878a992984f49c83d6d8be337f7be7ab"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
