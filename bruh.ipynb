{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEVICE= cuda\n"
     ]
    }
   ],
   "source": [
    "import _thread\n",
    "import os\n",
    "\n",
    "import albumentations\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.utils.data\n",
    "import torchvision\n",
    "import tqdm\n",
    "from PIL import Image\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from data.mask_to_submission import masks_to_submission\n",
    "from torchvision.transforms.functional import resize\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"DEVICE=\", DEVICE)\n",
    "Train_image_path = \"data/training/images\"\n",
    "Train_mask_path = \"data/training/groundtruth\"\n",
    "Validation_image_path = \"data/validating/images\"\n",
    "Validation_mask_path = \"data/validating/groundtruth\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data processing"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "class RoadDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    The class RoadDataset loads the data and executes the pre-processing operations on it.\n",
    "    More specifically, it re-applies the specified transform every time data is fetched via a dataloader.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        image_path: str,\n",
    "        mask_path: str,\n",
    "        transform,\n",
    "    ):\n",
    "        self.transform = transform\n",
    "        self.images = self.load_images(image_path)\n",
    "        self.masks = self.load_images(mask_path)\n",
    "        self.images_augmented = []\n",
    "        self.masks_augmented = []\n",
    "\n",
    "        # Data augmentation\n",
    "        for i in range(len(self.images)):\n",
    "            output = self.transform(image=self.images[i], mask=self.masks[i])\n",
    "            self.images_augmented.append(output[\"image\"])\n",
    "            self.masks_augmented.append(output[\"mask\"])\n",
    "\n",
    "    def get_images(self):\n",
    "        return self.images, self.masks\n",
    "\n",
    "    @staticmethod\n",
    "    def load_images(image_path):\n",
    "        \"\"\"This method loads the images from the given path\"\"\"\n",
    "        images = []\n",
    "        for img in os.listdir(image_path):\n",
    "            path = os.path.join(image_path, img)\n",
    "            image = Image.open(path)\n",
    "            images.append(np.asarray(image))\n",
    "            # images.append(cv2.imread(path))\n",
    "        return np.asarray(images)\n",
    "\n",
    "    def augment(self, index):\n",
    "        \"\"\"This method applies data augmentation to the images\"\"\"\n",
    "        output = self.transform(image=self.images[index], mask=self.masks[index])\n",
    "        self.images_augmented[index] = output[\"image\"]\n",
    "        self.masks_augmented[index] = output[\"mask\"]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"This method returns the image at a certain position and its mask\"\"\"\n",
    "        image = self.images_augmented[index]\n",
    "        mask = self.masks_augmented[index]\n",
    "        _thread.start_new_thread(self.augment, (index,))\n",
    "        return (image / 255), (mask.unsqueeze(0) > 100).float()\n",
    "\n",
    "\n",
    "def get_loader(\n",
    "    data_path: str,\n",
    "    mask_path: str,\n",
    "    transform,\n",
    "    batch_size: int = 4,\n",
    ") -> torch.utils.data.DataLoader:\n",
    "    \"\"\"Create the DataLoader class\"\"\"\n",
    "    dataset = RoadDataset(\n",
    "        data_path,\n",
    "        mask_path,\n",
    "        transform,\n",
    "    )\n",
    "    return torch.utils.data.DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        pin_memory=True,\n",
    "        generator=torch.Generator().manual_seed(127),\n",
    "    )\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Models"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "# ================================================================================================\n",
    "# UNET\n",
    "# ================================================================================================\n",
    "\n",
    "\n",
    "class DoubleConv(torch.nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.double_conv = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(\n",
    "                in_channels, out_channels, kernel_size=3, padding=1, bias=False\n",
    "            ),\n",
    "            torch.nn.BatchNorm2d(out_channels),\n",
    "            torch.nn.ReLU(inplace=True),\n",
    "            torch.nn.Conv2d(\n",
    "                out_channels, out_channels, kernel_size=3, padding=1, bias=False\n",
    "            ),\n",
    "            torch.nn.BatchNorm2d(out_channels),\n",
    "            torch.nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        return self.double_conv(x)\n",
    "\n",
    "\n",
    "class Down(torch.nn.Module):\n",
    "    \"\"\"Downscaling with maxpool then double conv\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.maxpool_conv = torch.nn.Sequential(\n",
    "            torch.nn.MaxPool2d(2), DoubleConv(in_channels, out_channels)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.maxpool_conv(x)\n",
    "\n",
    "\n",
    "class Up(torch.nn.Module):\n",
    "    \"\"\"Upscaling then double conv\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.up = torch.nn.ConvTranspose2d(\n",
    "            in_channels, in_channels // 2, kernel_size=2, stride=2\n",
    "        )\n",
    "        self.conv = DoubleConv(in_channels, out_channels)\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.up(x1)\n",
    "        x = torch.cat([x2, x1], dim=1)\n",
    "        del x1\n",
    "        del x2\n",
    "        torch.cuda.empty_cache()\n",
    "        return self.conv(x)\n",
    "\n",
    "\n",
    "class UNet(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.inconv = DoubleConv(3, 64)\n",
    "        self.down1 = Down(64, 128)\n",
    "        self.down2 = Down(128, 256)\n",
    "        self.down3 = Down(256, 512)\n",
    "        self.down4 = Down(512, 1024)\n",
    "        self.up1 = Up(1024, 512)\n",
    "        self.up2 = Up(512, 256)\n",
    "        self.up3 = Up(256, 128)\n",
    "        self.up4 = Up(128, 64)\n",
    "        self.outconv = torch.nn.Conv2d(64, 1, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.inconv(x)\n",
    "        x2 = self.down1(x1)\n",
    "        x3 = self.down2(x2)\n",
    "        x4 = self.down3(x3)\n",
    "        x5 = self.down4(x4)\n",
    "        x = self.up1(x5, x4)\n",
    "        x = self.up2(x, x3)\n",
    "        x = self.up3(x, x2)\n",
    "        x = self.up4(x, x1)\n",
    "        logits = self.outconv(x)\n",
    "        torch.cuda.empty_cache()\n",
    "        return logits\n",
    "\n",
    "\n",
    "# ================================================================================================\n",
    "# LINKNET\n",
    "# ================================================================================================\n",
    "\n",
    "\n",
    "class LinkNetDecoderBlock(torch.nn.Module):\n",
    "    def __init__(self, in_channel, out_channel):\n",
    "        super(LinkNetDecoderBlock, self).__init__()\n",
    "        self.conv1 = torch.nn.Conv2d(in_channel, in_channel // 4, kernel_size=1)\n",
    "        self.bn1 = torch.nn.BatchNorm2d(in_channel // 4)\n",
    "        self.up = torch.nn.ConvTranspose2d(\n",
    "            in_channel // 4,\n",
    "            in_channel // 4,\n",
    "            kernel_size=3,\n",
    "            stride=2,\n",
    "            padding=1,\n",
    "            output_padding=1,\n",
    "        )\n",
    "        self.bn2 = torch.nn.BatchNorm2d(in_channel // 4)\n",
    "        self.conv2 = torch.nn.Conv2d(in_channel // 4, out_channel, kernel_size=1)\n",
    "        self.bn3 = torch.nn.BatchNorm2d(out_channel)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = torch.nn.functional.relu(x)\n",
    "        x = self.up(x)\n",
    "        x = self.bn2(x)\n",
    "        x = torch.nn.functional.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn3(x)\n",
    "        x = torch.nn.functional.relu(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class LinkNet(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        encoder,\n",
    "        channels=(64, 128, 256, 512),\n",
    "    ):\n",
    "        super().__init__()\n",
    "        assert len(channels) == 4\n",
    "        self.conv1 = encoder.conv1\n",
    "        self.bn1 = encoder.bn1\n",
    "        self.maxpool1 = encoder.maxpool\n",
    "        self.encoders = torch.nn.ModuleList()\n",
    "        self.encoders.append(encoder.layer1)\n",
    "        self.encoders.append(encoder.layer2)\n",
    "        self.encoders.append(encoder.layer3)\n",
    "        self.encoders.append(encoder.layer4)\n",
    "\n",
    "        self.decoders = torch.nn.ModuleList()\n",
    "        channels = channels[::-1]\n",
    "        for i in range(len(channels) - 1):\n",
    "            self.decoders.append(LinkNetDecoderBlock(channels[i], channels[i + 1]))\n",
    "        self.decoders.append(LinkNetDecoderBlock(channels[-1], channels[-1]))\n",
    "        self.up = torch.nn.ConvTranspose2d(channels[-1], 32, kernel_size=3, stride=2)\n",
    "        self.conv2 = torch.nn.Conv2d(32, 32, kernel_size=2)\n",
    "        self.conv3 = torch.nn.Conv2d(32, 1, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        xs = []\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = torch.nn.functional.relu(x)\n",
    "        x = self.maxpool1(x)\n",
    "        for enc in self.encoders:\n",
    "            x = enc(x)\n",
    "            xs.append(x)\n",
    "        xs = xs[::-1]\n",
    "        for i in range(3):\n",
    "            x = self.decoders[i](x)\n",
    "            if x.shape[2:] != xs[i + 1].shape[2:]:\n",
    "                x = resize(x, xs[i + 1].shape[2:])\n",
    "            x = x + xs[i + 1]\n",
    "        x = self.decoders[3](x)\n",
    "\n",
    "        x = self.up(x)\n",
    "        x = torch.nn.functional.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = torch.nn.functional.relu(x)\n",
    "        x = self.conv3(x)\n",
    "        return x\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Training"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "def f1(pred, label):\n",
    "    \"\"\"Compute F1 score\"\"\"\n",
    "    pred = pred.view(-1)\n",
    "    label = label.view(-1)\n",
    "    tp = (label * pred).sum().to(torch.float32)\n",
    "    # tn = ((1 - label) * (1 - pred)).sum().to(torch.float32)\n",
    "    fp = ((1 - label) * pred).sum().to(torch.float32)\n",
    "    fn = (label * (1 - pred)).sum().to(torch.float32)\n",
    "    eps = 1e-7\n",
    "    precision = tp / (tp + fp + eps)\n",
    "    recall = tp / (tp + fn + eps)\n",
    "    return 2 * precision * recall / (precision + recall + eps), precision, recall\n",
    "\n",
    "\n",
    "def compute_metrics(loader, model, device):\n",
    "    \"\"\"Compute the accuracy rate on the given dataset with the input model\"\"\"\n",
    "    model.eval()\n",
    "    log = dict()\n",
    "    num_correct = 0\n",
    "    num_pixels = 0\n",
    "    f1_score = 0\n",
    "    precision = 0\n",
    "    recall = 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            output = model(x)\n",
    "            output = output[:, -1, :, :].unsqueeze(1)\n",
    "            pred: torch.Tensor = (torch.sigmoid(output) >= 0.5).float()\n",
    "            num_correct += torch.sum(pred == y).item()\n",
    "            num_pixels += torch.numel(pred)\n",
    "            a, b, c = f1(pred, y)\n",
    "            f1_score += a.item()\n",
    "            precision += b.item()\n",
    "            recall += c.item()\n",
    "\n",
    "    log[\"acc\"] = num_correct / num_pixels * 100\n",
    "    log[\"f1 score\"] = f1_score / len(loader) * 100\n",
    "    log[\"precision\"] = precision / len(loader) * 100\n",
    "    log[\"recall\"] = recall / len(loader) * 100\n",
    "    model.train()\n",
    "    return log\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "def epoch(model, loader, optimizer, criterion, scaler):\n",
    "    \"\"\"Train the model\"\"\"\n",
    "    acc_loss = 0\n",
    "    for data, target in loader:\n",
    "        data = data.to(DEVICE)\n",
    "        target = target.to(DEVICE)\n",
    "        with torch.cuda.amp.autocast():\n",
    "            output = model(data)\n",
    "            loss = 0\n",
    "            for i in range(output.shape[1]):\n",
    "                pred = output[:, i, :, :].unsqueeze(1)\n",
    "                loss += criterion(pred, target)\n",
    "\n",
    "            acc_loss += loss.item()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "    return acc_loss\n",
    "\n",
    "\n",
    "def train(\n",
    "    model,\n",
    "    model_name,\n",
    "    train_loader,\n",
    "    validation_loader,\n",
    "    lr: float = 1.0e-4,\n",
    "    epochs: int = 10,\n",
    "):\n",
    "    log_file_name = os.path.relpath(os.path.join(\"logs\", model_name + \".csv\"))\n",
    "    with open(log_file_name, \"w\") as f:\n",
    "        f.write(\"epoch,loss,f1,iou,accuracy,precision,recall\\n\")\n",
    "\n",
    "    model_file_name = os.path.join(\"checkpoints\", model_name + \".pth\")\n",
    "\n",
    "    # Define the criterion and optimizer\n",
    "    criterion = torch.nn.BCEWithLogitsLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    # Define the scheduler and scaler\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=50)\n",
    "    scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "    # Define the data augmentation used for the training set, and also create the data loader for it\n",
    "\n",
    "    # Train the model, then save the training logs and the best model\n",
    "    loop = tqdm.tqdm(range(epochs))\n",
    "    max_f1 = 0\n",
    "    for e in loop:\n",
    "        loss = epoch(model, train_loader, optimizer, criterion, scaler)\n",
    "        scheduler.step()\n",
    "        metrics = compute_metrics(validation_loader, model, DEVICE)\n",
    "        if metrics[\"f1 score\"] > max_f1:\n",
    "            max_f1 = metrics[\"f1 score\"]\n",
    "            if max_f1 > 80.0:\n",
    "                torch.save(model, model_file_name + \".maxf1\")\n",
    "\n",
    "        with open(log_file_name, \"a\") as f:\n",
    "            f.write(\n",
    "                \"{},{},{},{},{},{},{}\\n\".format(\n",
    "                    e,\n",
    "                    loss,\n",
    "                    metrics[\"f1 score\"],\n",
    "                    0,\n",
    "                    metrics[\"acc\"],\n",
    "                    metrics[\"precision\"],\n",
    "                    metrics[\"recall\"],\n",
    "                )\n",
    "            )\n",
    "\n",
    "        loop.set_postfix(loss=loss, f1_score=metrics[\"f1 score\"], max_f1=max_f1)\n",
    "\n",
    "    # Save the logs into a file\n",
    "    torch.save(model, model_file_name)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "train_transform = albumentations.Compose(\n",
    "    [\n",
    "        albumentations.Flip(),\n",
    "        albumentations.Transpose(),\n",
    "        albumentations.Rotate(),\n",
    "        albumentations.CoarseDropout(max_holes=8, max_height=8, max_width=8),\n",
    "        albumentations.OneOf(\n",
    "            [\n",
    "                albumentations.OpticalDistortion(),\n",
    "                albumentations.GridDistortion(),\n",
    "                albumentations.ElasticTransform(),\n",
    "            ],\n",
    "            p=0.5,\n",
    "        ),\n",
    "        ToTensorV2(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "train_loader = get_loader(\n",
    "    data_path=Train_image_path,\n",
    "    mask_path=Train_mask_path,\n",
    "    transform=train_transform,\n",
    "    batch_size=4,\n",
    ")\n",
    "\n",
    "# Define the data augmentation used for the validation set, and also create the data loader for it\n",
    "val_transform = albumentations.Compose(\n",
    "    [\n",
    "        albumentations.Flip(),\n",
    "        albumentations.Transpose(),\n",
    "        ToTensorV2(),\n",
    "    ]\n",
    ")\n",
    "val_loader = get_loader(\n",
    "    data_path=Validation_image_path,\n",
    "    mask_path=Validation_mask_path,\n",
    "    transform=val_transform,\n",
    "    batch_size=4,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Actual training"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [1:54:06<00:00,  6.85s/it, f1_score=87.2, loss=1.09, max_f1=89.5]\n"
     ]
    }
   ],
   "source": [
    "train(\n",
    "    model=UNet().to(DEVICE),\n",
    "    model_name=\"unet\",\n",
    "    epochs=1000,\n",
    "    train_loader=train_loader,\n",
    "    validation_loader=val_loader,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 168/1000 [18:06<1:28:40,  6.40s/it, f1_score=85.7, loss=1.73, max_f1=87.3]"
     ]
    }
   ],
   "source": [
    "train(\n",
    "    model=LinkNet(\n",
    "        encoder=torchvision.models.resnet18(\n",
    "            weights=torchvision.models.ResNet18_Weights.IMAGENET1K_V1\n",
    "        ),\n",
    "        channels=(64, 128, 256, 512),\n",
    "    ).to(DEVICE),\n",
    "    model_name=\"linknet18\",\n",
    "    epochs=1000,\n",
    "    train_loader=train_loader,\n",
    "    validation_loader=val_loader,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train(\n",
    "    model=LinkNet(\n",
    "        encoder=torchvision.models.resnet34(\n",
    "            weights=torchvision.models.ResNet34_Weights.IMAGENET1K_V1\n",
    "        ),\n",
    "        channels=(64, 128, 256, 512),\n",
    "    ).to(DEVICE),\n",
    "    model_name=\"linknet34\",\n",
    "    epochs=1000,\n",
    "    train_loader=train_loader,\n",
    "    validation_loader=val_loader,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train(\n",
    "    model=LinkNet(\n",
    "        encoder=torchvision.models.resnet50(\n",
    "            weights=torchvision.models.ResNet50_Weights.IMAGENET1K_V2\n",
    "        ),\n",
    "        channels=(256, 512, 1024, 2048),\n",
    "    ).to(DEVICE),\n",
    "    model_name=\"linknet50\",\n",
    "    epochs=1000,\n",
    "    train_loader=train_loader,\n",
    "    validation_loader=val_loader,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train(\n",
    "    model=LinkNet(\n",
    "        encoder=torchvision.models.resnet101(\n",
    "            weights=torchvision.models.ResNet101_Weights.IMAGENET1K_V2\n",
    "        ),\n",
    "        channels=(256, 512, 1024, 2048),\n",
    "    ).to(DEVICE),\n",
    "    model_name=\"linknet101\",\n",
    "    epochs=1000,\n",
    "    train_loader=train_loader,\n",
    "    validation_loader=val_loader,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train(\n",
    "    model=LinkNet(\n",
    "        encoder=torchvision.models.resnet152(\n",
    "            weights=torchvision.models.ResNet152_Weights.IMAGENET1K_V2\n",
    "        ),\n",
    "        channels=(256, 512, 1024, 2048),\n",
    "    ).to(DEVICE),\n",
    "    model_name=\"linknet152\",\n",
    "    epochs=1000,\n",
    "    train_loader=train_loader,\n",
    "    validation_loader=val_loader,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Create submissions"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "def create_postprocessing_images(images, rotations, transposes):\n",
    "    \"\"\"Apply transformations to the image and return different prospectives\"\"\"\n",
    "    ims = []\n",
    "    for image in images:\n",
    "        for rotation in rotations:\n",
    "            ims.append(albumentations.rotate(image, rotation))\n",
    "            if transposes:\n",
    "                im = albumentations.hflip(image)\n",
    "                ims.append(albumentations.rotate(im, rotation))\n",
    "    ims = np.array(ims)\n",
    "    ims = torch.tensor(ims).transpose(1, -1).transpose(2, -1).float()\n",
    "    return ims\n",
    "\n",
    "\n",
    "def combine_postprocessing_images(images, rotations, transposes):\n",
    "    \"\"\"Combine predictions of different prospectives\"\"\"\n",
    "    outputs = []\n",
    "    index = 0\n",
    "    while index < len(images):\n",
    "        output = np.zeros(images[0].shape)\n",
    "        for rotation in rotations:\n",
    "            im = images[index, 0]\n",
    "            output += albumentations.rotate(im, -rotation)\n",
    "            index += 1\n",
    "            if transposes:\n",
    "                im = images[index, 0]\n",
    "                im = albumentations.rotate(im, -rotation)\n",
    "                output += albumentations.hflip(im)\n",
    "                index += 1\n",
    "        output = output / len(images)\n",
    "        outputs.append(output)\n",
    "    return np.array(outputs)\n",
    "\n",
    "\n",
    "def create_submission(model_name: str):\n",
    "    model_file_name = os.path.join(\"checkpoints\", model_name + \".pth.maxf1\")\n",
    "    model = torch.load(model_file_name).to(DEVICE)\n",
    "    model.eval()\n",
    "\n",
    "    # Create the directory to store the predictions\n",
    "    path = \"data/test_set_images\"\n",
    "    pred_path = \"predictions/\" + model_name\n",
    "    if not os.path.exists(pred_path):\n",
    "        os.makedirs(pred_path)\n",
    "\n",
    "    # For each image, apply postprocessing augmentation, make predictions and save predictions\n",
    "    for image in tqdm.tqdm(os.listdir(path)):\n",
    "        img_path = os.path.join(path, image, image + \".png\")\n",
    "        im = np.asarray(Image.open(img_path)) / 255\n",
    "        ims = create_postprocessing_images(\n",
    "            [im], rotations=[0, 90, 180, 270], transposes=True\n",
    "        )\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output = model(ims.to(DEVICE))\n",
    "            # output = output[:, 0].unsqueeze(1)\n",
    "            predicts = torch.sigmoid(output).cpu().detach()\n",
    "\n",
    "        predict = combine_postprocessing_images(\n",
    "            predicts.numpy(), rotations=[0, 90, 180, 270], transposes=True\n",
    "        ).reshape((608, 608))\n",
    "        predict[predict < 0.5] = 0\n",
    "        predict[predict >= 0.5] = 1\n",
    "        predict *= 255\n",
    "        Image.fromarray(predict).convert(\"L\").save(\n",
    "            os.path.join(pred_path, image) + \".png\"\n",
    "        )\n",
    "\n",
    "    # Generate the submission file\n",
    "    submission_filename = os.path.join(\n",
    "        \"submissions\", \"submission_{}.csv\".format(model_name)\n",
    "    )\n",
    "    image_filenames = []\n",
    "    for i in range(1, 51):\n",
    "        image_filename = pred_path + \"/test_\" + str(i) + \".png\"\n",
    "        image_filenames.append(image_filename)\n",
    "    masks_to_submission(submission_filename, *image_filenames)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:34<00:00,  1.45it/s]\n"
     ]
    }
   ],
   "source": [
    "# create_submission(\"unet\")\n",
    "create_submission(\"resnet18\")\n",
    "create_submission(\"resnet34\")\n",
    "create_submission(\"resnet50\")\n",
    "create_submission(\"resnet101\")\n",
    "create_submission(\"resnet152\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
