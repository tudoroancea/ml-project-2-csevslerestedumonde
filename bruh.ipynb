{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEVICE= cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import albumentations\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "import tqdm\n",
    "from PIL import Image\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "from data.mask_to_submission import masks_to_submission\n",
    "from data_processing import get_loader\n",
    "from models import LinkNet, UNet\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"DEVICE=\", DEVICE)\n",
    "prefix = \"./\"\n",
    "Train_image_path = prefix + \"data/training/images\"\n",
    "Train_mask_path = prefix + \"data/training/groundtruth\"\n",
    "Validation_image_path = prefix + \"data/validating/images\"\n",
    "Validation_mask_path = prefix + \"data/validating/groundtruth\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Training"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "def f1(pred, label):\n",
    "    \"\"\"Compute F1 score\"\"\"\n",
    "    pred = pred.view(-1)\n",
    "    label = label.view(-1)\n",
    "    tp = (label * pred).sum().to(torch.float32)\n",
    "    # tn = ((1 - label) * (1 - pred)).sum().to(torch.float32)\n",
    "    fp = ((1 - label) * pred).sum().to(torch.float32)\n",
    "    fn = (label * (1 - pred)).sum().to(torch.float32)\n",
    "    eps = 1e-7\n",
    "    precision = tp / (tp + fp + eps)\n",
    "    recall = tp / (tp + fn + eps)\n",
    "    return 2 * precision * recall / (precision + recall + eps), precision, recall\n",
    "\n",
    "\n",
    "def compute_metrics(loader, model, device):\n",
    "    \"\"\"Compute the accuracy rate on the given dataset with the input model\"\"\"\n",
    "    model.eval()\n",
    "    log = dict()\n",
    "    num_correct = 0\n",
    "    num_pixels = 0\n",
    "    f1_score = 0\n",
    "    precision = 0\n",
    "    recall = 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            output = model(x)\n",
    "            output = output[:, -1, :, :].unsqueeze(1)\n",
    "            pred: torch.Tensor = (torch.sigmoid(output) >= 0.5).float()\n",
    "            num_correct += torch.sum(pred == y).item()\n",
    "            num_pixels += torch.numel(pred)\n",
    "            a, b, c = f1(pred, y)\n",
    "            f1_score += a.item()\n",
    "            precision += b.item()\n",
    "            recall += c.item()\n",
    "\n",
    "    log[\"acc\"] = num_correct / num_pixels * 100\n",
    "    log[\"f1 score\"] = f1_score / len(loader) * 100\n",
    "    log[\"precision\"] = precision / len(loader) * 100\n",
    "    log[\"recall\"] = recall / len(loader) * 100\n",
    "    model.train()\n",
    "    return log\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "def epoch(model, loader, optimizer, criterion, scaler):\n",
    "    \"\"\"Train the model\"\"\"\n",
    "    acc_loss = 0\n",
    "    for data, target in loader:\n",
    "        data = data.to(DEVICE)\n",
    "        target = target.to(DEVICE)\n",
    "        with torch.cuda.amp.autocast():\n",
    "            output = model(data)\n",
    "            loss = 0\n",
    "            for i in range(output.shape[1]):\n",
    "                pred = output[:, i, :, :].unsqueeze(1)\n",
    "                loss += criterion(pred, target)\n",
    "\n",
    "            acc_loss += loss.item()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "    return acc_loss\n",
    "\n",
    "\n",
    "def train(\n",
    "    model,\n",
    "    model_name,\n",
    "    train_loader,\n",
    "    validation_loader,\n",
    "    lr: float = 1.0e-4,\n",
    "    epochs: int = 10,\n",
    "):\n",
    "    log_file_name = os.path.join(\"logs\", model_name)\n",
    "    with open(log_file_name, \"w\") as f:\n",
    "        f.write(\"epoch,loss,f1,iou,accuracy,precision,recall\")\n",
    "\n",
    "    model_file_name = os.path.join(\"checkpoints\", model_name + \".pth\")\n",
    "\n",
    "    # Define the criterion and optimizer\n",
    "    criterion = torch.nn.BCEWithLogitsLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    # Define the scheduler and scaler\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=50)\n",
    "    scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "    # Define the data augmentation used for the training set, and also create the data loader for it\n",
    "\n",
    "    # Train the model, then save the training logs and the best model\n",
    "    loop = tqdm.tqdm(range(epochs))\n",
    "    max_f1 = 0\n",
    "    for e in loop:\n",
    "        loss = epoch(model, train_loader, optimizer, criterion, scaler)\n",
    "        scheduler.step()\n",
    "        metrics = compute_metrics(validation_loader, model, DEVICE)\n",
    "        if metrics[\"f1 score\"] > max_f1:\n",
    "            max_f1 = metrics[\"f1 score\"]\n",
    "            if max_f1 > 80.0:\n",
    "                torch.save(model, model_file_name + \".maxf1\")\n",
    "\n",
    "        with open(log_file_name, \"a\") as f:\n",
    "            f.write(\n",
    "                \"{},{},{},{},{},{},{}\".format(\n",
    "                    e,\n",
    "                    loss,\n",
    "                    metrics[\"f1 score\"],\n",
    "                    0,\n",
    "                    metrics[\"acc\"],\n",
    "                    metrics[\"precision\"],\n",
    "                    metrics[\"recall\"],\n",
    "                )\n",
    "            )\n",
    "\n",
    "        loop.set_postfix(loss=loss, f1_score=metrics[\"f1 score\"], max_f1=max_f1)\n",
    "\n",
    "    # Save the logs into a file\n",
    "    torch.save(model, model_file_name)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_transform = albumentations.Compose(\n",
    "    [\n",
    "        albumentations.Flip(),\n",
    "        albumentations.Transpose(),\n",
    "        albumentations.Rotate(),\n",
    "        albumentations.CoarseDropout(max_holes=8, max_height=8, max_width=8),\n",
    "        albumentations.OneOf(\n",
    "            [\n",
    "                albumentations.OpticalDistortion(),\n",
    "                albumentations.GridDistortion(),\n",
    "                albumentations.ElasticTransform(),\n",
    "            ],\n",
    "            p=0.5,\n",
    "        ),\n",
    "        ToTensorV2(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "train_loader = get_loader(\n",
    "    data_path=Train_image_path,\n",
    "    mask_path=Train_mask_path,\n",
    "    transform=train_transform,\n",
    "    batch_size=4,\n",
    ")\n",
    "\n",
    "# Define the data augmentation used for the validation set, and also create the data loader for it\n",
    "val_transform = albumentations.Compose(\n",
    "    [\n",
    "        albumentations.Flip(),\n",
    "        albumentations.Transpose(),\n",
    "        ToTensorV2(),\n",
    "    ]\n",
    ")\n",
    "val_loader = get_loader(\n",
    "    data_path=Validation_image_path,\n",
    "    mask_path=Validation_mask_path,\n",
    "    transform=val_transform,\n",
    "    batch_size=4,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Actual training"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train(\n",
    "    model=UNet(),\n",
    "    model_name=\"unet\",\n",
    "    epochs=1000,\n",
    "    train_loader=train_loader,\n",
    "    validation_loader=val_loader,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train(\n",
    "    model=LinkNet(\n",
    "        encoder=torchvision.models.resnet18(\n",
    "            weights=torchvision.models.ResNet18_Weights.IMAGENET1K_V1\n",
    "        ),\n",
    "        channels=(64, 128, 256, 512),\n",
    "    ),\n",
    "    model_name=\"linknet18\",\n",
    "    epochs=1000,\n",
    "    train_loader=train_loader,\n",
    "    validation_loader=val_loader,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train(\n",
    "    model=LinkNet(\n",
    "        encoder=torchvision.models.resnet34(\n",
    "            weights=torchvision.models.ResNet34_Weights.IMAGENET1K_V1\n",
    "        ),\n",
    "        channels=(64, 128, 256, 512),\n",
    "    ),\n",
    "    model_name=\"linknet34\",\n",
    "    epochs=1000,\n",
    "    train_loader=train_loader,\n",
    "    validation_loader=val_loader,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train(\n",
    "    model=LinkNet(\n",
    "        encoder=torchvision.models.resnet50(\n",
    "            weights=torchvision.models.ResNet50_Weights.IMAGENET1K_V2\n",
    "        ),\n",
    "        channels=(256, 512, 1024, 2048),\n",
    "    ),\n",
    "    model_name=\"linknet50\",\n",
    "    epochs=1000,\n",
    "    train_loader=train_loader,\n",
    "    validation_loader=val_loader,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train(\n",
    "    model=LinkNet(\n",
    "        encoder=torchvision.models.resnet101(\n",
    "            weights=torchvision.models.ResNet101_Weights.IMAGENET1K_V2\n",
    "        ),\n",
    "        channels=(256, 512, 1024, 2048),\n",
    "    ),\n",
    "    model_name=\"linknet101\",\n",
    "    epochs=1000,\n",
    "    train_loader=train_loader,\n",
    "    validation_loader=val_loader,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train(\n",
    "    model=LinkNet(\n",
    "        encoder=torchvision.models.resnet152(\n",
    "            weights=torchvision.models.ResNet152_Weights.IMAGENET1K_V2\n",
    "        ),\n",
    "        channels=(256, 512, 1024, 2048),\n",
    "    ),\n",
    "    model_name=\"linknet152\",\n",
    "    epochs=1000,\n",
    "    train_loader=train_loader,\n",
    "    validation_loader=val_loader,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Create submissions"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "def create_postprocessing_images(images, rotations, transposes):\n",
    "    \"\"\"Apply transformations to the image and return different prospectives\"\"\"\n",
    "    ims = []\n",
    "    for image in images:\n",
    "        for rotation in rotations:\n",
    "            ims.append(albumentations.rotate(image, rotation))\n",
    "            if transposes:\n",
    "                im = albumentations.hflip(image)\n",
    "                ims.append(albumentations.rotate(im, rotation))\n",
    "    ims = np.array(ims)\n",
    "    ims = torch.tensor(ims).transpose(1, -1).transpose(2, -1).float()\n",
    "    return ims\n",
    "\n",
    "\n",
    "def combine_postprocessing_images(images, rotations, transposes):\n",
    "    \"\"\"Combine predictions of different prospectives\"\"\"\n",
    "    outputs = []\n",
    "    index = 0\n",
    "    while index < len(images):\n",
    "        output = np.zeros(images[0].shape)\n",
    "        for rotation in rotations:\n",
    "            im = images[index, 0]\n",
    "            output += albumentations.rotate(im, -rotation)\n",
    "            index += 1\n",
    "            if transposes:\n",
    "                im = images[index, 0]\n",
    "                im = albumentations.rotate(im, -rotation)\n",
    "                output += albumentations.hflip(im)\n",
    "                index += 1\n",
    "        output = output / len(images)\n",
    "        outputs.append(output)\n",
    "    return np.array(outputs)\n",
    "\n",
    "\n",
    "def create_submission(model_name: str):\n",
    "    model_file_name = os.path.join(\"checkpoints\", model_name + \".pth.maxf1\")\n",
    "    model = torch.load(model_file_name).to(DEVICE)\n",
    "    model.eval()\n",
    "\n",
    "    # Create the directory to store the predictions\n",
    "    path = \"data/test_set_images\"\n",
    "    pred_path = \"predictions/\" + model_name\n",
    "    if not os.path.exists(pred_path):\n",
    "        os.makedirs(pred_path)\n",
    "\n",
    "    # For each image, apply postprocessing augmentation, make predictions and save predictions\n",
    "    for image in tqdm.tqdm(os.listdir(path)):\n",
    "        img_path = os.path.join(path, image, image + \".png\")\n",
    "        im = np.asarray(Image.open(img_path)) / 255\n",
    "        ims = create_postprocessing_images(\n",
    "            [im], rotations=[0, 90, 180, 270], transposes=True\n",
    "        )\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output = model(ims.to(DEVICE))\n",
    "            # output = output[:, 0].unsqueeze(1)\n",
    "            predicts = torch.sigmoid(output).cpu().detach()\n",
    "\n",
    "        predict = combine_postprocessing_images(\n",
    "            predicts.numpy(), rotations=[0, 90, 180, 270], transposes=True\n",
    "        ).reshape((608, 608))\n",
    "        predict[predict < 0.5] = 0\n",
    "        predict[predict >= 0.5] = 1\n",
    "        predict *= 255\n",
    "        Image.fromarray(predict).convert(\"L\").save(\n",
    "            os.path.join(pred_path, image) + \".png\"\n",
    "        )\n",
    "\n",
    "    # Generate the submission file\n",
    "    submission_filename = \"submission_{}.csv\".format(model_name[:-4])\n",
    "    image_filenames = []\n",
    "    for i in range(1, 51):\n",
    "        image_filename = pred_path + \"/test_\" + str(i) + \".png\"\n",
    "        image_filenames.append(image_filename)\n",
    "    masks_to_submission(submission_filename, *image_filenames)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
