{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 26,
            "outputs": [
                {
                    "data": {"text/plain": "<torch._C.Generator at 0x1176d8110>"},
                    "execution_count": 26,
                    "metadata": {},
                    "output_type": "execute_result",
                }
            ],
            "source": [
                "# %matplotlib inline\n",
                "import os\n",
                "\n",
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.utils.data as tdata\n",
                "import torchvision\n",
                "from torch.nn import functional as F\n",
                "\n",
                'device = "mps"\n',
                "torch.manual_seed(127)",
            ],
            "metadata": {"collapsed": false},
        },
        {
            "cell_type": "code",
            "execution_count": 27,
            "outputs": [],
            "source": [
                "# Dataset creation\n",
                "class RoadsDataset(tdata.Dataset):\n",
                "    root: str\n",
                "    num_images: int\n",
                "    images: list[torch.Tensor]\n",
                "    gt_images: list[torch.Tensor]\n",
                "    # transform: torchvision.transforms.Tran\n",
                "\n",
                "    def __init__(self, root: str, num_images=20, transform=None, target_transform=None):\n",
                "        self.root = root\n",
                "        self.transform = transform\n",
                "        self.target_transform = target_transform\n",
                "        self.num_images = num_images\n",
                "        assert 10 <= num_images <= 100\n",
                "        self.images = []\n",
                "        self.gt_images = []\n",
                "        for i in range(num_images):\n",
                '            image_path = os.path.join(self.root, "images/satImage_%.3d.png" % (i + 1))\n',
                "            self.images.append(torchvision.io.read_image(image_path).to(device))\n",
                "            gt_image_path = os.path.join(\n",
                '                self.root, "groundtruth/satImage_%.3d.png" % (i + 1)\n',
                "            )\n",
                "            self.gt_images.append(torchvision.io.read_image(gt_image_path).to(device))\n",
                "\n",
                '        print("Loaded {} images from {}".format(num_images, root))\n',
                "\n",
                "    def __len__(self):\n",
                "        return self.num_images\n",
                "\n",
                "    def __getitem__(self, item: int) -> tuple[torch.Tensor, torch.Tensor]:\n",
                "        if self.transform:\n",
                "            image = self.transform(self.images[item])\n",
                "        else:\n",
                "            image = self.images[item]\n",
                "\n",
                "        if self.target_transform:\n",
                "            gt_image = self.target_transform(self.gt_images[item])\n",
                "        else:\n",
                "            gt_image = self.gt_images[item]\n",
                "\n",
                "        return image, gt_image\n",
                "\n",
            ],
            "metadata": {"collapsed": false},
        },
        {
            "cell_type": "code",
            "execution_count": 28,
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": ["Loaded 10 images from data/training\n"],
                }
            ],
            "source": [
                'training_data = RoadsDataset(root="data/training", num_images=10)\n',
                "training_dataloader = tdata.DataLoader(training_data, batch_size=5, shuffle=True)",
            ],
            "metadata": {"collapsed": false},
        },
        {
            "cell_type": "code",
            "execution_count": 31,
            "outputs": [],
            "source": [
                "# U-Net implementation\n",
                "class Block(nn.Module):\n",
                "    def __init__(self, in_ch, out_ch):\n",
                "        super().__init__()\n",
                "        self.conv1 = nn.Conv2d(in_ch, out_ch, 3)\n",
                "        self.relu = nn.ReLU()\n",
                "        self.conv2 = nn.Conv2d(out_ch, out_ch, 3)\n",
                "\n",
                "    def forward(self, x):\n",
                "        return self.conv2(self.relu(self.conv1(x)))\n",
                "\n",
                "\n",
                "class Encoder(nn.Module):\n",
                "    def __init__(self, chs=(3, 64, 128, 256, 512, 1024)):\n",
                "        super().__init__()\n",
                "        self.enc_blocks = nn.ModuleList(\n",
                "            [Block(chs[i], chs[i + 1]) for i in range(len(chs) - 1)]\n",
                "        )\n",
                "        self.pool = nn.MaxPool2d(2)\n",
                "\n",
                "    def forward(self, x):\n",
                "        ftrs = []\n",
                "        for block in self.enc_blocks:\n",
                "            x = block(x)\n",
                "            ftrs.append(x)\n",
                "            x = self.pool(x)\n",
                "        return ftrs\n",
                "\n",
                "\n",
                "class Decoder(nn.Module):\n",
                "    def __init__(self, chs=(1024, 512, 256, 128, 64)):\n",
                "        super().__init__()\n",
                "        self.chs = chs\n",
                "        self.upconvs = nn.ModuleList(\n",
                "            [nn.ConvTranspose2d(chs[i], chs[i + 1], 2, 2) for i in range(len(chs) - 1)]\n",
                "        )\n",
                "        self.dec_blocks = nn.ModuleList(\n",
                "            [Block(chs[i], chs[i + 1]) for i in range(len(chs) - 1)]\n",
                "        )\n",
                "\n",
                "    def forward(self, x, encoder_features):\n",
                "        for i in range(len(self.chs) - 1):\n",
                "            x = self.upconvs[i](x)\n",
                "            enc_ftrs = self.crop(encoder_features[i], x)\n",
                "            x = torch.cat([x, enc_ftrs], dim=1)\n",
                "            x = self.dec_blocks[i](x)\n",
                "        return x\n",
                "\n",
                "    def crop(self, enc_ftrs, x):\n",
                "        _, _, H, W = x.shape\n",
                "        enc_ftrs = torchvision.transforms.CenterCrop([H, W])(enc_ftrs)\n",
                "        return enc_ftrs\n",
                "\n",
                "\n",
                "class UNet(nn.Module):\n",
                "    def __init__(\n",
                "        self,\n",
                "        enc_chs=(3, 64, 128, 256, 512, 1024),\n",
                "        dec_chs=(1024, 512, 256, 128, 64),\n",
                "        num_class=1,\n",
                "        retain_dim=False,\n",
                "        out_sz=(400, 400),\n",
                "    ):\n",
                "        super().__init__()\n",
                "        self.encoder = Encoder(enc_chs)\n",
                "        self.decoder = Decoder(dec_chs)\n",
                "        self.head = nn.Conv2d(dec_chs[-1], num_class, 1)\n",
                "        self.retain_dim = retain_dim\n",
                "        self.out_sz = out_sz\n",
                "\n",
                "    def forward(self, x):\n",
                "        enc_ftrs = self.encoder(x)\n",
                "        out = self.decoder(enc_ftrs[::-1][0], enc_ftrs[::-1][1:])\n",
                "        out = self.head(out)\n",
                "        if self.retain_dim:\n",
                "            out = F.interpolate(out, self.out_sz)\n",
                "        return out\n",
                "\n",
            ],
            "metadata": {"collapsed": false},
        },
        {
            "cell_type": "code",
            "execution_count": 34,
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": ["Epoch 1\n", "-------------------------------\n"],
                },
                {
                    "ename": "RuntimeError",
                    "evalue": "Input type (MPSByteType) and weight type (MPSFloatType) should be the same",
                    "output_type": "error",
                    "traceback": [
                        "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
                        "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
                        'Cell \u001B[0;32mIn [34], line 43\u001B[0m\n\u001B[1;32m     41\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m t \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(epochs):\n\u001B[1;32m     42\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m"\u001B[39m\u001B[38;5;124mEpoch \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mt\u001B[38;5;241m+\u001B[39m\u001B[38;5;241m1\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m-------------------------------\u001B[39m\u001B[38;5;124m"\u001B[39m)\n\u001B[0;32m---> 43\u001B[0m     \u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtraining_dataloader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43munet_model\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mloss_fun\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     44\u001B[0m     test(training_dataloader, unet_model, loss_fun)\n\u001B[1;32m     45\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m"\u001B[39m\u001B[38;5;124mDone!\u001B[39m\u001B[38;5;124m"\u001B[39m)\n',
                        "Cell \u001B[0;32mIn [34], line 12\u001B[0m, in \u001B[0;36mtrain\u001B[0;34m(dataloader, model, loss_fun, optimizer)\u001B[0m\n\u001B[1;32m     10\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m batch, (X, y) \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(dataloader):\n\u001B[1;32m     11\u001B[0m     X, y \u001B[38;5;241m=\u001B[39m X\u001B[38;5;241m.\u001B[39mto(device), y\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[0;32m---> 12\u001B[0m     pred \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     13\u001B[0m     loss \u001B[38;5;241m=\u001B[39m loss_fun(pred, y)\n\u001B[1;32m     14\u001B[0m     loss\u001B[38;5;241m.\u001B[39mbackwards()\n",
                        "File \u001B[0;32m~/miniforge3/envs/torch_gpu/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1126\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1127\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1128\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1129\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1130\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1131\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1132\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
                        "Cell \u001B[0;32mIn [31], line 72\u001B[0m, in \u001B[0;36mUNet.forward\u001B[0;34m(self, x)\u001B[0m\n\u001B[1;32m     71\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x):\n\u001B[0;32m---> 72\u001B[0m     enc_ftrs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mencoder\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     73\u001B[0m     out \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdecoder(enc_ftrs[::\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m][\u001B[38;5;241m0\u001B[39m], enc_ftrs[::\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m][\u001B[38;5;241m1\u001B[39m:])\n\u001B[1;32m     74\u001B[0m     out \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhead(out)\n",
                        "File \u001B[0;32m~/miniforge3/envs/torch_gpu/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1126\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1127\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1128\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1129\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1130\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1131\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1132\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
                        "Cell \u001B[0;32mIn [31], line 24\u001B[0m, in \u001B[0;36mEncoder.forward\u001B[0;34m(self, x)\u001B[0m\n\u001B[1;32m     22\u001B[0m ftrs \u001B[38;5;241m=\u001B[39m []\n\u001B[1;32m     23\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m block \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39menc_blocks:\n\u001B[0;32m---> 24\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[43mblock\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     25\u001B[0m     ftrs\u001B[38;5;241m.\u001B[39mappend(x)\n\u001B[1;32m     26\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpool(x)\n",
                        "File \u001B[0;32m~/miniforge3/envs/torch_gpu/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1126\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1127\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1128\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1129\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1130\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1131\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1132\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
                        "Cell \u001B[0;32mIn [31], line 10\u001B[0m, in \u001B[0;36mBlock.forward\u001B[0;34m(self, x)\u001B[0m\n\u001B[1;32m      9\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x):\n\u001B[0;32m---> 10\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconv2(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrelu(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconv1\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m))\n",
                        "File \u001B[0;32m~/miniforge3/envs/torch_gpu/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1126\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1127\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1128\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1129\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1130\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1131\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1132\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
                        "File \u001B[0;32m~/miniforge3/envs/torch_gpu/lib/python3.10/site-packages/torch/nn/modules/conv.py:457\u001B[0m, in \u001B[0;36mConv2d.forward\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m    456\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[0;32m--> 457\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_conv_forward\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbias\u001B[49m\u001B[43m)\u001B[49m\n",
                        "File \u001B[0;32m~/miniforge3/envs/torch_gpu/lib/python3.10/site-packages/torch/nn/modules/conv.py:453\u001B[0m, in \u001B[0;36mConv2d._conv_forward\u001B[0;34m(self, input, weight, bias)\u001B[0m\n\u001B[1;32m    449\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpadding_mode \u001B[38;5;241m!=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mzeros\u001B[39m\u001B[38;5;124m'\u001B[39m:\n\u001B[1;32m    450\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m F\u001B[38;5;241m.\u001B[39mconv2d(F\u001B[38;5;241m.\u001B[39mpad(\u001B[38;5;28minput\u001B[39m, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reversed_padding_repeated_twice, mode\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpadding_mode),\n\u001B[1;32m    451\u001B[0m                     weight, bias, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstride,\n\u001B[1;32m    452\u001B[0m                     _pair(\u001B[38;5;241m0\u001B[39m), \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdilation, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgroups)\n\u001B[0;32m--> 453\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconv2d\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbias\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstride\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    454\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpadding\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdilation\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgroups\u001B[49m\u001B[43m)\u001B[49m\n",
                        "\u001B[0;31mRuntimeError\u001B[0m: Input type (MPSByteType) and weight type (MPSFloatType) should be the same",
                    ],
                },
            ],
            "source": [
                "# train Unet on the training set\n",
                "unet_model = UNet().to(device)\n",
                "loss_fun = nn.CrossEntropyLoss()\n",
                "optimizer = torch.optim.Adam(unet_model.parameters(), lr=1e-3)\n",
                "\n",
                "\n",
                "def train(dataloader: tdata.DataLoader, model: nn.Module, loss_fun, optimizer):\n",
                "    size = len(dataloader.dataset)\n",
                "    model.train()\n",
                "    for batch, (X, y) in enumerate(dataloader):\n",
                "        X, y = X.to(device), y.to(device)\n",
                "        pred = model(X)\n",
                "        loss = loss_fun(pred, y)\n",
                "        loss.backwards()\n",
                "        optimizer.step()\n",
                "        optimizer.zero_grad()\n",
                "        if batch % 100 == 0:\n",
                "            loss, current = loss.item(), batch * len(X)\n",
                '            print(f"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]")\n',
                "\n",
                "\n",
                "def test(dataloader: tdata.DataLoader, model, loss_fn):\n",
                "    size = len(dataloader.dataset)\n",
                "    num_batches = len(dataloader)\n",
                "    model.eval()\n",
                "    test_loss, correct = 0, 0\n",
                "    with torch.no_grad():\n",
                "        for X, y in dataloader:\n",
                "            X, y = X.to(device), y.to(device)\n",
                "            pred = model(X)\n",
                "            test_loss += loss_fn(pred, y).item()\n",
                "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
                "    test_loss /= num_batches\n",
                "    correct /= size\n",
                "    print(\n",
                '        f"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n"\n',
                "    )\n",
                "\n",
                "\n",
                "epochs = 5\n",
                "for t in range(epochs):\n",
                '    print(f"Epoch {t+1}\\n-------------------------------")\n',
                "    train(training_dataloader, unet_model, loss_fun, optimizer)\n",
                "    test(training_dataloader, unet_model, loss_fun)\n",
                'print("Done!")\n',
            ],
            "metadata": {"collapsed": false},
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "outputs": [],
            "source": [],
            "metadata": {"collapsed": false},
        },
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3",
        },
        "language_info": {
            "codemirror_mode": {"name": "ipython", "version": 2},
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython2",
            "version": "2.7.6",
        },
    },
    "nbformat": 4,
    "nbformat_minor": 0,
}
