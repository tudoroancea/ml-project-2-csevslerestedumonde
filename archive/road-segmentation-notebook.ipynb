{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"958b279b-3e97-422e-a886-9fb1c79cb7e5","_uuid":"3a593c5f-5641-4767-bf17-cab79121de60","trusted":true},"source":["<a href=\"https://www.kaggle.com/code/tudoroancea/road-segmentation-notebook?scriptVersionId=111171434\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"3f5b08e7-8c40-4560-80ad-0b3dcbf1388b","_uuid":"e54281a5-574b-4893-85e5-569ac641b1c5","collapsed":false,"execution":{"iopub.execute_input":"2022-11-19T13:18:28.445879Z","iopub.status.busy":"2022-11-19T13:18:28.445470Z","iopub.status.idle":"2022-11-19T13:18:28.454674Z","shell.execute_reply":"2022-11-19T13:18:28.453292Z","shell.execute_reply.started":"2022-11-19T13:18:28.445843Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["import os\n","\n","import torch\n","import torch.nn as nn\n","import torch.utils.data as tdata\n","import torchvision\n","from torch.nn import functional as F\n","import matplotlib.pyplot as plt\n","\n","device = \"cuda\"\n","torch.manual_seed(127)"]},{"cell_type":"markdown","metadata":{"_cell_guid":"37ea7640-802d-4ed5-8ae4-855774e964ca","_uuid":"16119d8f-2a0a-4f2e-b6d6-f9b3e8f46023","trusted":true},"source":["# Data import, exploration and pre-processing"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"41f5b809-2be2-4c94-8050-5fd7b5001ad4","_uuid":"bd2a9243-689f-4e2a-97fa-bb5d73f3aea0","collapsed":false,"execution":{"iopub.execute_input":"2022-11-19T13:40:48.749381Z","iopub.status.busy":"2022-11-19T13:40:48.748404Z","iopub.status.idle":"2022-11-19T13:40:48.770545Z","shell.execute_reply":"2022-11-19T13:40:48.769460Z","shell.execute_reply.started":"2022-11-19T13:40:48.749331Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["class RoadsDataset(tdata.Dataset):\n","    root: str\n","    num_images: int\n","    images: list\n","    gt_images: list\n","    gt_images_one_hot: list\n","\n","    def __init__(self, root: str, num_images=20, transform=None, target_transform=None):\n","        self.root = root\n","        self.transform = transform\n","        self.target_transform = target_transform\n","        self.num_images = num_images\n","        assert 10 <= num_images <= 100\n","        self.images = []\n","        self.gt_images = []\n","        self.gt_images_one_hot = []\n","        for i in range(num_images):\n","            image_path = os.path.join(self.root, \"image/image_%.5d.png\" % (i + 1))\n","            img = torchvision.io.read_image(image_path).type(torch.float32).to(device)\n","            img /= 255.0\n","            self.images.append(img)\n","            gt_image_path = os.path.join(\n","                self.root, \"groundtruth/ground_truth_%.5d.png\" % (i + 1)\n","            )\n","            gt_image = torchvision.io.read_image(gt_image_path)\n","            gt_image_one_hot = torch.movedim(\n","                F.one_hot(\n","                    torch.div(\n","                        torch.squeeze(gt_image.to(device)),\n","                        255,\n","                    ).type(torch.int64),\n","                    2,\n","                ),\n","                2,\n","                0,\n","            ).to(dtype=torch.float32)\n","            self.gt_images.append(gt_image)\n","            self.gt_images_one_hot.append(gt_image_one_hot)\n","\n","        print(\"Loaded {} images from {}\".format(num_images, root))\n","\n","    def __len__(self):\n","        return self.num_images\n","\n","    def __getitem__(self, item: int) -> tuple:\n","        if self.transform:\n","            image = self.transform(self.images[item])\n","        else:\n","            image = self.images[item]\n","\n","        if self.target_transform:\n","            gt_image_one_hot = self.target_transform(self.gt_images_one_hot[item])\n","        else:\n","            gt_image_one_hot = self.gt_images_one_hot[item]\n","\n","        return image, gt_image_one_hot"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"1e7785fa-348f-49c5-80b1-a23148a5e910","_uuid":"be8f81ce-f1e1-4a49-a28f-057ce3fd74a9","collapsed":false,"execution":{"iopub.execute_input":"2022-11-19T13:40:54.078929Z","iopub.status.busy":"2022-11-19T13:40:54.078202Z","iopub.status.idle":"2022-11-19T13:40:55.309574Z","shell.execute_reply":"2022-11-19T13:40:55.308434Z","shell.execute_reply.started":"2022-11-19T13:40:54.078890Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["training_data = RoadsDataset(root=\"../input/d/tudoroancea/road-segmentation-dataset/data/train\", num_images=100)\n","training_dataloader = tdata.DataLoader(training_data, batch_size=5, shuffle=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"9fff67e3-61f7-438a-a184-bb347e726700","_uuid":"794e116d-be8e-4d85-97b5-9b6d8982ec5b","collapsed":false,"execution":{"iopub.execute_input":"2022-11-19T13:40:57.630122Z","iopub.status.busy":"2022-11-19T13:40:57.629385Z","iopub.status.idle":"2022-11-19T13:40:58.165537Z","shell.execute_reply":"2022-11-19T13:40:58.164561Z","shell.execute_reply.started":"2022-11-19T13:40:57.630071Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["index = torch.randint(0, len(training_data), (1,)).item()\n","image = training_data.images[index].to(device=\"cpu\")\n","gt_image = training_data.gt_images[index].to(device=\"cpu\")\n","plt.subplot(121)\n","plt.imshow(torch.movedim(image, 0, 2))\n","plt.subplot(122)\n","plt.imshow(torch.movedim(gt_image,0,2), cmap=\"gray\")\n","plt.tight_layout()"]},{"cell_type":"markdown","metadata":{"_cell_guid":"caa938b1-54e6-4777-89e2-3a296173079f","_uuid":"e3f80309-be99-49f2-ae8c-89c158e98da3","trusted":true},"source":["# UNet model"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"d8be5731-84fd-4f26-99db-33bb58fe8323","_uuid":"c5e20a2f-58d4-4610-94b3-3fdded9a396e","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"86b6b02e-84a1-4168-90dd-da28fea4aa7a","_uuid":"e5fe7b1c-29cd-4901-862b-cef8821643fa","collapsed":false,"execution":{"iopub.execute_input":"2022-11-19T13:41:01.420187Z","iopub.status.busy":"2022-11-19T13:41:01.419798Z","iopub.status.idle":"2022-11-19T13:41:01.758327Z","shell.execute_reply":"2022-11-19T13:41:01.757223Z","shell.execute_reply.started":"2022-11-19T13:41:01.420154Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["class DoubleConv(nn.Module):\n","    \"\"\"(convolution => [BN] => ReLU) * 2\"\"\"\n","\n","    def __init__(self, in_channels, out_channels):\n","        super().__init__()\n","        self.double_conv = nn.Sequential(\n","            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, bias=False),\n","            nn.BatchNorm2d(out_channels),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1, bias=False),\n","            nn.BatchNorm2d(out_channels),\n","            nn.ReLU(inplace=True),\n","        )\n","\n","    def forward(self, x: torch.Tensor):\n","        return self.double_conv(x)\n","\n","\n","class Down(nn.Module):\n","    \"\"\"Downscaling with maxpool then double conv\"\"\"\n","\n","    def __init__(self, in_channels, out_channels):\n","        super().__init__()\n","        self.maxpool_conv = nn.Sequential(\n","            nn.MaxPool2d(2), DoubleConv(in_channels, out_channels)\n","        )\n","\n","    def forward(self, x):\n","        return self.maxpool_conv(x)\n","\n","\n","class Up(nn.Module):\n","    \"\"\"Upscaling then double conv\"\"\"\n","\n","    def __init__(self, in_channels, out_channels):\n","        super().__init__()\n","        self.up = nn.ConvTranspose2d(\n","            in_channels, in_channels // 2, kernel_size=2, stride=2\n","        )\n","        self.conv = DoubleConv(in_channels, out_channels)\n","\n","    def forward(self, x1, x2):\n","        x1 = self.up(x1)\n","        x = torch.cat([x2, x1], dim=1)\n","        return self.conv(x)\n","\n","\n","class OutConv(nn.Module):\n","    def __init__(self, in_channels, out_channels):\n","        super(OutConv, self).__init__()\n","        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n","        self.softmax = nn.Softmax(dim=1)\n","\n","    def forward(self, x):\n","        return self.softmax(self.conv(x))\n","\n","\n","class UNet(nn.Module):\n","    def __init__(self, n_channels, n_classes):\n","        super().__init__()\n","        self.n_channels = n_channels\n","        self.n_classes = n_classes\n","\n","        self.inc = DoubleConv(n_channels, 64)\n","        self.down1 = Down(64, 128)\n","        self.down2 = Down(128, 256)\n","        self.down3 = Down(256, 512)\n","        self.down4 = Down(512, 1024)\n","        self.up1 = Up(1024, 512)\n","        self.up2 = Up(512, 256)\n","        self.up3 = Up(256, 128)\n","        self.up4 = Up(128, 64)\n","        self.outc = OutConv(64, n_classes)\n","\n","    def forward(self, x):\n","        x1 = self.inc(x)\n","        x2 = self.down1(x1)\n","        x3 = self.down2(x2)\n","        x4 = self.down3(x3)\n","        x5 = self.down4(x4)\n","        x = self.up1(x5, x4)\n","        x = self.up2(x, x3)\n","        x = self.up3(x, x2)\n","        x = self.up4(x, x1)\n","        logits = self.outc(x)\n","        return logits\n","\n","\n","unet_model = UNet(n_channels=3, n_classes=2).to(device)"]},{"cell_type":"markdown","metadata":{"_cell_guid":"b1fd594b-26c8-4725-86c4-18620df1ec5a","_uuid":"8b0961f6-5c8e-406b-bdd6-7a2783ec25f4","trusted":true},"source":["# Model training"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"c853d65c-7ebb-47a8-a827-436543b69736","_kg_hide-output":true,"_uuid":"a5c61c56-9d47-48bd-b817-20d37248fa15","collapsed":false,"execution":{"iopub.execute_input":"2022-11-19T13:41:03.899504Z","iopub.status.busy":"2022-11-19T13:41:03.898717Z","iopub.status.idle":"2022-11-19T13:47:56.194135Z","shell.execute_reply":"2022-11-19T13:47:56.193046Z","shell.execute_reply.started":"2022-11-19T13:41:03.899466Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["loss_fun = nn.BCELoss()\n","optimizer = torch.optim.Adam(unet_model.parameters(), lr=1e-3)\n","\n","\n","def train(dataloader: tdata.DataLoader, model: nn.Module, loss_fun, optimizer):\n","    size = len(dataloader.dataset)\n","    model.train()\n","    for batch_num, (X_batch, Y_batch) in enumerate(dataloader):\n","        X_batch, Y_batch = X_batch.to(device), Y_batch.to(device)\n","        # Compute prediction error\n","        pred = model(X_batch)\n","        loss = loss_fun(pred, Y_batch)\n","\n","        # Backpropagation\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","        loss, current = loss.item(), (batch_num + 1) * len(X_batch)\n","        print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n","\n","\n","epochs = 20\n","for t in range(epochs):\n","    print(f\"Epoch {t+1}\\n-------------------------------\")\n","    train(training_dataloader, unet_model, loss_fun, optimizer)\n","\n","print(\"Done training!\")\n","torch.save(unet_model.state_dict(), \"unet_model2.pth\")\n","print(\"Saved PyTorch Model State to unet_model2.pth\")"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"75440b6a-8464-4ed2-9560-52ff55f769e9","_uuid":"d2aad876-c481-4880-be14-7f984d922876","collapsed":false,"execution":{"iopub.execute_input":"2022-11-19T13:49:14.533795Z","iopub.status.busy":"2022-11-19T13:49:14.533399Z","iopub.status.idle":"2022-11-19T13:49:15.114575Z","shell.execute_reply":"2022-11-19T13:49:15.113544Z","shell.execute_reply.started":"2022-11-19T13:49:14.533744Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["unet_model = UNet(3, 2).to(device)\n","unet_model.load_state_dict(torch.load(\"unet_model2.pth\"))"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"aab42aa5-81ef-4fbb-bce7-ade699500e43","_uuid":"548944d9-2048-4295-986f-93d771590c83","collapsed":false,"execution":{"iopub.execute_input":"2022-11-19T14:04:11.446746Z","iopub.status.busy":"2022-11-19T14:04:11.446343Z","iopub.status.idle":"2022-11-19T14:04:11.979817Z","shell.execute_reply":"2022-11-19T14:04:11.978805Z","shell.execute_reply.started":"2022-11-19T14:04:11.446712Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["unet_model.eval()\n","index = 0\n","X, Y, Y_one_hot = training_data.images[index], training_data.gt_images[index], training_data.gt_images_one_hot[index]\n","X = X.to(device)\n","Y = Y.to(device)\n","Y_one_hot = Y_one_hot.to(device)\n","with torch.no_grad():\n","    Y_pred = torch.squeeze(unet_model(torch.unsqueeze(X, 0)))\n","    print(Y_one_hot)\n","    print(nn.BCELoss()(Y_pred[0], Y_one_hot[0]))\n","    Y_pred = Y_pred[1,:,:]\n","#     print(Y_pred[0,190:210,:])\n","#     print(Y_pred[1,190:210,:])\n","#     Y_pred = torch.argmax(Y_pred, dim=0)\n","#     print(Y_pred)\n","#     print(torch.max(Y_pred))\n","    Y_pred = torch.unsqueeze(Y_pred, 0)\n","    Y_pred *= 255\n","    print(Y_pred.shape)\n","\n","plt.subplot(1, 3, 1)\n","plt.imshow(torch.movedim(X, 0, 2).to(\"cpu\"))\n","plt.title(\"Input\")\n","plt.subplot(1, 3, 2)\n","plt.imshow(torch.movedim(Y, 0, 2).to(\"cpu\"), cmap=\"gray\")\n","plt.title(\"Ground Truth\")\n","plt.subplot(1, 3, 3)\n","plt.imshow(torch.movedim(Y_pred, 0, 2).to(device=\"cpu\"), cmap=\"gray\")\n","plt.title(\"Prediction\")"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"51ffc1e7-6497-4031-b864-4bba17dd8ea6","_uuid":"b89870d7-a9e3-4103-9a11-299fa8196f2c","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["def train_with_params(lr: float, batch_size):\n","    loss_fun = nn.BCELoss()\n","optimizer = torch.optim.Adam(unet_model.parameters(), lr=1e-3)\n","\n","    size = len(dataloader.dataset)\n","    model.train()\n","    for batch_num, (X_batch, Y_batch) in enumerate(dataloader):\n","        X_batch, Y_batch = X_batch.to(device), Y_batch.to(device)\n","        # Compute prediction error\n","        pred = model(X_batch)\n","        loss = loss_fun(pred, Y_batch)\n","\n","        # Backpropagation\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","        loss, current = loss.item(), (batch_num + 1) * len(X_batch)\n","        print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")"]}],"metadata":{"kernelspec":{"display_name":"Python 3.7.7 ('venv': venv)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.7"},"vscode":{"interpreter":{"hash":"cfb72c651b262ddc6a63c8f30667f80f525b16df8a4704d49859b1049702e631"}}},"nbformat":4,"nbformat_minor":4}
