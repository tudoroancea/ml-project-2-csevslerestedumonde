{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEVICE= cuda\n"
     ]
    }
   ],
   "source": [
    "# =================================================================================\n",
    "# IMPORTS\n",
    "# =================================================================================\n",
    "import _thread\n",
    "import os\n",
    "import re\n",
    "import albumentations\n",
    "import matplotlib.image as mpimg\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.utils.data\n",
    "import torchvision\n",
    "import tqdm\n",
    "from PIL import Image\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from torchvision.transforms.functional import resize\n",
    "import random\n",
    "\n",
    "\n",
    "# =================================================================================\n",
    "# GENERAL PARAMETERS\n",
    "# =================================================================================\n",
    "random.seed(127)\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"DEVICE: \", DEVICE)\n",
    "SUBMISSION_THRESHOLD = 0.25 # Given, DO NOT change it\n",
    "\n",
    "\n",
    "# =================================================================================\n",
    "# PATHS\n",
    "# =================================================================================\n",
    "Train_image_path = \"data/training/images\"\n",
    "Train_mask_path = \"data/training/groundtruth\"\n",
    "Validation_image_path = \"data/validating/images\"\n",
    "Validation_mask_path = \"data/validating/groundtruth\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "class RoadDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    The class RoadDataset loads the data and executes the pre-processing operations on it.\n",
    "    More specifically, it re-applies the specified transform every time data is fetched via a dataloader.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        image_path: str,\n",
    "        mask_path: str,\n",
    "        transform,\n",
    "    ):\n",
    "        # Remember transforms\n",
    "        self.transform = transform\n",
    "\n",
    "        # Load images and masks\n",
    "        self.images = self.load_images(image_path)\n",
    "        self.masks = self.load_images(mask_path)\n",
    "\n",
    "        # Augmented images and masks\n",
    "        self.images_augmented = []\n",
    "        self.masks_augmented = []\n",
    "\n",
    "        # Data augmentation using transforms\n",
    "        for i in range(len(self.images)):\n",
    "            output = self.transform(image=self.images[i], mask=self.masks[i])\n",
    "            self.images_augmented.append(output[\"image\"])\n",
    "            self.masks_augmented.append(output[\"mask\"])\n",
    "\n",
    "    def get_images(self):\n",
    "        return self.images, self.masks\n",
    "\n",
    "    @staticmethod\n",
    "    def load_images(image_path):\n",
    "        \"\"\"This method loads the images from the given path\"\"\"\n",
    "        images = []\n",
    "        for img in os.listdir(image_path):\n",
    "            path = os.path.join(image_path, img)\n",
    "            image = Image.open(path)\n",
    "            images.append(np.asarray(image))\n",
    "\n",
    "        return np.asarray(images)\n",
    "\n",
    "    def augment(self, index):\n",
    "        \"\"\"This method applies data augmentation to the images again to change precedent augmentation transformations\"\"\"\n",
    "        output = self.transform(image=self.images[index], mask=self.masks[index])\n",
    "        self.images_augmented[index] = output[\"image\"]\n",
    "        self.masks_augmented[index] = output[\"mask\"]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"This method returns the image at a certain position and its mask\"\"\"\n",
    "        image = self.images_augmented[index]\n",
    "        mask = self.masks_augmented[index]\n",
    "\n",
    "        # Start a new thread to augment the data (thread level parallelism)\n",
    "        _thread.start_new_thread(self.augment, (index,))\n",
    "\n",
    "        # Return scaled image and mask\n",
    "        return (image / 255), (mask.unsqueeze(0) > 200).float()\n",
    "\n",
    "\n",
    "def get_loader(\n",
    "    data_path: str,\n",
    "    mask_path: str,\n",
    "    transform,\n",
    "    batch_size: int = 4,\n",
    ") -> torch.utils.data.DataLoader:\n",
    "    \"\"\"Create the DataLoader class\"\"\"\n",
    "    # Use our dataset and defined transformations\n",
    "    dataset = RoadDataset(\n",
    "        data_path,\n",
    "        mask_path,\n",
    "        transform,\n",
    "    )\n",
    "\n",
    "    # Use the dataset in the torch dataloader\n",
    "    return torch.utils.data.DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size, # Specify batch size\n",
    "        shuffle=True, # Shuffle the data to avoid learning the order\n",
    "        pin_memory=True, # Copy tensors to CUDA pinned memory\n",
    "        generator=torch.Generator().manual_seed(127), # Set seed for reproducibility\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# ================================================================================================\n",
    "# UNET\n",
    "# ================================================================================================\n",
    "# ------------------------------------------------------------------------------\n",
    "# USEFUL FUNCTIONS\n",
    "# ------------------------------------------------------------------------------\n",
    "class DoubleConv(torch.nn.Module):\n",
    "    \"\"\"Performs a double convolution (used once every up/down)\"\"\"\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.double_conv = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(\n",
    "                in_channels, out_channels, kernel_size=3, padding=1, bias=False\n",
    "            ),\n",
    "            torch.nn.BatchNorm2d(out_channels),\n",
    "            torch.nn.ReLU(inplace=True),\n",
    "            torch.nn.Conv2d(\n",
    "                out_channels, out_channels, kernel_size=3, padding=1, bias=False\n",
    "            ),\n",
    "            torch.nn.BatchNorm2d(out_channels),\n",
    "            torch.nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        return self.double_conv(x)\n",
    "\n",
    "class Down(torch.nn.Module):\n",
    "    \"\"\"Downscaling with maxpool then double conv\"\"\"\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.maxpool_conv = torch.nn.Sequential(\n",
    "            torch.nn.MaxPool2d(2), DoubleConv(in_channels, out_channels)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.maxpool_conv(x)\n",
    "\n",
    "class Up(torch.nn.Module):\n",
    "    \"\"\"Upscaling then double conv\"\"\"\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.up = torch.nn.ConvTranspose2d(\n",
    "            in_channels, in_channels // 2, kernel_size=2, stride=2\n",
    "        )\n",
    "        self.conv = DoubleConv(in_channels, out_channels)\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.up(x1)\n",
    "        x = torch.cat([x2, x1], dim=1)\n",
    "        del x1\n",
    "        del x2\n",
    "        torch.cuda.empty_cache()\n",
    "        return self.conv(x)\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# GENERAL UNET\n",
    "# ------------------------------------------------------------------------------\n",
    "class UNet(torch.nn.Module):\n",
    "    \"\"\"U-Net as presented in the paper \"U-Net: Convolutional Networks for Biomedical Image Segmentation\" (https://arxiv.org/pdf/1505.04597.pdf)\"\"\"\n",
    "    def __init__(self):\n",
    "        # Module init\n",
    "        super().__init__()\n",
    "\n",
    "        # Define layers as presented in the paper \"U-Net: Convolutional Networks for Biomedical\n",
    "        # Image Segmentation\" (https://arxiv.org/pdf/1505.04597.pdf) with batch normalization\n",
    "\n",
    "        # Encoder part\n",
    "        self.inconv = DoubleConv(3, 64)\n",
    "        self.down1 = Down(64, 128)\n",
    "        self.down2 = Down(128, 256)\n",
    "        self.down3 = Down(256, 512)\n",
    "        self.down4 = Down(512, 1024)\n",
    "\n",
    "        # Decoder part\n",
    "        self.up1 = Up(1024, 512)\n",
    "        self.up2 = Up(512, 256)\n",
    "        self.up3 = Up(256, 128)\n",
    "        self.up4 = Up(128, 64)\n",
    "\n",
    "        # Output part: convert to 1 channel (binary)\n",
    "        self.outconv = torch.nn.Conv2d(64, 1, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder part\n",
    "        x1 = self.inconv(x)\n",
    "        x2 = self.down1(x1)\n",
    "        x3 = self.down2(x2)\n",
    "        x4 = self.down3(x3)\n",
    "        x5 = self.down4(x4)\n",
    "\n",
    "        # Decoder part\n",
    "        x = self.up1(x5, x4)\n",
    "        x = self.up2(x, x3)\n",
    "        x = self.up3(x, x2)\n",
    "        x = self.up4(x, x1)\n",
    "\n",
    "        # Output part: binary with help of sigmoid\n",
    "        logits = self.outconv(x)\n",
    "\n",
    "        # Free memory of GPU\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        # Return output\n",
    "        return logits\n",
    "\n",
    "\n",
    "# ================================================================================================\n",
    "# LINKNET\n",
    "# ================================================================================================\n",
    "class LinkNetDecoderBlock(torch.nn.Module):\n",
    "    \"\"\"Decoder block of LinkNet\"\"\"\n",
    "    def __init__(self, in_channel, out_channel):\n",
    "        # Module init\n",
    "        super(LinkNetDecoderBlock, self).__init__()\n",
    "\n",
    "        # Define decoder layers as presented in the paper \"LinkNet: Exploiting Encoder Representations\n",
    "        # for Efficient Semantic Segmentation\" (https://arxiv.org/pdf/1707.03718.pdf)\n",
    "        self.conv1 = torch.nn.Conv2d(in_channel, in_channel // 4, kernel_size=1)\n",
    "        self.bn1 = torch.nn.BatchNorm2d(in_channel // 4)\n",
    "        self.up = torch.nn.ConvTranspose2d(\n",
    "            in_channel // 4,\n",
    "            in_channel // 4,\n",
    "            kernel_size=3, # 3x3 kernel\n",
    "            stride=2, # downsampling by 2\n",
    "            padding=1, # padding to keep same size\n",
    "            output_padding=1, # output padding to keep same size\n",
    "        )\n",
    "        self.bn2 = torch.nn.BatchNorm2d(in_channel // 4)\n",
    "        self.conv2 = torch.nn.Conv2d(in_channel // 4, out_channel, kernel_size=1)\n",
    "        self.bn3 = torch.nn.BatchNorm2d(out_channel)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Again, same as in the paper for ReLU and batch normalization\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = torch.nn.functional.relu(x)\n",
    "        x = self.up(x)\n",
    "        x = self.bn2(x)\n",
    "        x = torch.nn.functional.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn3(x)\n",
    "        x = torch.nn.functional.relu(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class LinkNet(torch.nn.Module):\n",
    "    \"\"\"LinkNet model as presented in the paper LinkNet: Exploiting Encoder Representations for Efficient Semantic Segmentation (https://arxiv.org/pdf/1707.03718.pdf)\"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        encoder,\n",
    "        channels=(64, 128, 256, 512), # Potentially different channel widths depending on PyTorch resnet model\n",
    "    ):\n",
    "        # Module init\n",
    "        super().__init__()\n",
    "\n",
    "        # We use 4 different channel widths, like in the paper\n",
    "        assert len(channels) == 4\n",
    "\n",
    "        # While the decoder is the same as in the paper, the encoder is not: we use a custom encoder (ResNet) for better performance\n",
    "        self.conv1 = encoder.conv1\n",
    "        self.bn1 = encoder.bn1\n",
    "        self.maxpool = encoder.maxpool\n",
    "        self.encoders = torch.nn.ModuleList()\n",
    "        self.encoders.append(encoder.layer1)\n",
    "        self.encoders.append(encoder.layer2)\n",
    "        self.encoders.append(encoder.layer3)\n",
    "        self.encoders.append(encoder.layer4)\n",
    "\n",
    "        # Decoder part, same as in the paper (again)\n",
    "        self.decoders = torch.nn.ModuleList()\n",
    "        channels = channels[::-1] # Reverse channels\n",
    "        for i in range(len(channels) - 1):\n",
    "            self.decoders.append(LinkNetDecoderBlock(channels[i], channels[i + 1]))\n",
    "        self.decoders.append(LinkNetDecoderBlock(channels[-1], channels[-1]))\n",
    "        self.up = torch.nn.ConvTranspose2d(channels[-1], 32, kernel_size=3, stride=2)\n",
    "        self.conv2 = torch.nn.Conv2d(32, 32, kernel_size=2)\n",
    "        self.conv3 = torch.nn.Conv2d(32, 1, kernel_size=1)\n",
    "\n",
    "    def forward(self, x): # NEED CHECK\n",
    "        # Encoder part\n",
    "        # We save the output of each encoder layer to use it in the decoder part\n",
    "        x_save = []\n",
    "\n",
    "        # Same as defined on resnet for pytorch: https://pytorch.org/vision/0.8/_modules/torchvision/models/resnet.html\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = torch.nn.functional.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "        for enc in self.encoders:\n",
    "            x = enc(x)\n",
    "            x_save .append(x)\n",
    "\n",
    "        # Decoder part\n",
    "        x_save  = x_save[::-1] # Reverse x_save\n",
    "        for i in range(3):\n",
    "            x = self.decoders[i](x)\n",
    "\n",
    "            # Resize x to the size of the next decoder layer\n",
    "            if x.shape[2:] != x_save[i + 1].shape[2:]:\n",
    "                x = resize(x, x_save[i + 1].shape[2:])\n",
    "\n",
    "            # Add the output of the encoder layer to the output of the decoder layer\n",
    "            x = x + x_save[i + 1]\n",
    "        x = self.decoders[3](x)\n",
    "\n",
    "        # Output part after decoder, same as in the paper\n",
    "        x = self.up(x)\n",
    "        x = torch.nn.functional.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = torch.nn.functional.relu(x)\n",
    "        x = self.conv3(x)\n",
    "\n",
    "        # Return output\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def f1(pred, label):\n",
    "    \"\"\"Compute F1 score\"\"\"\n",
    "    # Flatten tensors\n",
    "    pred = pred.view(-1)\n",
    "    label = label.view(-1)\n",
    "\n",
    "    # True positive, false positive and negative\n",
    "    tp = (label * pred).sum().to(torch.float32)\n",
    "    fp = ((1 - label) * pred).sum().to(torch.float32)\n",
    "    fn = (label * (1 - pred)).sum().to(torch.float32)\n",
    "\n",
    "    # Precision and recall calculation\n",
    "    eps = 1e-7\n",
    "    precision = tp / (tp + fp + eps)\n",
    "    recall = tp / (tp + fn + eps)\n",
    "\n",
    "    # F1 score is returned\n",
    "    return 2 * precision * recall / (precision + recall + eps), precision, recall\n",
    "\n",
    "\n",
    "def compute_metrics(loader, model, device):\n",
    "    \"\"\"Compute the accuracy rate on the given dataset with the input model\"\"\"\n",
    "    # Set model to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # Dictionnary to store metrics\n",
    "    logs = dict()\n",
    "\n",
    "    # Parameters to compute metrics\n",
    "    num_correct = 0\n",
    "    num_pixels = 0\n",
    "\n",
    "    # Metrics computation\n",
    "    f1_score = 0\n",
    "    precision = 0\n",
    "    recall = 0\n",
    "\n",
    "    # Eval mode, so no gradient computation because no training\n",
    "    with torch.no_grad():\n",
    "        # Iterate over the dataset\n",
    "        for x, y in loader:\n",
    "            # Move data to device\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "\n",
    "            # Compute output via the model\n",
    "            output = model(x)\n",
    "\n",
    "            # Drop the first dimension of the output (batch size)\n",
    "            output = output[:, -1, :, :].unsqueeze(1)\n",
    "\n",
    "            # Apply sigmoid to the output and round it to 0 or 1 to get the prediction for each pixel\n",
    "            pred: torch.Tensor = (torch.sigmoid(output) >= 0.5).float()\n",
    "\n",
    "            # Compute the number of correct pixels and the total number of pixels\n",
    "            num_correct += torch.sum(pred == y).item()\n",
    "            num_pixels += torch.numel(pred)\n",
    "\n",
    "            # Compute F1 score\n",
    "            f1_pixel, precision_pixel, recall_pixel = f1(pred, y)\n",
    "            f1_score += f1_pixel.item()\n",
    "            precision += precision_pixel.item()\n",
    "            recall += recall_pixel.item()\n",
    "\n",
    "    # Add metrics to the dictionnary and multiply by 100 to get a percentage\n",
    "    logs[\"acc\"] = 100*num_correct/num_pixels \n",
    "    logs[\"f1 score\"] = 100*f1_score/len(loader)\n",
    "    logs[\"precision\"] = 100*precision/len(loader)\n",
    "    logs[\"recall\"] = 100*recall/len(loader)\n",
    "\n",
    "    # Set model back to training mode\n",
    "    model.train()\n",
    "\n",
    "    # Return logs\n",
    "    return logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiceLoss(nn.Module):\n",
    "    \"\"\"Dice loss\"\"\"\n",
    "    def __init__(self, smooth=1):\n",
    "        super().__init__()\n",
    "        self.smooth = smooth # Smoothing factor\n",
    "\n",
    "    def forward(self, logits, targets):\n",
    "        # Determine probabilities\n",
    "        probs = torch.sigmoid(logits)\n",
    "\n",
    "        # Flatten\n",
    "        num = targets.size(0)\n",
    "        m1 = probs.view(num, -1)\n",
    "        m2 = targets.view(num, -1)\n",
    "\n",
    "        # Find intersection and compute dice\n",
    "        intersection = (m1 * m2)\n",
    "        score = (2. * intersection.sum(1) + self.smooth) / (m1.sum(1) + m2.sum(1) + self.smooth)\n",
    "        score = 1 - score.sum() / num\n",
    "\n",
    "        # Return score\n",
    "        return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def epoch(model, loader, optimizer, criterion, scaler):\n",
    "    \"\"\"Train the model for one epoch\"\"\"\n",
    "    # Total loss for the epoch\n",
    "    total_loss = 0\n",
    "\n",
    "    # Iterate over the dataset\n",
    "    for data, target in loader:\n",
    "        # Move data to device\n",
    "        data = data.to(DEVICE)\n",
    "        target = target.to(DEVICE)\n",
    "\n",
    "        # Compute output via the model with mixed precision to speed up training\n",
    "        with torch.cuda.amp.autocast():\n",
    "            # Compute output\n",
    "            output = model(data)\n",
    "\n",
    "            # Define local loss variable\n",
    "            loss = 0\n",
    "            # Compute loss for each output\n",
    "            for i in range(output.shape[1]):\n",
    "                # Get the output for the current time step, add a dimension to match the target shape\n",
    "                pred = output[:, i, :, :].unsqueeze(1)\n",
    "\n",
    "                # Compute the loss for the current time step\n",
    "                loss += criterion(pred, target)\n",
    "\n",
    "            # Add the loss for the current time step to the total loss\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        # Backpropagation and scaler to avoid vanishing gradient\n",
    "        optimizer.zero_grad()\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "    # Return total loss\n",
    "    return total_loss\n",
    "\n",
    "\n",
    "def train(\n",
    "    model,\n",
    "    model_name,\n",
    "    train_loader,\n",
    "    validation_loader,\n",
    "    lr: float = 1.0e-4, # Learning rate\n",
    "    epochs: int = 10, # Number of epochs\n",
    "    with_dice: bool = False, # Use dice loss instead of BCE\n",
    "):\n",
    "    \"\"\"Train the model\"\"\"\n",
    "    # Create the log file\n",
    "    log_file_name = os.path.relpath(os.path.join(\"logs\", model_name + \".csv\"))\n",
    "    with open(log_file_name, \"w\") as f:\n",
    "        f.write(\"epoch,loss,f1,iou,accuracy,precision,recall\\n\")\n",
    "\n",
    "    # Create a checkpoint file to store the best model\n",
    "    model_file_name = os.path.join(\"checkpoints\", model_name + \".pth\")\n",
    "\n",
    "    # Define the criterion\n",
    "    if with_dice:\n",
    "        criterion = DiceLoss()\n",
    "    else:\n",
    "        criterion = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "    # Define the optimizer\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    # Define the scheduler and scaler\n",
    "    lambda1 = lambda epoch: 0.99 ** epoch\n",
    "    # scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda1)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=2, eta_min=lr)\n",
    "    # scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=50)\n",
    "    scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "    # Define the data augmentation used for the training set, and also create the data loader for it\n",
    "\n",
    "\n",
    "    # Variables to store current best metrics\n",
    "    max_f1 = 0\n",
    "    min_loss = 0.1\n",
    "\n",
    "    # Train the model, then save the training logs and the best model\n",
    "    loop = tqdm.tqdm(range(epochs)) \n",
    "    for e in tqdm.tqdm(range(epochs)):\n",
    "        # Train the model for one epoch and get the loss\n",
    "        loss = epoch(model, train_loader, optimizer, criterion, scaler)\n",
    "\n",
    "        # Take a step in the scheduler\n",
    "        scheduler.step()\n",
    "\n",
    "        # Compute the metrics on the validation set\n",
    "        metrics = compute_metrics(validation_loader, model, DEVICE)\n",
    "\n",
    "        # Save the model if it surpasses the current best metrics\n",
    "        # in terms of F1 score\n",
    "        if metrics[\"f1 score\"] > max_f1:\n",
    "            max_f1 = metrics[\"f1 score\"]\n",
    "            if max_f1 > 80.0:\n",
    "                torch.save(model, model_file_name + \".maxf1\")\n",
    "\n",
    "        # or in terms of loss\n",
    "        if loss < min_loss:\n",
    "            torch.save(model, model_file_name + \".minloss\")\n",
    "            min_loss = loss\n",
    "\n",
    "        # Save the logs into a file\n",
    "        with open(log_file_name, \"a\") as f:\n",
    "            f.write(\n",
    "                \"{},{},{},{},{},{},{},{}\\n\".format(\n",
    "                    e,\n",
    "                    loss,\n",
    "                    metrics[\"f1 score\"],\n",
    "                    0,\n",
    "                    metrics[\"acc\"],\n",
    "                    metrics[\"precision\"],\n",
    "                    metrics[\"recall\"],\n",
    "                    min_loss\n",
    "                )\n",
    "            )\n",
    "\n",
    "        # Update the progress bar\n",
    "        loop.set_postfix(loss=loss, f1_score=metrics[\"f1 score\"], max_f1=max_f1, min_loss=min_loss)\n",
    "\n",
    "    # Save the logs into a file\n",
    "    torch.save(model, model_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# RUN THIS CELL ==============================================================================\n",
    "# Define the data augmentation with albumentations for the training set and convert it to tensor\n",
    "train_transform = albumentations.Compose(\n",
    "    [\n",
    "        albumentations.Flip(p=0.5),\n",
    "        albumentations.Transpose(p=0.5),\n",
    "        albumentations.Rotate(p=0.5),\n",
    "        albumentations.ShiftScaleRotate(p=0.5),\n",
    "        albumentations.CoarseDropout(min_holes= 5, max_holes=20, min_height=5, max_height=20, min_width=5, max_width=20, p=0.5),\n",
    "        albumentations.OpticalDistortion(p=0.5),\n",
    "        albumentations.GridDistortion(p=0.5),\n",
    "        albumentations.ElasticTransform(p=0.5),\n",
    "        albumentations.RandomBrightnessContrast(p=0.5),\n",
    "        albumentations.PiecewiseAffine(p=0.5),\n",
    "        ToTensorV2(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Create the data loader for the training\n",
    "train_loader = get_loader(\n",
    "    data_path=Train_image_path,\n",
    "    mask_path=Train_mask_path,\n",
    "    transform=train_transform,\n",
    "    batch_size=32, # Choose the batch size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Define the data augmentation used for the validation set and convert it to tensor\n",
    "val_transform = albumentations.Compose(\n",
    "    [\n",
    "        albumentations.HorizontalFlip(p=0.5),\n",
    "        albumentations.RandomRotate90(p=0.5),\n",
    "        ToTensorV2(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Create the data loader for the validation set\n",
    "val_loader = get_loader(\n",
    "    data_path=Validation_image_path,\n",
    "    mask_path=Validation_mask_path,\n",
    "    transform=val_transform,\n",
    "    batch_size=32, # Choose the batch size\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Actual training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BCE loss with distortions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [1:54:06<00:00,  6.85s/it, f1_score=87.2, loss=1.09, max_f1=89.5]\n"
     ]
    }
   ],
   "source": [
    "train(\n",
    "    model=UNet().to(DEVICE),\n",
    "    model_name=\"unet\", # Choose the model name\n",
    "    epochs=1000, # Choose the number of epochs\n",
    "    train_loader=train_loader,\n",
    "    validation_loader=val_loader,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [1:47:39<00:00,  6.46s/it, f1_score=87.5, loss=0.962, max_f1=88.8]\n"
     ]
    }
   ],
   "source": [
    "train(\n",
    "    model=LinkNet(\n",
    "        encoder=torchvision.models.resnet18(\n",
    "            weights=torchvision.models.ResNet18_Weights.IMAGENET1K_V1\n",
    "        ),\n",
    "        channels=(64, 128, 256, 512),\n",
    "    ).to(DEVICE),\n",
    "    model_name=\"linknet18\",\n",
    "    epochs=1000,\n",
    "    train_loader=train_loader,\n",
    "    validation_loader=val_loader,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [1:54:06<00:00,  6.85s/it, f1_score=88.3, loss=0.921, max_f1=89.4]   \n"
     ]
    }
   ],
   "source": [
    "train(\n",
    "    model=LinkNet(\n",
    "        encoder=torchvision.models.resnet34(\n",
    "            weights=torchvision.models.ResNet34_Weights.IMAGENET1K_V1\n",
    "        ),\n",
    "        channels=(64, 128, 256, 512),\n",
    "    ).to(DEVICE),\n",
    "    model_name=\"linknet34\",\n",
    "    epochs=1000,\n",
    "    train_loader=train_loader,\n",
    "    validation_loader=val_loader,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet50-11ad3fa6.pth\" to /home/oancea/.cache/torch/hub/checkpoints/resnet50-11ad3fa6.pth\n",
      "100.0%\n",
      "100%|██████████| 1000/1000 [2:02:59<00:00,  7.38s/it, f1_score=86.2, loss=0.678, max_f1=89.2] \n"
     ]
    }
   ],
   "source": [
    "train(\n",
    "    model=LinkNet(\n",
    "        encoder=torchvision.models.resnet50(\n",
    "            weights=torchvision.models.ResNet50_Weights.IMAGENET1K_V2\n",
    "        ),\n",
    "        channels=(256, 512, 1024, 2048),\n",
    "    ).to(DEVICE),\n",
    "    model_name=\"linknet50\",\n",
    "    epochs=1000,\n",
    "    train_loader=train_loader,\n",
    "    validation_loader=val_loader,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "train(\n",
    "    model=LinkNet(\n",
    "        encoder=torchvision.models.resnet101(\n",
    "            weights=torchvision.models.ResNet101_Weights.IMAGENET1K_V2\n",
    "        ),\n",
    "        channels=(256, 512, 1024, 2048),\n",
    "    ).to(DEVICE),\n",
    "    model_name=\"linknet101\",\n",
    "    epochs=1000,\n",
    "    train_loader=train_loader,\n",
    "    validation_loader=val_loader,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [1:47:37<00:00,  3.23s/it, f1_score=88.6, loss=0.0918, max_f1=89.5, min_loss=0.08]    \n"
     ]
    }
   ],
   "source": [
    "train(\n",
    "    model=LinkNet(\n",
    "        encoder=torchvision.models.resnet152(\n",
    "            weights=torchvision.models.ResNet152_Weights.IMAGENET1K_V2\n",
    "        ),\n",
    "        channels=(256, 512, 1024, 2048),\n",
    "    ).to(DEVICE),\n",
    "    model_name=\"linknet152\",\n",
    "    epochs=2000,\n",
    "    train_loader=train_loader,\n",
    "    validation_loader=val_loader,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BCE loss without distortions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "train(\n",
    "    model=UNet().to(DEVICE),\n",
    "    model_name=\"unet_no_distortion\",\n",
    "    epochs=1000,\n",
    "    train_loader=train_loader_no_distortion,\n",
    "    validation_loader=val_loader,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dice loss with distortions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create submissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def patch_to_label(patch):\n",
    "    df = np.mean(patch)\n",
    "    if df > SUBMISSION_THRESHOLD:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "def sigmoid(x):\n",
    "    return 1.0 / (1.0 + np.exp(-x))\n",
    "    \n",
    "def separate_image_into_patches(image, patch_size=400):\n",
    "    \"\"\" Converts a 608x608 image into four 400x400 images that are situated in its corners\"\"\"\n",
    "    # patches = []\n",
    "    patch0 = image[0:patch_size, 0:patch_size]\n",
    "    # patches.append(patch0)\n",
    "    patch1 = image[0:patch_size, -patch_size:]\n",
    "    # patches.append(patch1)\n",
    "    patch2 = image[-patch_size:, 0:patch_size]\n",
    "    # patches.append(patch2)\n",
    "    patch3 = image[-patch_size:, -patch_size:]\n",
    "    # patches.append(patch3)\n",
    "\n",
    "    # return patches\n",
    "    return patch0, patch1, patch2, patch3\n",
    "\n",
    "def combine_patches_into_image(patches, patch_size=400):\n",
    "    # Parameters\n",
    "    side_length = 608\n",
    "    image = np.zeros((side_length, side_length))\n",
    "    \n",
    "    # Replace image\n",
    "    image[0:patch_size, 0:patch_size] = patches[0]\n",
    "    image[0:patch_size, -patch_size:] = patches[1]\n",
    "    image[-patch_size:, 0:patch_size] = patches[2]\n",
    "    image[-patch_size:, -patch_size:] = patches[3]\n",
    "    offset_1 = side_length - patch_size\n",
    "    offset_2 = 2*patch_size - side_length\n",
    "\n",
    "    # Sum results of the 4 patches in the middle\n",
    "    image[offset_1:-offset_1, offset_1:-offset_1] = (patches[0][offset_1:, offset_1:] + patches[1][offset_1:, :-offset_1] + patches[2][:-offset_1, offset_1:] + patches[3][:-offset_1, :-offset_1])/4\n",
    "    \n",
    "    # Return fusionned image\n",
    "    return image\n",
    "\n",
    "def mask_to_submission_strings(image_filename):\n",
    "    \"\"\"Reads a single image and outputs the strings that should go into the submission file\"\"\"\n",
    "    print(image_filename, os.path.basename(image_filename))\n",
    "    img_number = int(re.search(r\"\\d+\", os.path.basename(image_filename)).group(0))\n",
    "    print(img_number)\n",
    "    im = mpimg.imread(image_filename)\n",
    "    patch_size = 16\n",
    "    for j in range(0, im.shape[1], patch_size):\n",
    "        for i in range(0, im.shape[0], patch_size):\n",
    "            patch = im[i : i + patch_size, j : j + patch_size]\n",
    "            label = patch_to_label(patch)\n",
    "            yield (\"{:03d}_{}_{},{}\".format(img_number, j, i, label))\n",
    "\n",
    "\n",
    "def masks_to_submission(submission_filename, *image_filenames):\n",
    "    \"\"\"Converts images into a submission file\"\"\"\n",
    "    with open(submission_filename, \"w\") as f:\n",
    "        f.write(\"id,prediction\\n\")\n",
    "        for fn in image_filenames:\n",
    "            f.writelines(\"{}\\n\".format(s) for s in mask_to_submission_strings(fn))\n",
    "\n",
    "\n",
    "def create_postprocessing_images(images, rotations, transposes):\n",
    "    \"\"\"Apply transformations to the image and return different prospectives\"\"\"\n",
    "    ims = []\n",
    "    for image in images:\n",
    "        for rotation in rotations:\n",
    "            # im = albumentations.rotate(image, rotation)[:400,:400]\n",
    "            im = albumentations.rotate(image, rotation)\n",
    "            # ims.append(im)\n",
    "            tmp, tmp1, tmp2, tmp3 = separate_image_into_patches(image=im)\n",
    "            ims.append(tmp)\n",
    "            ims.append(tmp1)\n",
    "            ims.append(tmp2)\n",
    "            ims.append(tmp3)\n",
    "            # ims.append(albumentations.rotate(image, rotation))\n",
    "            if transposes:\n",
    "                im = albumentations.hflip(image)\n",
    "                tmp, tmp1, tmp2, tmp3 = separate_image_into_patches(albumentations.rotate(im, rotation))\n",
    "                ims.append(tmp)\n",
    "                ims.append(tmp1)\n",
    "                ims.append(tmp2)\n",
    "                ims.append(tmp3)\n",
    "                # ims.append(albumentations.rotate(im, rotation))\n",
    "    ims = np.array(ims)\n",
    "    # print(ims[0])\n",
    "    # ims = separate_image_into_patches(ims)\n",
    "    ims = torch.tensor(ims).transpose(1, -1).transpose(2, -1).float()\n",
    "    return ims\n",
    "\n",
    "\n",
    "# def combine_postprocessing_images(imagess, rotations, transposes):\n",
    "def combine_postprocessing_images(images, rotations, transposes):\n",
    "    \"\"\"Combine predictions of different prospectives\"\"\"\n",
    "    outputs = []\n",
    "    index = 0\n",
    "    while index < len(images):\n",
    "        output = np.zeros(images[0].shape)\n",
    "        for rotation in rotations:\n",
    "            # im = images[index, 0]\n",
    "            im = images[index]\n",
    "            output += albumentations.rotate(im, -rotation)\n",
    "            index += 1\n",
    "            if transposes:\n",
    "                # im = images[index, 0]\n",
    "                im = images[index]\n",
    "                im = albumentations.rotate(im, -rotation)\n",
    "                output += albumentations.hflip(im)\n",
    "                index += 1\n",
    "        output = output / len(images)\n",
    "        outputs.append(output)\n",
    "    return np.array(outputs)\n",
    "\n",
    "\n",
    "def create_submission(model_name: str):\n",
    "    # Parameters\n",
    "    rotations=[90]\n",
    "    transposes = False\n",
    "    model_file_name = os.path.join(\"checkpoints\", model_name + \".pth.maxf1\")\n",
    "    model = torch.load(model_file_name, map_location=torch.device(DEVICE)).to(DEVICE)\n",
    "    model.eval()\n",
    "\n",
    "    # Create the directory to store the predictions\n",
    "    path = \"data/test_set_images/test_1/test_1.png\"\n",
    "    pred_path = \"predictions/\" + model_name + \"_temp\"\n",
    "    if not os.path.exists(pred_path):\n",
    "        os.makedirs(pred_path)\n",
    "\n",
    "    # For each image, apply postprocessing augmentation, make predictions and save predictions\n",
    "    img_path = path\n",
    "    # im = np.asarray(Image.open(img_path)) / 255\n",
    "    im = np.asarray(Image.open(img_path)) \n",
    "    print(im.shape)\n",
    "    ims = create_postprocessing_images(\n",
    "        [im], rotations=rotations, transposes=transposes\n",
    "    )\n",
    "\n",
    "    with torch.no_grad():\n",
    "        images = []\n",
    "        for i in range(0, len(ims)//4, 4):\n",
    "        # for i in range(2):\n",
    "            output0 = model(ims[i].unsqueeze(0).to(DEVICE)).cpu().detach().numpy()\n",
    "            output1 = model(ims[i+1].unsqueeze(0).to(DEVICE)).cpu().detach().numpy()\n",
    "            output2 = model(ims[i+2].unsqueeze(0).to(DEVICE)).cpu().detach().numpy()\n",
    "            output3 = model(ims[i+3].unsqueeze(0).to(DEVICE)).cpu().detach().numpy()\n",
    "            \n",
    "            # TEST\n",
    "            predict = sigmoid(output0[0,0])\n",
    "            predict[predict < 0.5] = 0\n",
    "            predict[predict >= 0.5] = 1\n",
    "            predict *= 255 \n",
    "            final_image = Image.fromarray(predict).convert(\"L\")\n",
    "            print(\"0\", rotations[i])\n",
    "            display(final_image)\n",
    "            \n",
    "            predict = sigmoid(output1[0,0])\n",
    "            predict[predict < 0.5] = 0\n",
    "            predict[predict >= 0.5] = 1\n",
    "            predict *= 255 \n",
    "            final_image = Image.fromarray(predict).convert(\"L\")\n",
    "            print(\"1\", rotations[i])\n",
    "            display(final_image)\n",
    "            \n",
    "            predict = sigmoid(output2[0,0])\n",
    "            predict[predict < 0.5] = 0\n",
    "            predict[predict >= 0.5] = 1\n",
    "            predict *= 255 \n",
    "            final_image = Image.fromarray(predict).convert(\"L\")\n",
    "            print(\"2\", rotations[i])\n",
    "            display(final_image)\n",
    "            \n",
    "            predict = sigmoid(output3[0,0])\n",
    "            predict[predict < 0.5] = 0\n",
    "            predict[predict >= 0.5] = 1\n",
    "            predict *= 255 \n",
    "            final_image = Image.fromarray(predict).convert(\"L\")\n",
    "            print(\"3\", rotations[i])\n",
    "            display(final_image)\n",
    "            \n",
    "            \n",
    "            # tmp = combine_patches_into_image([output0[0,0],output1[0,0],output2[0,0],output3[0,0]])\n",
    "            # predict = sigmoid(tmp)\n",
    "            # predict[predict < 0.5] = 0\n",
    "            # predict[predict >= 0.5] = 1\n",
    "            # predict *= 255\n",
    "            \n",
    "            # final_image = Image.fromarray(predict).convert(\"L\")\n",
    "            # print(\"combined\", rotations[i])\n",
    "            # display(final_image)\n",
    "            # albumentations.rotate(predict, -rotations[i])\n",
    "            # final_image = Image.fromarray(predict).convert(\"L\")\n",
    "            # print(\"Rotated\", rotations[i])\n",
    "            # display(final_image)\n",
    "            # print(\"=======================================================================================================================================\")\n",
    "\n",
    "#             tmp = combine_patches_into_image([output0[0,0],output1[0,0],output2[0,0],output3[0,0]])\n",
    "#             tmp = sigmoid(tmp)\n",
    "#             images.append(tmp)\n",
    "\n",
    "#     predict = combine_postprocessing_images(\n",
    "#         images, rotations=rotations, transposes=transposes\n",
    "#     ).reshape((608, 608))\n",
    "#     predict[predict < 0.5] = 0\n",
    "#     predict[predict >= 0.5] = 1\n",
    "#     predict *= 255\n",
    "    # Image.fromarray(im).convert(\"L\").save(\n",
    "    #     os.path.join(pred_path, \"1\") + \".png\"\n",
    "    # )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(608, 608, 3)\n",
      "0 90\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-149-33e3c9906e5e>:9: RuntimeWarning: overflow encountered in exp\n",
      "  return 1.0 / (1.0 + np.exp(-x))\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZAAAAGQCAAAAACl1GkQAAAAsklEQVR4nO3BAQ0AAADCoPdPbQ43oAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAeDRyrgABKT7rhwAAAABJRU5ErkJggg==",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=400x400 at 0x7FB3907871C0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 90\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZAAAAGQCAAAAACl1GkQAAAAsklEQVR4nO3BAQ0AAADCoPdPbQ43oAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAeDRyrgABKT7rhwAAAABJRU5ErkJggg==",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=400x400 at 0x7FB390787BE0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 90\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZAAAAGQCAAAAACl1GkQAAAAsklEQVR4nO3BAQ0AAADCoPdPbQ43oAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAeDRyrgABKT7rhwAAAABJRU5ErkJggg==",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=400x400 at 0x7FB390787160>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 90\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZAAAAGQCAAAAACl1GkQAAAAsklEQVR4nO3BAQ0AAADCoPdPbQ43oAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAeDRyrgABKT7rhwAAAABJRU5ErkJggg==",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=400x400 at 0x7FB390787E20>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# create_submission(\"unet\")\n",
    "# create_submission(\"linknet18\")\n",
    "# create_submission(\"linknet34\")\n",
    "# create_submission(\"linknet50\")\n",
    "# create_submission(\"linknet101\")\n",
    "create_submission(\"linknet152\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def patch_to_label(patch):\n",
    "    df = np.mean(patch)\n",
    "    if df > SUBMISSION_THRESHOLD:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "\n",
    "def mask_to_submission_strings(image_filename):\n",
    "    \"\"\"Reads a single image and outputs the strings that should go into the submission file\"\"\"\n",
    "    print(image_filename, os.path.basename(image_filename))\n",
    "    img_number = int(re.search(r\"\\d+\", os.path.basename(image_filename)).group(0))\n",
    "    print(img_number)\n",
    "    im = mpimg.imread(image_filename)\n",
    "    patch_size = 16\n",
    "    for j in range(0, im.shape[1], patch_size):\n",
    "        for i in range(0, im.shape[0], patch_size):\n",
    "            patch = im[i : i + patch_size, j : j + patch_size]\n",
    "            label = patch_to_label(patch)\n",
    "            yield (\"{:03d}_{}_{},{}\".format(img_number, j, i, label))\n",
    "\n",
    "\n",
    "def masks_to_submission(submission_filename, *image_filenames):\n",
    "    \"\"\"Converts images into a submission file\"\"\"\n",
    "    with open(submission_filename, \"w\") as f:\n",
    "        f.write(\"id,prediction\\n\")\n",
    "        for fn in image_filenames:\n",
    "            f.writelines(\"{}\\n\".format(s) for s in mask_to_submission_strings(fn))\n",
    "\n",
    "\n",
    "def create_postprocessing_images(images, rotations, transposes):\n",
    "    \"\"\"Apply transformations to the image and return different prospectives\"\"\"\n",
    "    ims = []\n",
    "    for image in images:\n",
    "        for rotation in rotations:\n",
    "            ims.append(albumentations.rotate(image, rotation))\n",
    "            if transposes:\n",
    "                im = albumentations.hflip(image)\n",
    "                ims.append(albumentations.rotate(im, rotation))\n",
    "    ims = np.array(ims)\n",
    "    ims = torch.tensor(ims).transpose(1, -1).transpose(2, -1).float()\n",
    "    return ims\n",
    "\n",
    "\n",
    "def combine_postprocessing_images(images, rotations, transposes):\n",
    "    \"\"\"Combine predictions of different prospectives\"\"\"\n",
    "    outputs = []\n",
    "    index = 0\n",
    "    while index < len(images):\n",
    "        output = np.zeros(images[0].shape)\n",
    "        for rotation in rotations:\n",
    "            im = images[index, 0]\n",
    "            output += albumentations.rotate(im, -rotation)\n",
    "            index += 1\n",
    "            if transposes:\n",
    "                im = images[index, 0]\n",
    "                im = albumentations.rotate(im, -rotation)\n",
    "                output += albumentations.hflip(im)\n",
    "                index += 1\n",
    "        output = output / len(images)\n",
    "        outputs.append(output)\n",
    "    return np.array(outputs)\n",
    "\n",
    "\n",
    "def create_submission(model_name: str):\n",
    "    # model_file_name = os.path.join(\"checkpoints\", model_name + \".pth.maxf1\")\n",
    "    model_file_name = os.path.join(\"checkpoints\", model_name + \".pth.minloss\")\n",
    "    model = torch.load(model_file_name, map_location=torch.device(DEVICE)).to(DEVICE)\n",
    "    model.eval()\n",
    "\n",
    "    # Create the directory to store the predictions\n",
    "    path = \"data/test_set_images\"\n",
    "    pred_path = \"predictions/\" + model_name\n",
    "    if not os.path.exists(pred_path):\n",
    "        os.makedirs(pred_path)\n",
    "\n",
    "    # For each image, apply postprocessing augmentation, make predictions and save predictions\n",
    "    for image in tqdm.tqdm(os.listdir(path)):\n",
    "        img_path = os.path.join(path, image, image + \".png\")\n",
    "        im = np.asarray(Image.open(img_path)) / 255\n",
    "        ims = create_postprocessing_images(\n",
    "            [im], rotations=[0, 90, 180, 270], transposes=True\n",
    "        )\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output = model(ims.to(DEVICE))\n",
    "            # output = output.cpu().detach().numpy()\n",
    "            predicts = torch.sigmoid(output).cpu().detach()\n",
    "\n",
    "        predict = combine_postprocessing_images(\n",
    "            predicts.numpy(), rotations=[0, 90, 180, 270], transposes=True\n",
    "        ).reshape((608, 608))\n",
    "        predict[predict < 0.5] = 0\n",
    "        predict[predict >= 0.5] = 1\n",
    "        predict *= 255\n",
    "        Image.fromarray(predict).convert(\"L\").save(\n",
    "            os.path.join(pred_path, image) + \".png\"\n",
    "        )\n",
    "\n",
    "    # Generate the submission file\n",
    "    submission_filename = os.path.join(\n",
    "        \"submissions\", \"submission_{}.csv\".format(model_name)\n",
    "    )\n",
    "    image_filenames = []\n",
    "    for i in range(1, 51):\n",
    "        image_filename = pred_path + \"/test_\" + str(i) + \".png\"\n",
    "        image_filenames.append(image_filename)\n",
    "    masks_to_submission(submission_filename, *image_filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:40<00:00,  1.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictions/linknet152/test_1.png test_1.png\n",
      "1\n",
      "predictions/linknet152/test_2.png test_2.png\n",
      "2\n",
      "predictions/linknet152/test_3.png test_3.png\n",
      "3\n",
      "predictions/linknet152/test_4.png test_4.png\n",
      "4\n",
      "predictions/linknet152/test_5.png test_5.png\n",
      "5\n",
      "predictions/linknet152/test_6.png test_6.png\n",
      "6\n",
      "predictions/linknet152/test_7.png test_7.png\n",
      "7\n",
      "predictions/linknet152/test_8.png test_8.png\n",
      "8\n",
      "predictions/linknet152/test_9.png test_9.png\n",
      "9\n",
      "predictions/linknet152/test_10.png test_10.png\n",
      "10\n",
      "predictions/linknet152/test_11.png test_11.png\n",
      "11\n",
      "predictions/linknet152/test_12.png test_12.png\n",
      "12\n",
      "predictions/linknet152/test_13.png test_13.png\n",
      "13\n",
      "predictions/linknet152/test_14.png test_14.png\n",
      "14\n",
      "predictions/linknet152/test_15.png test_15.png\n",
      "15\n",
      "predictions/linknet152/test_16.png test_16.png\n",
      "16\n",
      "predictions/linknet152/test_17.png test_17.png\n",
      "17\n",
      "predictions/linknet152/test_18.png test_18.png\n",
      "18\n",
      "predictions/linknet152/test_19.png test_19.png\n",
      "19\n",
      "predictions/linknet152/test_20.png test_20.png\n",
      "20\n",
      "predictions/linknet152/test_21.png test_21.png\n",
      "21\n",
      "predictions/linknet152/test_22.png test_22.png\n",
      "22\n",
      "predictions/linknet152/test_23.png test_23.png\n",
      "23\n",
      "predictions/linknet152/test_24.png test_24.png\n",
      "24\n",
      "predictions/linknet152/test_25.png test_25.png\n",
      "25\n",
      "predictions/linknet152/test_26.png test_26.png\n",
      "26\n",
      "predictions/linknet152/test_27.png test_27.png\n",
      "27\n",
      "predictions/linknet152/test_28.png test_28.png\n",
      "28\n",
      "predictions/linknet152/test_29.png test_29.png\n",
      "29\n",
      "predictions/linknet152/test_30.png test_30.png\n",
      "30\n",
      "predictions/linknet152/test_31.png test_31.png\n",
      "31\n",
      "predictions/linknet152/test_32.png test_32.png\n",
      "32\n",
      "predictions/linknet152/test_33.png test_33.png\n",
      "33\n",
      "predictions/linknet152/test_34.png test_34.png\n",
      "34\n",
      "predictions/linknet152/test_35.png test_35.png\n",
      "35\n",
      "predictions/linknet152/test_36.png test_36.png\n",
      "36\n",
      "predictions/linknet152/test_37.png test_37.png\n",
      "37\n",
      "predictions/linknet152/test_38.png test_38.png\n",
      "38\n",
      "predictions/linknet152/test_39.png test_39.png\n",
      "39\n",
      "predictions/linknet152/test_40.png test_40.png\n",
      "40\n",
      "predictions/linknet152/test_41.png test_41.png\n",
      "41\n",
      "predictions/linknet152/test_42.png test_42.png\n",
      "42\n",
      "predictions/linknet152/test_43.png test_43.png\n",
      "43\n",
      "predictions/linknet152/test_44.png test_44.png\n",
      "44\n",
      "predictions/linknet152/test_45.png test_45.png\n",
      "45\n",
      "predictions/linknet152/test_46.png test_46.png\n",
      "46\n",
      "predictions/linknet152/test_47.png test_47.png\n",
      "47\n",
      "predictions/linknet152/test_48.png test_48.png\n",
      "48\n",
      "predictions/linknet152/test_49.png test_49.png\n",
      "49\n",
      "predictions/linknet152/test_50.png test_50.png\n",
      "50\n"
     ]
    }
   ],
   "source": [
    "create_submission(\"linknet152\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6 (tags/v3.10.6:9c7b4bd, Aug  1 2022, 21:53:49) [MSC v.1932 64 bit (AMD64)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "a957044d351fb82484f120dca125b0411d05a6141ed71a6e42fbd3678d62e425"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
