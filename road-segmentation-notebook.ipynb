{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<a href=\"https://www.kaggle.com/code/tudoroancea/road-segmentation-notebook?scriptVersionId=111171434\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{"_uuid":"3a593c5f-5641-4767-bf17-cab79121de60","_cell_guid":"958b279b-3e97-422e-a886-9fb1c79cb7e5","trusted":true}},{"cell_type":"code","source":"import os\n\nimport torch\nimport torch.nn as nn\nimport torch.utils.data as tdata\nimport torchvision\nfrom torch.nn import functional as F\nimport matplotlib.pyplot as plt\n\ndevice = \"cuda\"\ntorch.manual_seed(127)","metadata":{"_uuid":"e54281a5-574b-4893-85e5-569ac641b1c5","_cell_guid":"3f5b08e7-8c40-4560-80ad-0b3dcbf1388b","collapsed":false,"execution":{"iopub.status.busy":"2022-11-19T13:18:28.445470Z","iopub.execute_input":"2022-11-19T13:18:28.445879Z","iopub.status.idle":"2022-11-19T13:18:28.454674Z","shell.execute_reply.started":"2022-11-19T13:18:28.445843Z","shell.execute_reply":"2022-11-19T13:18:28.453292Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data import, exploration and pre-processing","metadata":{"_uuid":"16119d8f-2a0a-4f2e-b6d6-f9b3e8f46023","_cell_guid":"37ea7640-802d-4ed5-8ae4-855774e964ca","trusted":true}},{"cell_type":"code","source":"class RoadsDataset(tdata.Dataset):\n    root: str\n    num_images: int\n    images: list\n    gt_images: list\n    gt_images_one_hot: list\n\n    def __init__(self, root: str, num_images=20, transform=None, target_transform=None):\n        self.root = root\n        self.transform = transform\n        self.target_transform = target_transform\n        self.num_images = num_images\n        assert 10 <= num_images <= 100\n        self.images = []\n        self.gt_images = []\n        self.gt_images_one_hot = []\n        for i in range(num_images):\n            image_path = os.path.join(self.root, \"image/image_%.5d.png\" % (i + 1))\n            img = torchvision.io.read_image(image_path).type(torch.float32).to(device)\n            img /= 255.0\n            self.images.append(img)\n            gt_image_path = os.path.join(\n                self.root, \"groundtruth/ground_truth_%.5d.png\" % (i + 1)\n            )\n            gt_image = torchvision.io.read_image(gt_image_path)\n            gt_image_one_hot = torch.movedim(\n                F.one_hot(\n                    torch.div(\n                        torch.squeeze(gt_image.to(device)),\n                        255,\n                    ).type(torch.int64),\n                    2,\n                ),\n                2,\n                0,\n            ).to(dtype=torch.float32)\n            self.gt_images.append(gt_image)\n            self.gt_images_one_hot.append(gt_image_one_hot)\n\n        print(\"Loaded {} images from {}\".format(num_images, root))\n\n    def __len__(self):\n        return self.num_images\n\n    def __getitem__(self, item: int) -> tuple:\n        if self.transform:\n            image = self.transform(self.images[item])\n        else:\n            image = self.images[item]\n\n        if self.target_transform:\n            gt_image_one_hot = self.target_transform(self.gt_images_one_hot[item])\n        else:\n            gt_image_one_hot = self.gt_images_one_hot[item]\n\n        return image, gt_image_one_hot","metadata":{"_uuid":"bd2a9243-689f-4e2a-97fa-bb5d73f3aea0","_cell_guid":"41f5b809-2be2-4c94-8050-5fd7b5001ad4","collapsed":false,"execution":{"iopub.status.busy":"2022-11-19T13:40:48.748404Z","iopub.execute_input":"2022-11-19T13:40:48.749381Z","iopub.status.idle":"2022-11-19T13:40:48.770545Z","shell.execute_reply.started":"2022-11-19T13:40:48.749331Z","shell.execute_reply":"2022-11-19T13:40:48.769460Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"training_data = RoadsDataset(root=\"../input/d/tudoroancea/road-segmentation-dataset/data/train\", num_images=100)\ntraining_dataloader = tdata.DataLoader(training_data, batch_size=5, shuffle=True)","metadata":{"_uuid":"be8f81ce-f1e1-4a49-a28f-057ce3fd74a9","_cell_guid":"1e7785fa-348f-49c5-80b1-a23148a5e910","collapsed":false,"execution":{"iopub.status.busy":"2022-11-19T13:40:54.078202Z","iopub.execute_input":"2022-11-19T13:40:54.078929Z","iopub.status.idle":"2022-11-19T13:40:55.309574Z","shell.execute_reply.started":"2022-11-19T13:40:54.078890Z","shell.execute_reply":"2022-11-19T13:40:55.308434Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"index = torch.randint(0, len(training_data), (1,)).item()\nimage = training_data.images[index].to(device=\"cpu\")\ngt_image = training_data.gt_images[index].to(device=\"cpu\")\nplt.subplot(121)\nplt.imshow(torch.movedim(image, 0, 2))\nplt.subplot(122)\nplt.imshow(torch.movedim(gt_image,0,2), cmap=\"gray\")\nplt.tight_layout()","metadata":{"_uuid":"794e116d-be8e-4d85-97b5-9b6d8982ec5b","_cell_guid":"9fff67e3-61f7-438a-a184-bb347e726700","collapsed":false,"execution":{"iopub.status.busy":"2022-11-19T13:40:57.629385Z","iopub.execute_input":"2022-11-19T13:40:57.630122Z","iopub.status.idle":"2022-11-19T13:40:58.165537Z","shell.execute_reply.started":"2022-11-19T13:40:57.630071Z","shell.execute_reply":"2022-11-19T13:40:58.164561Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# UNet model","metadata":{"_uuid":"e3f80309-be99-49f2-ae8c-89c158e98da3","_cell_guid":"caa938b1-54e6-4777-89e2-3a296173079f","trusted":true}},{"cell_type":"code","source":"","metadata":{"_uuid":"c5e20a2f-58d4-4610-94b3-3fdded9a396e","_cell_guid":"d8be5731-84fd-4f26-99db-33bb58fe8323","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class DoubleConv(nn.Module):\n    \"\"\"(convolution => [BN] => ReLU) * 2\"\"\"\n\n    def __init__(self, in_channels, out_channels):\n        super().__init__()\n        self.double_conv = nn.Sequential(\n            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, bias=False),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1, bias=False),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True),\n        )\n\n    def forward(self, x: torch.Tensor):\n        return self.double_conv(x)\n\n\nclass Down(nn.Module):\n    \"\"\"Downscaling with maxpool then double conv\"\"\"\n\n    def __init__(self, in_channels, out_channels):\n        super().__init__()\n        self.maxpool_conv = nn.Sequential(\n            nn.MaxPool2d(2), DoubleConv(in_channels, out_channels)\n        )\n\n    def forward(self, x):\n        return self.maxpool_conv(x)\n\n\nclass Up(nn.Module):\n    \"\"\"Upscaling then double conv\"\"\"\n\n    def __init__(self, in_channels, out_channels):\n        super().__init__()\n        self.up = nn.ConvTranspose2d(\n            in_channels, in_channels // 2, kernel_size=2, stride=2\n        )\n        self.conv = DoubleConv(in_channels, out_channels)\n\n    def forward(self, x1, x2):\n        x1 = self.up(x1)\n        x = torch.cat([x2, x1], dim=1)\n        return self.conv(x)\n\n\nclass OutConv(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super(OutConv, self).__init__()\n        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n        self.softmax = nn.Softmax(dim=1)\n\n    def forward(self, x):\n        return self.softmax(self.conv(x))\n\n\nclass UNet(nn.Module):\n    def __init__(self, n_channels, n_classes):\n        super().__init__()\n        self.n_channels = n_channels\n        self.n_classes = n_classes\n\n        self.inc = DoubleConv(n_channels, 64)\n        self.down1 = Down(64, 128)\n        self.down2 = Down(128, 256)\n        self.down3 = Down(256, 512)\n        self.down4 = Down(512, 1024)\n        self.up1 = Up(1024, 512)\n        self.up2 = Up(512, 256)\n        self.up3 = Up(256, 128)\n        self.up4 = Up(128, 64)\n        self.outc = OutConv(64, n_classes)\n\n    def forward(self, x):\n        x1 = self.inc(x)\n        x2 = self.down1(x1)\n        x3 = self.down2(x2)\n        x4 = self.down3(x3)\n        x5 = self.down4(x4)\n        x = self.up1(x5, x4)\n        x = self.up2(x, x3)\n        x = self.up3(x, x2)\n        x = self.up4(x, x1)\n        logits = self.outc(x)\n        return logits\n\n\nunet_model = UNet(n_channels=3, n_classes=2).to(device)","metadata":{"_uuid":"e5fe7b1c-29cd-4901-862b-cef8821643fa","_cell_guid":"86b6b02e-84a1-4168-90dd-da28fea4aa7a","collapsed":false,"execution":{"iopub.status.busy":"2022-11-19T13:41:01.419798Z","iopub.execute_input":"2022-11-19T13:41:01.420187Z","iopub.status.idle":"2022-11-19T13:41:01.758327Z","shell.execute_reply.started":"2022-11-19T13:41:01.420154Z","shell.execute_reply":"2022-11-19T13:41:01.757223Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model training","metadata":{"_uuid":"8b0961f6-5c8e-406b-bdd6-7a2783ec25f4","_cell_guid":"b1fd594b-26c8-4725-86c4-18620df1ec5a","trusted":true}},{"cell_type":"code","source":"loss_fun = nn.BCELoss()\noptimizer = torch.optim.Adam(unet_model.parameters(), lr=1e-3)\n\n\ndef train(dataloader: tdata.DataLoader, model: nn.Module, loss_fun, optimizer):\n    size = len(dataloader.dataset)\n    model.train()\n    for batch_num, (X_batch, Y_batch) in enumerate(dataloader):\n        X_batch, Y_batch = X_batch.to(device), Y_batch.to(device)\n        # Compute prediction error\n        pred = model(X_batch)\n        loss = loss_fun(pred, Y_batch)\n\n        # Backpropagation\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        loss, current = loss.item(), (batch_num + 1) * len(X_batch)\n        print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n\n\nepochs = 20\nfor t in range(epochs):\n    print(f\"Epoch {t+1}\\n-------------------------------\")\n    train(training_dataloader, unet_model, loss_fun, optimizer)\n\nprint(\"Done training!\")\ntorch.save(unet_model.state_dict(), \"unet_model2.pth\")\nprint(\"Saved PyTorch Model State to unet_model2.pth\")","metadata":{"_uuid":"a5c61c56-9d47-48bd-b817-20d37248fa15","_cell_guid":"c853d65c-7ebb-47a8-a827-436543b69736","collapsed":false,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-11-19T13:41:03.898717Z","iopub.execute_input":"2022-11-19T13:41:03.899504Z","iopub.status.idle":"2022-11-19T13:47:56.194135Z","shell.execute_reply.started":"2022-11-19T13:41:03.899466Z","shell.execute_reply":"2022-11-19T13:47:56.193046Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"unet_model = UNet(3, 2).to(device)\nunet_model.load_state_dict(torch.load(\"unet_model2.pth\"))","metadata":{"_uuid":"d2aad876-c481-4880-be14-7f984d922876","_cell_guid":"75440b6a-8464-4ed2-9560-52ff55f769e9","collapsed":false,"execution":{"iopub.status.busy":"2022-11-19T13:49:14.533399Z","iopub.execute_input":"2022-11-19T13:49:14.533795Z","iopub.status.idle":"2022-11-19T13:49:15.114575Z","shell.execute_reply.started":"2022-11-19T13:49:14.533744Z","shell.execute_reply":"2022-11-19T13:49:15.113544Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"unet_model.eval()\nindex = 0\nX, Y, Y_one_hot = training_data.images[index], training_data.gt_images[index], training_data.gt_images_one_hot[index]\nX = X.to(device)\nY = Y.to(device)\nY_one_hot = Y_one_hot.to(device)\nwith torch.no_grad():\n    Y_pred = torch.squeeze(unet_model(torch.unsqueeze(X, 0)))\n    print(Y_one_hot)\n    print(nn.BCELoss()(Y_pred[0], Y_one_hot[0]))\n    Y_pred = Y_pred[1,:,:]\n#     print(Y_pred[0,190:210,:])\n#     print(Y_pred[1,190:210,:])\n#     Y_pred = torch.argmax(Y_pred, dim=0)\n#     print(Y_pred)\n#     print(torch.max(Y_pred))\n    Y_pred = torch.unsqueeze(Y_pred, 0)\n    Y_pred *= 255\n    print(Y_pred.shape)\n\nplt.subplot(1, 3, 1)\nplt.imshow(torch.movedim(X, 0, 2).to(\"cpu\"))\nplt.title(\"Input\")\nplt.subplot(1, 3, 2)\nplt.imshow(torch.movedim(Y, 0, 2).to(\"cpu\"), cmap=\"gray\")\nplt.title(\"Ground Truth\")\nplt.subplot(1, 3, 3)\nplt.imshow(torch.movedim(Y_pred, 0, 2).to(device=\"cpu\"), cmap=\"gray\")\nplt.title(\"Prediction\")","metadata":{"_uuid":"548944d9-2048-4295-986f-93d771590c83","_cell_guid":"aab42aa5-81ef-4fbb-bce7-ade699500e43","collapsed":false,"execution":{"iopub.status.busy":"2022-11-19T14:04:11.446343Z","iopub.execute_input":"2022-11-19T14:04:11.446746Z","iopub.status.idle":"2022-11-19T14:04:11.979817Z","shell.execute_reply.started":"2022-11-19T14:04:11.446712Z","shell.execute_reply":"2022-11-19T14:04:11.978805Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_with_params(lr: float, batch_size):\n    loss_fun = nn.BCELoss()\noptimizer = torch.optim.Adam(unet_model.parameters(), lr=1e-3)\n\n    size = len(dataloader.dataset)\n    model.train()\n    for batch_num, (X_batch, Y_batch) in enumerate(dataloader):\n        X_batch, Y_batch = X_batch.to(device), Y_batch.to(device)\n        # Compute prediction error\n        pred = model(X_batch)\n        loss = loss_fun(pred, Y_batch)\n\n        # Backpropagation\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        loss, current = loss.item(), (batch_num + 1) * len(X_batch)\n        print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")","metadata":{"_uuid":"b89870d7-a9e3-4103-9a11-299fa8196f2c","_cell_guid":"51ffc1e7-6497-4031-b864-4bba17dd8ea6","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]}]}