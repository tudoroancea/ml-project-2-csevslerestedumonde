{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e69196b",
   "metadata": {},
   "source": [
    "## Table of content:\n",
    "* [Data processing](#data)\n",
    "* [Training setup](#training_setup)\n",
    " * [Metrics computation](#metrics)\n",
    " * [Training functions](#training_func)\n",
    " * [Data augmentation](#data_aug)\n",
    "* [Actual training](#training)\n",
    " * [Phase 1: influence of the pre-processing](#phase_1)\n",
    " * [Phase 2: influence of the model](#phase_2)\n",
    "    * [LinkNet18](#ln_18)\n",
    "    * [LinkNet34](#ln_34)\n",
    "    * [LinkNet50](#ln_50)\n",
    "    * [LinkNet101](#ln_101)\n",
    "    * [LinkNet152](#ln_152)\n",
    " * [Phase 3: hyperparameters choice](#hyper)\n",
    " * [Phase 4: best model](#phase_4)\n",
    "* [Create submissions](#submissions)\n",
    "* [Local testing](#testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4cc0abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =================================================================================\n",
    "# IMPORTS\n",
    "# =================================================================================\n",
    "import _thread\n",
    "import os\n",
    "import re\n",
    "\n",
    "#to plot and save images\n",
    "import matplotlib.image as mpimg\n",
    "from PIL import Image\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "#pytorch\n",
    "import torch\n",
    "import torch.utils.data\n",
    "\n",
    "#library segmentation models for UNet, LinkNet and metrics implementations \n",
    "from segmentation_models_pytorch import metrics\n",
    "import segmentation_models_pytorch as smp\n",
    "\n",
    "#progress bar\n",
    "import tqdm\n",
    "\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import albumentations\n",
    "\n",
    "#set the seed espacially for albumentations\n",
    "import random\n",
    "\n",
    "# =================================================================================\n",
    "# GENERAL PARAMETERS\n",
    "# =================================================================================\n",
    "random.seed(127)\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"DEVICE: \", DEVICE)\n",
    "SUBMISSION_THRESHOLD = 0.25 # Given, DO NOT change it\n",
    "random.seed(127)\n",
    "\n",
    "# =================================================================================\n",
    "# PATHS\n",
    "# =================================================================================\n",
    "# the 100 training images are divided in two sets : training (1 to 80) and validating (81 to 100)\n",
    "TRAIN_IMAGE_PATH = \"data/training/images\"\n",
    "TRAIN_MASK_PATH = \"data/training/groundtruth\"\n",
    "VALIDATION_IMAGE_PATH = \"data/validating/images\"\n",
    "VALIDATION_MASK_PATH = \"data/validating/groundtruth\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e42e94e",
   "metadata": {},
   "source": [
    "# Data processing <a class=\"anchor\" id=\"data\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5915025b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RoadDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    The class RoadDataset loads the data and executes the pre-processing operations on it.\n",
    "    More specifically, it re-applies the specified transform every time data is fetched via a dataloader.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        image_path: str,\n",
    "        mask_path: str,\n",
    "        transform,\n",
    "    ):\n",
    "        # Remember transforms\n",
    "        self.transform = transform\n",
    "\n",
    "        # Load images and masks\n",
    "        self.images = self.load_images(image_path)\n",
    "        self.masks = self.load_images(mask_path)\n",
    "\n",
    "        # Augmented images and masks\n",
    "        self.images_augmented = []\n",
    "        self.masks_augmented = []\n",
    "\n",
    "        # Data augmentation using transforms\n",
    "        for i in range(len(self.images)):\n",
    "            output = self.transform(image=self.images[i], mask=self.masks[i])\n",
    "            self.images_augmented.append(output[\"image\"])\n",
    "            self.masks_augmented.append(output[\"mask\"])\n",
    "\n",
    "    def get_images(self):\n",
    "        return self.images, self.masks\n",
    "\n",
    "    @staticmethod\n",
    "    def load_images(image_path):\n",
    "        \"\"\"This method loads the images from the given path\"\"\"\n",
    "        images = []\n",
    "        for img in os.listdir(image_path):\n",
    "            path = os.path.join(image_path, img)\n",
    "            image = Image.open(path)\n",
    "            images.append(np.asarray(image))\n",
    "\n",
    "        return np.asarray(images)\n",
    "\n",
    "    def augment(self, index):\n",
    "        \"\"\"This method applies data augmentation to the images again to change precedent augmentation transformations\"\"\"\n",
    "        output = self.transform(image=self.images[index], mask=self.masks[index])\n",
    "        self.images_augmented[index] = output[\"image\"]\n",
    "        self.masks_augmented[index] = output[\"mask\"]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"This method returns the image at a certain position and its mask\"\"\"\n",
    "        image = self.images_augmented[index]\n",
    "        mask = self.masks_augmented[index]\n",
    "\n",
    "        # Start a new thread to augment the data (thread level parallelism)\n",
    "        _thread.start_new_thread(self.augment, (index,))\n",
    "\n",
    "        # Return scaled image and mask\n",
    "        return (image / 255), (mask.unsqueeze(0) > 200).float()\n",
    "\n",
    "\n",
    "def get_loader(\n",
    "    data_path: str,\n",
    "    mask_path: str,\n",
    "    transform,\n",
    "    batch_size: int = 4,\n",
    "):\n",
    "    \"\"\"Create the pytorch DataLoader\"\"\"\n",
    "    # Use our dataset and defined transformations\n",
    "    dataset = RoadDataset(\n",
    "        data_path,\n",
    "        mask_path,\n",
    "        transform,\n",
    "    )\n",
    "\n",
    "    # Use the dataset in the torch dataloader\n",
    "    return torch.utils.data.DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size, # Specify batch size\n",
    "        shuffle=True, # Shuffle the data to avoid learning the order\n",
    "        pin_memory=True, # Copy tensors to CUDA pinned memory\n",
    "        generator=torch.Generator().manual_seed(127), # Set seed for reproducibility\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb71f707",
   "metadata": {},
   "source": [
    "# Training setup <a class=\"anchor\" id=\"training_setup\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a668865a",
   "metadata": {},
   "source": [
    "## Metrics computation <a class=\"anchor\" id=\"metrics\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd2affd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(loader, model, device):\n",
    "    \"\"\"Compute the accuracy rate on the validation dataset with the input model\"\"\"\n",
    "    # Set model to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # Dictionnary to store metrics\n",
    "    logs = dict()\n",
    "\n",
    "    # Parameters to compute metrics\n",
    "    num_correct = 0\n",
    "    num_pixels = 0\n",
    "\n",
    "    # Metrics computation\n",
    "    f1_score = 0\n",
    "    precision = 0\n",
    "    recall = 0\n",
    "\n",
    "    # Eval mode, so no gradient computation because no training\n",
    "    with torch.no_grad():\n",
    "        # Iterate over the dataset\n",
    "        for x, y in loader:\n",
    "            # Move data to device\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "\n",
    "            # # Create simple transformed images\n",
    "            # x = create_simple_transformed_images(x, [0, 90, 180, 270], True)\n",
    "\n",
    "            # Compute output via the model\n",
    "            output = model(x)\n",
    "\n",
    "            # Drop the first dimension of the output (batch size)\n",
    "            output = output[:, -1, :, :].unsqueeze(1)\n",
    "\n",
    "            # Apply sigmoid to the output and round it to 0 or 1 to get the prediction for each pixel\n",
    "            pred: torch.Tensor = (torch.sigmoid(output) >= 0.5)\n",
    "\n",
    "            # Compute the number of correct pixels and the total number of pixels\n",
    "            num_correct += torch.sum(pred == y).item()\n",
    "            num_pixels += torch.numel(pred)\n",
    "            \n",
    "            # True positive and negative, false positive and negative using segmentation models functions\n",
    "            tp, fp, fn, tn = metrics.get_stats(pred, y.int(), mode='binary')\n",
    "\n",
    "            # Compute F1 score, precision and recall\n",
    "            f1_score += metrics.f1_score(tp, fp, fn, tn, reduction='micro')\n",
    "            precision += metrics.precision(tp, fp, fn, tn, reduction='micro')\n",
    "            recall += metrics.recall(tp, fp, fn, tn, reduction='micro')\n",
    "\n",
    "    # Add metrics to the dictionnary and multiply by 100 to get a percentage\n",
    "    logs[\"acc\"] = num_correct / num_pixels * 100\n",
    "    logs[\"f1 score\"] = f1_score.cpu().numpy() / len(loader) * 100\n",
    "    logs[\"precision\"] = precision.cpu().numpy() / len(loader) * 100\n",
    "    logs[\"recall\"] = recall.cpu().numpy() / len(loader) * 100\n",
    "\n",
    "    # Set model back to training mode\n",
    "    model.train()\n",
    "\n",
    "    # Return logs\n",
    "    return logs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5e6ffec",
   "metadata": {},
   "source": [
    "## Training functions <a class=\"anchor\" id=\"training_func\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a895433b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch(model, loader, optimizer, criterion, scaler):\n",
    "    \"\"\"Train the model for one epoch\"\"\"\n",
    "    # Total loss for the epoch\n",
    "    total_loss = 0\n",
    "\n",
    "    # Iterate over the dataset\n",
    "    for data, target in loader:\n",
    "        # Move data to device\n",
    "        data = data.to(DEVICE)\n",
    "        target = target.to(DEVICE)\n",
    "\n",
    "        # Compute output via the model with mixed precision to speed up training\n",
    "        with torch.cuda.amp.autocast():\n",
    "            # Compute output\n",
    "            output = model(data)\n",
    "\n",
    "            # Define local loss variable\n",
    "            loss = 0\n",
    "            # Compute loss for each output\n",
    "            for i in range(output.shape[1]):\n",
    "                # Get the output for the current time step, add a dimension to match the target shape\n",
    "                pred = output[:, i, :, :].unsqueeze(1)\n",
    "\n",
    "                # Compute the loss for the current time step\n",
    "                loss += criterion(pred, target)\n",
    "\n",
    "            # Add the loss for the current time step to the total loss\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        # Backpropagation and scaler to avoid vanishing gradient\n",
    "        optimizer.zero_grad()\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "    # Return total loss\n",
    "    return total_loss\n",
    "\n",
    "\n",
    "def train(\n",
    "    model,\n",
    "    model_name,\n",
    "    train_loader,\n",
    "    validation_loader,\n",
    "    lr_max: float = 1.0e-3, # Learning rate minimal\n",
    "    lr_min: float = 1.0e-5, # Learning rate minimal\n",
    "    epochs: int = 10, # Number of epochs\n",
    "):\n",
    "    \"\"\"Train the model\"\"\"\n",
    "    # Create the log file\n",
    "    log_file_name = os.path.relpath(os.path.join(\"logs\", model_name + \".csv\"))\n",
    "    with open(log_file_name, \"w\") as f:\n",
    "        f.write(\"epoch,loss,f1,iou,accuracy,precision,recall\\n\")\n",
    "\n",
    "    # Create a checkpoint file to store the best model\n",
    "    model_file_name = os.path.join(\"checkpoints\", model_name + \".pth\")\n",
    "\n",
    "    # Define the criterion\n",
    "    criterion = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "    # Define the optimizer\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr_max)\n",
    "    # optimizer = torch.optim.SGD(model.parameters(), lr=1, momentum=0.9)\n",
    "\n",
    "    # Define the scheduler and scaler\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=2, eta_min=lr_min)\n",
    "    # scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=50)\n",
    "    scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "    # Variables to store current best metrics\n",
    "    max_f1 = 0\n",
    "    min_loss = 0.5\n",
    "\n",
    "    # Train the model, then save the training logs and the best model\n",
    "    loop = tqdm.tqdm(range(epochs)) \n",
    "    for e in loop:\n",
    "        # Train the model for one epoch and get the loss\n",
    "        loss = epoch(model, train_loader, optimizer, criterion, scaler)\n",
    "        # if loss < 0.5:\n",
    "        #     optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "\n",
    "        # Take a step in the scheduler\n",
    "        scheduler.step()\n",
    "\n",
    "        # Compute the metrics on the validation set\n",
    "        metrics = compute_metrics(validation_loader, model, DEVICE)\n",
    "\n",
    "        # Save the model if it surpasses the current best metrics\n",
    "        # in terms of F1 score\n",
    "        if metrics[\"f1 score\"] > max_f1:\n",
    "            max_f1 = metrics[\"f1 score\"]\n",
    "            if max_f1 > 80.0:\n",
    "                torch.save(model, model_file_name + \".maxf1\")\n",
    "\n",
    "        # or in terms of loss\n",
    "        if loss < min_loss:\n",
    "            torch.save(model, model_file_name + \".minloss\")\n",
    "            min_loss = loss\n",
    "\n",
    "        # Save the logs into a file\n",
    "        with open(log_file_name, \"a\") as f:\n",
    "            f.write(\n",
    "                \"{},{},{},{},{},{},{},{}\\n\".format(\n",
    "                    e,\n",
    "                    loss,\n",
    "                    metrics[\"f1 score\"],\n",
    "                    0,\n",
    "                    metrics[\"acc\"],\n",
    "                    metrics[\"precision\"],\n",
    "                    metrics[\"recall\"],\n",
    "                    min_loss\n",
    "                )\n",
    "            )\n",
    "\n",
    "        # Update the progress bar\n",
    "        loop.set_postfix(loss=loss, f1_score=metrics[\"f1 score\"], max_f1=max_f1, min_loss=min_loss)\n",
    "\n",
    "    # Save the logs into a file\n",
    "    torch.save(model, model_file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e860dcdb",
   "metadata": {},
   "source": [
    "## Data augmentation <a class=\"anchor\" id=\"data_aug\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed0d4fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the data augmentation with albumentations for the training set and convert it to tensor\n",
    "train_transform = albumentations.Compose(\n",
    "    [\n",
    "        albumentations.Flip(p=0.5),\n",
    "        albumentations.Transpose(p=0.5),\n",
    "        albumentations.Rotate(p=0.5),\n",
    "        albumentations.ShiftScaleRotate(p=0.5),\n",
    "        albumentations.RandomBrightnessContrast(p=0.5),\n",
    "        albumentations.CoarseDropout(min_holes= 5, max_holes=20, min_height=5, max_height=20, min_width=5, max_width=20, p=0.5),\n",
    "        albumentations.OpticalDistortion(p=0.5),\n",
    "        albumentations.GridDistortion(p=0.5),\n",
    "        albumentations.ElasticTransform(p=0.5),\n",
    "        albumentations.PiecewiseAffine(p=0.5),\n",
    "        ToTensorV2(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "val_transform = albumentations.Compose(\n",
    "    [\n",
    "        albumentations.HorizontalFlip(p=0.5),\n",
    "        albumentations.RandomRotate90(p=0.5),\n",
    "        ToTensorV2(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Create the data loader for the training\n",
    "train_loader = get_loader(\n",
    "    data_path=TRAIN_IMAGE_PATH,\n",
    "    mask_path=TRAIN_MASK_PATH,\n",
    "    transform=train_transform,\n",
    "    batch_size=4, # Choose the batch size\n",
    ")\n",
    "# Create the data loader for the validation\n",
    "val_loader = get_loader(\n",
    "    data_path=VALIDATION_IMAGE_PATH,\n",
    "    mask_path=VALIDATION_MASK_PATH,\n",
    "    transform=val_transform,\n",
    "    batch_size=4,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7db8df14",
   "metadata": {},
   "source": [
    "# Actual training <a class=\"anchor\" id=\"training\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0157751a",
   "metadata": {},
   "source": [
    "## Phase 1: influence of the pre-processing <a class=\"anchor\" id=\"phase_1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e5a197",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train UNet with preprocessing\n",
    "train(\n",
    "    model=smp.Unet(\n",
    "        encoder_depth=4,\n",
    "        encoder_weights=\"imagenet\",\n",
    "        decoder_channels=[1024, 512, 256, 128],\n",
    "        in_channels=3,\n",
    "    ).to(DEVICE),\n",
    "    model_name=\"unet_smp\",\n",
    "    epochs=1000,\n",
    "    train_loader=train_loader,\n",
    "    validation_loader=val_loader,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a6d56e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train UNet with raw data\n",
    "raw_transform = ToTensorV2()\n",
    "\n",
    "raw_loader = get_loader(\n",
    "    data_path=TRAIN_IMAGE_PATH,\n",
    "    mask_path=TRAIN_MASK_PATH,\n",
    "    transform=raw_transform,\n",
    "    batch_size=4, # Choose the batch size\n",
    ")\n",
    "raw_val_loader = get_loader(\n",
    "    data_path=VALIDATION_IMAGE_PATH,\n",
    "    mask_path=VALIDATION_MASK_PATH,\n",
    "    transform=raw_transform,\n",
    "    batch_size=4,\n",
    ")\n",
    "\n",
    "train(\n",
    "    model=smp.Unet(\n",
    "        encoder_depth=4,\n",
    "        encoder_weights=\"imagenet\",\n",
    "        decoder_channels=[1024, 512, 256, 128],\n",
    "        in_channels=3,\n",
    "    ).to(DEVICE),\n",
    "    model_name=\"unet_no_preprocessing\",\n",
    "    epochs=1000,\n",
    "    train_loader=raw_loader,\n",
    "    validation_loader=raw_val_loader,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f668bbea",
   "metadata": {},
   "source": [
    "## Phase 2: influence of the model <a class=\"anchor\" id=\"phase_2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db3baac9",
   "metadata": {},
   "source": [
    "### LinkNet18 <a class=\"anchor\" id=\"ln_18\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a9077d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train(\n",
    "    model=smp.Linknet(\n",
    "        encoder_name=\"resnet18\",\n",
    "        encoder_depth=4,\n",
    "        encoder_weights=\"imagenet\",\n",
    "        in_channels=3,\n",
    "    ).to(DEVICE),\n",
    "    model_name=\"linknet18_smp\",\n",
    "    epochs=1000,\n",
    "    train_loader=train_loader,\n",
    "    validation_loader=val_loader,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "046c8bb0",
   "metadata": {},
   "source": [
    "### LinkNet34 <a class=\"anchor\" id=\"ln_34\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b32368bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train(\n",
    "    model=smp.Linknet(\n",
    "        encoder_name=\"resnet34\",\n",
    "        encoder_depth=4,\n",
    "        encoder_weights=\"imagenet\",\n",
    "        in_channels=3,\n",
    "    ).to(DEVICE),\n",
    "    model_name=\"linknet34_smp\",\n",
    "    epochs=1000,\n",
    "    train_loader=train_loader,\n",
    "    validation_loader=val_loader,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9216d884",
   "metadata": {},
   "source": [
    "### LinkNet50 <a class=\"anchor\" id=\"ln_50\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b9a9d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train(\n",
    "    model=smp.Linknet(\n",
    "        encoder_name=\"resnet50\",\n",
    "        encoder_depth=4,\n",
    "        encoder_weights=\"imagenet\",\n",
    "        in_channels=3,\n",
    "    ).to(DEVICE),\n",
    "    model_name=\"linknet50_smp\",\n",
    "    epochs=1000,\n",
    "    train_loader=train_loader,\n",
    "    validation_loader=val_loader,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b444666b",
   "metadata": {},
   "source": [
    "### LinkNet101 <a class=\"anchor\" id=\"ln_101\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "020c67f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train(\n",
    "    model=smp.Linknet(\n",
    "        encoder_name=\"resnet101\",\n",
    "        encoder_depth=4,\n",
    "        encoder_weights=\"imagenet\",\n",
    "        in_channels=3,\n",
    "    ).to(DEVICE),\n",
    "    model_name=\"linknet101_smp\",\n",
    "    epochs=1000,\n",
    "    train_loader=train_loader,\n",
    "    validation_loader=val_loader,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f3c5d39",
   "metadata": {},
   "source": [
    "### LinkNet152 <a class=\"anchor\" id=\"ln_152\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ae7aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train(\n",
    "    model=smp.Linknet(\n",
    "        encoder_name=\"resnet152\",\n",
    "        encoder_depth=4,\n",
    "        encoder_weights=\"imagenet\",\n",
    "        in_channels=3,\n",
    "    ).to(DEVICE),\n",
    "    model_name=\"linknet152_smp\",\n",
    "    epochs=1000,\n",
    "    train_loader=train_loader,\n",
    "    validation_loader=val_loader,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8885c427",
   "metadata": {},
   "source": [
    "## Phase 3: hyperparameters choice  <a class=\"anchor\" id=\"hyper\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ab6398",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# Find best hyperparameters for 150 epochs on the LinkNet152\n",
    "# ==============================================================================\n",
    "lr_mins = np.logspace(-6, -3, 4)\n",
    "lr_maxs = np.logspace(-5, -2, 4)\n",
    "batch_sizes = [4, 8, 16, 32]\n",
    "\n",
    "for batch_size in batch_sizes:\n",
    "    # Load the training and validation datasets\n",
    "    train_loader = get_loader(\n",
    "        data_path=TRAIN_IMAGE_PATH,\n",
    "        mask_path=TRAIN_MASK_PATH,\n",
    "        transform=train_transform,\n",
    "        batch_size=batch_size, # Choose the batch size\n",
    "    )\n",
    "\n",
    "    validation_loader = get_loader(\n",
    "        data_path=VALIDATION_IMAGE_PATH,\n",
    "        mask_path=VALIDATION_MASK_PATH,\n",
    "        transform=val_transform,\n",
    "        batch_size=batch_size, # Choose the batch size\n",
    "    )\n",
    "    for lr_min in lr_mins:\n",
    "        for lr_max in lr_maxs:\n",
    "            if lr_max >= lr_min:         \n",
    "                train(\n",
    "                    model=smp.Linknet(\n",
    "                        encoder_name=\"resnet152\",\n",
    "                        encoder_depth=4,\n",
    "                        encoder_weights=\"imagenet\",\n",
    "                        in_channels=3,\n",
    "                    ).to(DEVICE),\n",
    "                    model_name=\"linknet152_\"+str(batch_size)+\"_\"+str(lr_min)+\"_\"+str(lr_max),\n",
    "                    epochs=150,\n",
    "                    train_loader=train_loader,\n",
    "                    validation_loader=validation_loader,\n",
    "                    lr_min=lr_min,\n",
    "                    lr_max=lr_max,\n",
    "                )\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08573107",
   "metadata": {},
   "source": [
    "## Phase 4: best model  <a class=\"anchor\" id=\"phase_4\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a60e063",
   "metadata": {},
   "outputs": [],
   "source": [
    "# our best model\n",
    "batch_size = 16\n",
    "train_loader = get_loader(\n",
    "        data_path=TRAIN_IMAGE_PATH,\n",
    "        mask_path=TRAIN_MASK_PATH,\n",
    "        transform=train_transform,\n",
    "        batch_size=batch_size, # Choose the batch size\n",
    "    )\n",
    "\n",
    "train(\n",
    "    model=smp.Linknet(\n",
    "        encoder_name=\"resnet152\",\n",
    "        encoder_depth=4,\n",
    "        encoder_weights=\"imagenet\",\n",
    "        in_channels=3,\n",
    "    ).to(DEVICE),\n",
    "    model_name=\"BESTMODEL\",\n",
    "    epochs=150, #increase\n",
    "    train_loader=train_loader,\n",
    "    validation_loader=validation_loader,\n",
    "    lr_min=1e-05,\n",
    "    lr_max=1e-05,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5132677e",
   "metadata": {},
   "source": [
    "# Create submissions <a class=\"anchor\" id=\"submissions\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d3d07d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def patch_to_label(patch):\n",
    "    df = np.mean(patch)\n",
    "    if df > SUBMISSION_THRESHOLD:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "\n",
    "def mask_to_submission_strings(image_filename):\n",
    "    \"\"\"Reads a single image and outputs the strings that should go into the submission file\"\"\"\n",
    "    print(image_filename, os.path.basename(image_filename))\n",
    "    img_number = int(re.search(r\"\\d+\", os.path.basename(image_filename)).group(0))\n",
    "    print(img_number)\n",
    "    im = mpimg.imread(image_filename)\n",
    "    patch_size = 16\n",
    "    for j in range(0, im.shape[1], patch_size):\n",
    "        for i in range(0, im.shape[0], patch_size):\n",
    "            patch = im[i : i + patch_size, j : j + patch_size]\n",
    "            label = patch_to_label(patch)\n",
    "            yield (\"{:03d}_{}_{},{}\".format(img_number, j, i, label))\n",
    "\n",
    "\n",
    "def masks_to_submission(submission_filename, *image_filenames):\n",
    "    \"\"\"Converts images into a submission file\"\"\"\n",
    "    with open(submission_filename, \"w\") as f:\n",
    "        f.write(\"id,prediction\\n\")\n",
    "        for fn in image_filenames:\n",
    "            f.writelines(\"{}\\n\".format(s) for s in mask_to_submission_strings(fn))\n",
    "\n",
    "\n",
    "def create_postprocessing_images(images, rotations, transposes):\n",
    "    \"\"\"Apply transformations to the image and return different prospectives\"\"\"\n",
    "    ims = []\n",
    "    for image in images:\n",
    "        for rotation in rotations:\n",
    "            ims.append(albumentations.rotate(image, rotation))\n",
    "            if transposes:\n",
    "                im = albumentations.hflip(image)\n",
    "                ims.append(albumentations.rotate(im, rotation))\n",
    "    ims = np.array(ims)\n",
    "    ims = torch.tensor(ims).transpose(1, -1).transpose(2, -1).float()\n",
    "    return ims\n",
    "\n",
    "\n",
    "def combine_postprocessing_images(images, rotations, transposes):\n",
    "    \"\"\"Combine predictions of different prospectives\"\"\"\n",
    "    outputs = []\n",
    "    index = 0\n",
    "    while index < len(images):\n",
    "        output = np.zeros(images[0].shape)\n",
    "        for rotation in rotations:\n",
    "            im = images[index, 0]\n",
    "            output += albumentations.rotate(im, -rotation)\n",
    "            index += 1\n",
    "            if transposes:\n",
    "                im = images[index, 0]\n",
    "                im = albumentations.rotate(im, -rotation)\n",
    "                output += albumentations.hflip(im)\n",
    "                index += 1\n",
    "        output = output / len(images)\n",
    "        outputs.append(output)\n",
    "    return np.array(outputs)\n",
    "\n",
    "\n",
    "def create_submission(model_name: str):\n",
    "    model_file_name = os.path.join(\"checkpoints\", model_name + \".pth.maxf1\")\n",
    "    model = torch.load(model_file_name, map_location=torch.device(DEVICE)).to(DEVICE)\n",
    "    model.eval()\n",
    "\n",
    "    # Create the directory to store the predictions\n",
    "    path = \"data/test_set_images\"\n",
    "    pred_path = \"predictions/\" + model_name\n",
    "    if not os.path.exists(pred_path):\n",
    "        os.makedirs(pred_path)\n",
    "\n",
    "    # For each image, apply postprocessing augmentation, make predictions and save predictions\n",
    "    for image in tqdm.tqdm(os.listdir(path)):\n",
    "        img_path = os.path.join(path, image, image + \".png\")\n",
    "        im = np.asarray(Image.open(img_path)) / 255\n",
    "        ims = create_postprocessing_images(\n",
    "            [im], rotations=[0, 90, 180, 270], transposes=True\n",
    "        )\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output = model(ims.to(DEVICE))\n",
    "            predicts = torch.sigmoid(output).cpu().detach()\n",
    "\n",
    "        predict = combine_postprocessing_images(\n",
    "            predicts.numpy(), rotations=[0, 90, 180, 270], transposes=True\n",
    "        ).reshape((608, 608))\n",
    "        predict[predict < 0.5] = 0\n",
    "        predict[predict >= 0.5] = 1\n",
    "        predict *= 255\n",
    "        Image.fromarray(predict).convert(\"L\").save(\n",
    "            os.path.join(pred_path, image) + \".png\"\n",
    "        )\n",
    "\n",
    "    # Generate the submission file\n",
    "    submission_filename = os.path.join(\n",
    "        \"submissions\", \"submission_{}.csv\".format(model_name)\n",
    "    )\n",
    "    image_filenames = []\n",
    "    for i in range(1, 51):\n",
    "        image_filename = pred_path + \"/test_\" + str(i) + \".png\"\n",
    "        image_filenames.append(image_filename)\n",
    "    masks_to_submission(submission_filename, *image_filenames)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b25cd94d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#finally create the submissions\n",
    "create_submission(\"unet_smp\")\n",
    "create_submission(\"unet_no_preprocessing\")\n",
    "create_submission(\"linknet18_smp\")\n",
    "create_submission(\"linknet34_smp\")\n",
    "create_submission(\"linknet50_smp\")\n",
    "create_submission(\"linknet101_smp\")\n",
    "create_submission(\"linknet152_smp\")\n",
    "create_submission(\"BESTMODEL\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75000804",
   "metadata": {},
   "source": [
    "# Local testing <a class=\"anchor\" id=\"testing\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ec550c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_postprocessing_images(images, rotations, transposes):\n",
    "    \"\"\"Apply transformations to the image and return different prospectives\"\"\"\n",
    "    ims = []\n",
    "    for image in images:\n",
    "        for rotation in rotations:\n",
    "            ims.append(albumentations.rotate(image, rotation))\n",
    "            if transposes:\n",
    "                im = albumentations.hflip(image)\n",
    "                ims.append(albumentations.rotate(im, rotation))\n",
    "    ims = np.array(ims)\n",
    "    ims = torch.tensor(ims).transpose(1, -1).transpose(2, -1).float()\n",
    "    return ims\n",
    "\n",
    "\n",
    "def combine_postprocessing_images(images, rotations, transposes):\n",
    "    \"\"\"Combine predictions of different prospectives\"\"\"\n",
    "    outputs = []\n",
    "    index = 0\n",
    "    while index < len(images):\n",
    "        output = np.zeros(images[0].shape)\n",
    "        for rotation in rotations:\n",
    "            im = images[index, 0]\n",
    "            output += albumentations.rotate(im, -rotation)\n",
    "            index += 1\n",
    "            if transposes:\n",
    "                im = images[index, 0]\n",
    "                im = albumentations.rotate(im, -rotation)\n",
    "                output += albumentations.hflip(im)\n",
    "                index += 1\n",
    "        output = output / len(images)\n",
    "        outputs.append(output)\n",
    "    return np.array(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d0ce4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_validation_metrics(model_name):\n",
    "    \"\"\"Compute the accuracy rate on the validation dataset with the input model\"\"\"\n",
    "    model_file_name = os.path.join(\"checkpoints\", model_name + \".pth.maxf1\")\n",
    "    model = torch.load(model_file_name, map_location=torch.device(DEVICE)).to(DEVICE)\n",
    "    model.eval()\n",
    "\n",
    "    # Create the directory to store the predictions\n",
    "    path = \"data/validating/\"\n",
    "    images_path = path+\"images\"\n",
    "    groundtruths_path = path+\"groundtruth\" \n",
    "    pred_path = \"predictions/validation/\" + model_name\n",
    "    if not os.path.exists(pred_path):\n",
    "        os.makedirs(pred_path)\n",
    "\n",
    "    # Dictionnary to store metrics\n",
    "    logs = dict()\n",
    "\n",
    "    # Parameters to compute metrics\n",
    "    num_correct = 0\n",
    "    num_pixels = 0\n",
    "\n",
    "    # Metrics computation\n",
    "    f1_score = 0\n",
    "    precision = 0\n",
    "    recall = 0\n",
    "    \n",
    "    # For each image, apply postprocessing augmentation, make predictions and save predictions\n",
    "    nb_images = len(os.listdir(images_path))\n",
    "    for image in tqdm.tqdm(os.listdir(images_path)):\n",
    "        img_path = os.path.join(images_path, image)\n",
    "        im = np.asarray(Image.open(img_path)) / 255\n",
    "        ims = create_postprocessing_images(\n",
    "            [im], rotations=[0, 90, 180, 270], transposes=True\n",
    "        )\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output = model(ims.to(DEVICE))\n",
    "            # output = output.cpu().detach().numpy()\n",
    "            predicts = torch.sigmoid(output).cpu().detach()\n",
    "\n",
    "        predict = combine_postprocessing_images(\n",
    "            predicts.numpy(), rotations=[0, 90, 180, 270], transposes=True\n",
    "        ).reshape((400, 400))\n",
    "\n",
    "        predict[predict < 0.5] = 0\n",
    "        predict[predict >= 0.5] = 1\n",
    "        predict_to_save = predict*255\n",
    "        Image.fromarray(predict_to_save).convert(\"L\").save(\n",
    "            os.path.join(pred_path, image) + \".png\"\n",
    "        )\n",
    "        \n",
    "        groundtruth_path = os.path.join(groundtruths_path, image)\n",
    "        y = torch.tensor(np.asarray(Image.open(groundtruth_path)))\n",
    "        y = (y > 200).int()\n",
    "        predict = torch.tensor(predict)\n",
    "        \n",
    "        num_correct += torch.sum(predict == y)\n",
    "        num_pixels += torch.numel(predict)\n",
    "        \n",
    "        # Compute F1 score\n",
    "        tp, fp, fn, tn = metrics.get_stats(predict.int(), y, mode='binary')\n",
    "        f1_score += metrics.f1_score(tp, fp, fn, tn, reduction='micro')\n",
    "        precision += metrics.precision(tp, fp, fn, tn, reduction='micro')\n",
    "        recall += metrics.recall(tp, fp, fn, tn, reduction='micro')\n",
    "\n",
    "    # Add metrics to the dictionnary and multiply by 100 to get a percentage\n",
    "    logs[\"acc\"] = num_correct / num_pixels * 100\n",
    "    logs[\"f1 score\"] = f1_score.cpu().numpy() / nb_images * 100\n",
    "    logs[\"precision\"] = precision.cpu().numpy() / nb_images * 100\n",
    "    logs[\"recall\"] = recall.cpu().numpy() / nb_images * 100\n",
    "\n",
    "    # Set model back to training mode\n",
    "    model.train()\n",
    "\n",
    "    if not os.path.exists(\"validation/logs\"):\n",
    "        os.makedirs(\"validation/logs\")\n",
    "    log_file_name = os.path.relpath(os.path.join(\"validation/logs\", model_name + \".csv\"))\n",
    "    with open(log_file_name, \"w\") as f:\n",
    "        f.write(\"f1,accuracy,precision,recall\\n\")\n",
    "        f.write(\n",
    "            \"{},{},{},{}\".format(\n",
    "                logs[\"f1 score\"],\n",
    "                logs[\"acc\"],\n",
    "                logs[\"precision\"],\n",
    "                logs[\"recall\"],\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef802a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute validation metrics\n",
    "compute_validation_metrics(\"linknet152_4_1e-05_1e-05\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "9fee673dc4fc01db422c313512934603878a992984f49c83d6d8be337f7be7ab"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
